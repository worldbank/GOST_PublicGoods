{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peartree version: 0.6.0 \n",
      "networkx version: 2.2 \n",
      "matplotlib version: 2.2.2 \n",
      "osmnx version: 0.8.2 \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os, sys\n",
    "sys.path.append(r'C:\\Users\\charl\\Documents\\GitHub\\GOST_PublicGoods\\GOSTNets\\GOSTNets')\n",
    "sys.path.append(r'C:\\Users\\charl\\Documents\\GitHub\\GOST')\n",
    "import GOSTnet as gn\n",
    "import importlib\n",
    "importlib.reload(gn)\n",
    "import geopandas as gpd\n",
    "import rasterio as rt\n",
    "from rasterio import features\n",
    "from shapely.wkt import loads\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from shapely.geometry import box, Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "basepth = r'C:\\Users\\charl\\Documents\\GOST\\Yemen'\n",
    "pth = r'C:\\Users\\charl\\Documents\\GOST\\Yemen\\graphtool'\n",
    "net_pth = pth\n",
    "pckle = r'walk_graph.pickle'\n",
    "WGS = {'init':'epsg:4326'}\n",
    "measure_crs = {'init':'epsg:32638'}\n",
    "subset = r'YEHNP_PHCs_driving_24th_newwalk' \n",
    "OD_name = r'output_driving_24th.csv'\n",
    "srtm_pth = r'C:\\Users\\charl\\Documents\\GOST\\Yemen\\SRTM'\n",
    "offroad_speed = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import All-Destination OD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "OD = pd.read_csv(os.path.join(pth, OD_name))\n",
    "OD = OD.rename(columns = {'Unnamed: 0':'O_ID'})\n",
    "OD = OD.set_index('O_ID')\n",
    "OD = OD.replace([np.inf, -np.inf], np.nan)\n",
    "OD_original = OD.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Subset to Accepted Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1003"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "acceptable_df = pd.read_csv(os.path.join(pth, 'HeRAMs_2016v2018_damages and functionality_cleaned_snapped.csv'))\n",
    "print(acceptable_df.columns)\n",
    "hosp_types = ['District L Rural\\r Hospital','Governorate/General\\r Hospital','Hospital','1']\n",
    "def hospcheck(x, hosp_types):\n",
    "    if x in hosp_types:\n",
    "        return 'HOS'\n",
    "    else:\n",
    "        return 'PHC'\n",
    "acceptable_df['hosp'] = acceptable_df['Facility Type2'].apply(lambda x: hospcheck(x, hosp_types))\n",
    "acceptable_df = acceptable_df.loc[acceptable_df['hosp'] == 'PHC']\n",
    "function_level = ['1','2',1,2]\n",
    "acceptable_df = acceptable_df.loc[acceptable_df['Functionality2018'].isin(function_level)]\n",
    "#service_level = ['1']\n",
    "#acceptable_df = acceptable_df.loc[acceptable_df['Comprehensive Emergency Obstetric Care (S424)'].isin(service_level)]\n",
    "\"\"\"\n",
    "acceptable_df = pd.read_csv(os.path.join(pth, 'YEHNP PHCs_unicef_snapped.csv'))\n",
    "\n",
    "\n",
    "len(acceptable_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "acceptable_df['geometry'] = acceptable_df['geometry'].apply(loads)\n",
    "acceptable_gdf = gpd.GeoDataFrame(acceptable_df, geometry = 'geometry', crs = {'init':'epsg:4326'})\n",
    "accepted_facilities = list(set(list(acceptable_df.NN)))\n",
    "accepted_facilities_str = [str(i) for i in accepted_facilities]\n",
    "OD = OD[accepted_facilities_str]\n",
    "acceptable_df.to_csv(os.path.join(basepth,'output_layers','%s.csv' % subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36158, 4114)\n",
      "(36158, 944)\n"
     ]
    }
   ],
   "source": [
    "print(OD_original.shape)\n",
    "print(OD.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to add elevation to a point GeoDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_elevation(df, x, y, srtm_pth):\n",
    "    # walk all tiles, find path\n",
    "    \n",
    "    tiles = []\n",
    "    for root, folder, files in os.walk(os.path.join(srtm_pth,'high_res')):\n",
    "        for f in files:\n",
    "            if f[-3:] == 'hgt':\n",
    "                tiles.append(f[:-4])\n",
    "\n",
    "    # load dictionary of tiles\n",
    "    arrs = {}\n",
    "    for t in tiles:\n",
    "        arrs[t] = rt.open(srtm_pth+r'\\high_res\\{}.hgt\\{}.hgt'.format(t, t), 'r')\n",
    "\n",
    "    # assign a code\n",
    "    uniques = []\n",
    "    df['code'] = 'placeholder'\n",
    "    def tile_code(z):\n",
    "        E = str(z[x])[:2]\n",
    "        N = str(z[y])[:2]\n",
    "        return 'N{}E0{}'.format(N, E)\n",
    "    df['code'] = df.apply(lambda z: tile_code(z), axis = 1)\n",
    "    unique_codes = list(set(df['code'].unique()))\n",
    "    \n",
    "    z = {}\n",
    "    # Match on High Precision Elevation\n",
    "    property_name = 'elevation'\n",
    "    for code in unique_codes:\n",
    "        \n",
    "        df2 = df.copy()\n",
    "        df2 = df2.loc[df2['code'] == code]\n",
    "        dataset = arrs[code]\n",
    "        b = dataset.bounds\n",
    "        datasetBoundary = box(b[0], b[1], b[2], b[3])\n",
    "        selKeys = []\n",
    "        selPts = []\n",
    "        for index, row in df2.iterrows():\n",
    "            if Point(row[x], row[y]).intersects(datasetBoundary):\n",
    "                selPts.append((row[x],row[y]))\n",
    "                selKeys.append(index)\n",
    "        raster_values = list(dataset.sample(selPts))\n",
    "        raster_values = [x[0] for x in raster_values]\n",
    "\n",
    "        # generate new dictionary of {node ID: raster values}\n",
    "        z.update(zip(selKeys, raster_values))\n",
    "        \n",
    "    elev_df = pd.DataFrame.from_dict(z, orient='index')\n",
    "    elev_df.columns = ['elevation']\n",
    "    \n",
    "    missing = elev_df.copy()\n",
    "    missing = missing.loc[missing.elevation < 0]\n",
    "    if len(missing) > 0:\n",
    "        missing_df = df.copy()\n",
    "        missing_df = missing_df.loc[missing.index]\n",
    "        low_res_tifpath = os.path.join(srtm_pth, 'clipped', 'clipped_e20N40.tif')\n",
    "        dataset = rt.open(low_res_tifpath, 'r')\n",
    "        b = dataset.bounds\n",
    "        datasetBoundary = box(b[0], b[1], b[2], b[3])\n",
    "        selKeys = []\n",
    "        selPts = []\n",
    "        for index, row in missing_df.iterrows():\n",
    "            if Point(row[x], row[y]).intersects(datasetBoundary):\n",
    "                selPts.append((row[x],row[y]))\n",
    "                selKeys.append(index)\n",
    "        raster_values = list(dataset.sample(selPts))\n",
    "        raster_values = [x[0] for x in raster_values]\n",
    "        z.update(zip(selKeys, raster_values))\n",
    "\n",
    "        elev_df = pd.DataFrame.from_dict(z, orient='index')\n",
    "        elev_df.columns = ['elevation']\n",
    "    df['point_elev'] = elev_df['elevation']\n",
    "    df = df.drop('code', axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to convert distances to walk times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_walktimes(df, start = 'point_elev', end = 'node_elev', dist = 'NN_dist', max_walkspeed = 6, min_speed = 0.1):\n",
    "    # Tobler's hiking function: https://en.wikipedia.org/wiki/Tobler%27s_hiking_function\n",
    "    def speed(incline_ratio, max_speed):\n",
    "        walkspeed = max_speed * np.exp(-3.5 * abs(incline_ratio + 0.05)) \n",
    "        return walkspeed\n",
    "\n",
    "    speeds = {}\n",
    "    times = {}\n",
    "\n",
    "    for index, data in df.iterrows():\n",
    "        if data[dist] > 0:\n",
    "            delta_elevation = data[end] - data[start]\n",
    "            incline_ratio = delta_elevation / data[dist]\n",
    "            speed_kmph = speed(incline_ratio = incline_ratio, max_speed = max_walkspeed)\n",
    "            speed_kmph = max(speed_kmph, min_speed)\n",
    "            speeds[index] = (speed_kmph)\n",
    "            times[index] = (data[dist] / 1000 * 3600 / speed_kmph)\n",
    "\n",
    "    speed_df = pd.DataFrame.from_dict(speeds, orient = 'index')\n",
    "    time_df = pd.DataFrame.from_dict(times, orient = 'index')\n",
    "\n",
    "    df['walkspeed'] = speed_df[0]\n",
    "    df['walk_time'] = time_df[0]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add elevation for destination nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\charl\\AppData\\Local\\Continuum\\anaconda3\\envs\\Cfox_2\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\charl\\AppData\\Local\\Continuum\\anaconda3\\envs\\Cfox_2\\lib\\site-packages\\ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\charl\\AppData\\Local\\Continuum\\anaconda3\\envs\\Cfox_2\\lib\\site-packages\\ipykernel_launcher.py:71: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "dest_df = acceptable_df[['NN','NN_dist','Latitude','Longitude']]\n",
    "dest_df = add_elevation(dest_df, 'Longitude','Latitude', srtm_pth).set_index('NN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add elevation from graph nodes (reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_gpickle(os.path.join(pth, pckle))\n",
    "G_node_df = gn.node_gdf_from_graph(G)\n",
    "G_node_df = add_elevation(G_node_df, 'x', 'y', srtm_pth)\n",
    "match_node_elevs = G_node_df[['node_ID','point_elev']].set_index('node_ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match on node elevations for dest_df; calculate travel times to nearest node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_df['node_elev'] = match_node_elevs['point_elev']\n",
    "dest_df = generate_walktimes(dest_df, start = 'node_elev', end = 'point_elev', dist = 'NN_dist', max_walkspeed = offroad_speed)\n",
    "dest_df = dest_df.sort_values(by = 'walk_time', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Walk Time to all travel times in OD matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_df = dest_df[['walk_time']]\n",
    "dest_df.index = dest_df.index.map(str)\n",
    "\n",
    "d_f = OD.transpose()\n",
    "\n",
    "for i in d_f.columns:\n",
    "    dest_df[i] = d_f[i]\n",
    "    \n",
    "for i in dest_df.columns:\n",
    "    if i == 'walk_time':\n",
    "        pass\n",
    "    else:\n",
    "        dest_df[i] = dest_df[i] + dest_df['walk_time']\n",
    "\n",
    "dest_df = dest_df.drop('walk_time', axis = 1)\n",
    "\n",
    "dest_df = dest_df.transpose()\n",
    "\n",
    "dest_df['min_time'] = dest_df.min(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match on network time from origin node (time travelling along network + walking to destination)\n",
    "grid_name = r'origins_1km_snapped.csv'\n",
    "grid = pd.read_csv(os.path.join(pth, grid_name))\n",
    "grid = grid.rename(columns = {'NN':'O_ID'})\n",
    "grid = grid.set_index(grid['O_ID'])\n",
    "grid = grid.rename({'Unnamed: 0':'PointID'}, axis = 1)\n",
    "grid['on_network_time'] = dest_df['min_time']\n",
    "grid['geometry'] = grid['geometry'].apply(loads)\n",
    "grid = grid.set_index('PointID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add origin node distance to network - walking time\n",
    "grid = add_elevation(grid, 'Longitude','Latitude', srtm_pth)\n",
    "grid = grid.reset_index()\n",
    "grid = grid.set_index('O_ID')\n",
    "grid['node_elev'] = match_node_elevs['point_elev']\n",
    "grid = grid.set_index('PointID')\n",
    "grid = generate_walktimes(grid, start = 'point_elev', end = 'node_elev', dist = 'NN_dist', max_walkspeed = offroad_speed)\n",
    "grid = grid.rename({'node_elev':'nr_node_on_net_elev', \n",
    "                    'walkspeed':'walkspeed_to_net', \n",
    "                    'walk_time':'walk_time_to_net',\n",
    "                   'NN_dist':'NN_dist_to_net'}, axis = 1)\n",
    "grid['total_time_net'] = grid['on_network_time'] + grid['walk_time_to_net']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Direct Walking Time (not using road network), vs. network Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_gdf = gpd.GeoDataFrame(grid, crs = WGS, geometry = 'geometry')\n",
    "grid = gn.pandana_snap_points(grid_gdf, \n",
    "                              acceptable_gdf, \n",
    "                              source_crs = 'epsg:4326', \n",
    "                              target_crs = 'epsg:32638', \n",
    "                              add_dist_to_node_col = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = grid.set_index('NN')\n",
    "grid['dest_NN_elev'] = match_node_elevs['point_elev']\n",
    "grid = grid.reset_index()\n",
    "grid2 = grid.copy()\n",
    "grid2 = generate_walktimes(grid2, start = 'point_elev', end = 'dest_NN_elev', dist = 'NN_dist', max_walkspeed = offroad_speed).reset_index()\n",
    "grid = grid2\n",
    "grid = grid.rename({'walkspeed':'walkspeed_direct', \n",
    "                    'walk_time':'walk_time_direct',\n",
    "                   'NN_dist':'NN_dist_direct'}, axis = 1)\n",
    "grid['PLOT_TIME_SECS'] = grid[['walk_time_direct','total_time_net']].min(axis = 1)\n",
    "grid['PLOT_TIME_MINS'] = grid['PLOT_TIME_SECS'] / 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Burn Raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "rst_fn = os.path.join(pth,'pop18_resampled.tif')\n",
    "out_fn = os.path.join(basepth,'output_layers','%s.tif' % subset)\n",
    "\n",
    "# Update metadata\n",
    "rst = rt.open(rst_fn, 'r')\n",
    "meta = rst.meta.copy()\n",
    "D_type = rt.float64\n",
    "meta.update(compress='lzw', dtype = D_type, count = 2)\n",
    "\n",
    "with rt.open(out_fn, 'w', **meta) as out:\n",
    "    with rt.open(rst_fn, 'r') as pop:\n",
    "        \n",
    "        # this is where we create a generator of geom, value pairs to use in rasterizing\n",
    "        shapes = ((geom,value) for geom, value in zip(grid.geometry, grid.PLOT_TIME_MINS))\n",
    "\n",
    "        population = pop.read(1).astype(D_type)\n",
    "        cpy = population.copy()\n",
    "\n",
    "        travel_times = features.rasterize(shapes=shapes, fill=0, out=cpy, transform=out.transform)\n",
    "\n",
    "        out.write_band(1, population)\n",
    "        out.write_band(2, travel_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Zonal Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zonalStats(inShp, inRaster, bandNum=1, mask_A = None, reProj = False, minVal = '', maxVal = '', verbose=False , rastType='N', unqVals=[]):\n",
    "    import sys, os, inspect, logging, json\n",
    "    import rasterio, affine\n",
    "\n",
    "    import pandas as pd\n",
    "    import geopandas as gpd\n",
    "    import numpy as np\n",
    "\n",
    "    from collections import Counter\n",
    "    from shapely.geometry import box\n",
    "    from affine import Affine\n",
    "    from rasterio import features\n",
    "    from rasterio.mask import mask\n",
    "    from rasterio.features import rasterize\n",
    "    from rasterio.warp import reproject, Resampling\n",
    "    from osgeo import gdal\n",
    "    \n",
    "    ''' Run zonal statistics against an input shapefile\n",
    "    \n",
    "    INPUT VARIABLES\n",
    "    inShp [string or geopandas object] - path to input shapefile\n",
    "    inRaster [string or rasterio object] - path to input raster\n",
    "    \n",
    "    OPTIONAL\n",
    "    bandNum [integer] - band in raster to analyze\n",
    "    reProj [boolean] -  whether to reproject data to match, if not, raise an error\n",
    "    minVal [number] - if defined, will only calculation statistics on values above this number\n",
    "    verbose [boolean] - whether to be loud with responses\n",
    "    rastType [string N or C] - N is numeric and C is categorical. Categorical returns counts of numbers\n",
    "    unqVals [array of numbers] - used in categorical zonal statistics, tabulates all these numbers, will report 0 counts\n",
    "    mask_A [numpy boolean mask] - mask the desired band using an identical shape boolean mask. Useful for doing conditional zonal stats\n",
    "    \n",
    "    RETURNS\n",
    "    array of arrays, one for each feature in inShp\n",
    "    '''   \n",
    "    if isinstance(inShp, str):\n",
    "        inVector = gpd.read_file(inShp) \n",
    "    else:\n",
    "        inVector = inShp\n",
    "    if isinstance(inRaster, str):\n",
    "        curRaster = rasterio.open(inRaster, 'r+')\n",
    "    else:\n",
    "        curRaster = inRaster\n",
    "        \n",
    "    # If mask is not none, apply mask \n",
    "    if mask_A is not None:\n",
    "        \n",
    "        curRaster.write_mask(np.invert(mask_A))\n",
    "    \n",
    "    outputData=[]\n",
    "    if inVector.crs != curRaster.crs:\n",
    "        if reProj:\n",
    "            inVector = inVector.to_crs(curRaster.crs)\n",
    "        else:\n",
    "            raise ValueError(\"Input CRS do not match\")\n",
    "    fCount = 0\n",
    "    tCount = len(inVector['geometry'])\n",
    "    #generate bounding box geometry for raster bbox\n",
    "    b = curRaster.bounds\n",
    "    rBox = box(b[0], b[1], b[2], b[3])\n",
    "    for geometry in inVector['geometry']:\n",
    "        #This test is used in case the geometry extends beyond the edge of the raster\n",
    "        #   I think it is computationally heavy, but I don't know of an easier way to do it\n",
    "        if not rBox.contains(geometry):\n",
    "            geometry = geometry.intersection(rBox)            \n",
    "        try:\n",
    "            fCount = fCount + 1\n",
    "            if fCount % 1000 == 0 and verbose:\n",
    "                tPrint(\"Processing %s of %s\" % (fCount, tCount) )\n",
    "            # get pixel coordinates of the geometry's bounding box\n",
    "            ul = curRaster.index(*geometry.bounds[0:2])\n",
    "            lr = curRaster.index(*geometry.bounds[2:4])\n",
    "            '''\n",
    "            TODO: There is a problem with the indexing - if the shape falls outside the boundaries, it errors\n",
    "                I want to change it to just grab what it can find, but my brain is wrecked and I cannot figure it out\n",
    "            print(geometry.bounds)\n",
    "            print(curRaster.shape)\n",
    "            print(lr)\n",
    "            print(ul)\n",
    "            lr = (max(lr[0], 0), min(lr[1], curRaster.shape[1]))\n",
    "            ul = (min(ul[0], curRaster.shape[0]), min(ul[1]))\n",
    "            '''\n",
    "            # read the subset of the data into a numpy array\n",
    "            window = ((float(lr[0]), float(ul[0]+1)), (float(ul[1]), float(lr[1]+1)))\n",
    "            \n",
    "            if mask is not None:\n",
    "                data = curRaster.read(bandNum, window=window, masked = True)\n",
    "            else:\n",
    "                data = curRaster.read(bandNum, window=window, masked = False)\n",
    "            \n",
    "            # create an affine transform for the subset data\n",
    "            t = curRaster.transform\n",
    "            shifted_affine = Affine(t.a, t.b, t.c+ul[1]*t.a, t.d, t.e, t.f+lr[0]*t.e)\n",
    "\n",
    "            # rasterize the geometry\n",
    "            mask = rasterize(\n",
    "                [(geometry, 0)],\n",
    "                out_shape=data.shape,\n",
    "                transform=shifted_affine,\n",
    "                fill=1,\n",
    "                all_touched=False,\n",
    "                dtype=np.uint8)\n",
    "\n",
    "            # create a masked numpy array\n",
    "            masked_data = np.ma.array(data=data, mask=mask.astype(bool))\n",
    "            if rastType == 'N':                \n",
    "                if minVal != '' or maxVal != '':\n",
    "                    if minVal != '':\n",
    "                        masked_data = np.ma.masked_where(masked_data < minVal, masked_data)\n",
    "                    if maxVal != '':\n",
    "                        masked_data = np.ma.masked_where(masked_data > maxVal, masked_data)                    \n",
    "                    if masked_data.count() > 0:                        \n",
    "                        results = [masked_data.sum(), masked_data.min(), masked_data.max(), masked_data.mean()]\n",
    "                    else :\n",
    "                        results = [-1, -1, -1, -1]                \n",
    "                else:\n",
    "                    results = [masked_data.sum(), masked_data.min(), masked_data.max(), masked_data.mean()]\n",
    "            if rastType == 'C':\n",
    "                if len(unqVals) > 0:                          \n",
    "                    xx = dict(Counter(data.flatten()))\n",
    "                    results = [xx.get(i, 0) for i in unqVals]                \n",
    "                else:\n",
    "                    results = np.unique(masked_data, return_counts=True)                    \n",
    "            outputData.append(results)\n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "            outputData.append([-1, -1, -1, -1])            \n",
    "    return outputData   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = r'HeRAMS Hospitals_driving_24th_newwalk'\n",
    "out_fn = r'C:\\Users\\charl\\Documents\\GOST\\Yemen\\output_layers\\%s.tif' % subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sys.path.append(r'C:\\Users\\charl\\Documents\\GitHub\\GOST\\GOSTRocks')\n",
    "#from GOSTRocks.rasterMisc import *\n",
    "### MODIFIED FUNCTION BELOW!!\n",
    "\n",
    "utils = r'C:\\Users\\charl\\Documents\\GOST\\Yemen\\util_files'\n",
    "\n",
    "yemen_shp_name = os.path.join(utils, r'Yemen_bound.shp')\n",
    "yemen_shp = gpd.read_file(yemen_shp_name)\n",
    "yemen_shp = yemen_shp.to_crs({'init': 'epsg:4326'})\n",
    "\n",
    "district_shp_name = os.path.join(utils, r'Yemen_adm2.shp')\n",
    "district_shp = gpd.read_file(district_shp_name)\n",
    "district_shp = district_shp.to_crs({'init': 'epsg:4326'})\n",
    "\n",
    "inraster = out_fn\n",
    "ras = rt.open(inraster, mode = 'r+')\n",
    "pop = ras.read(1)\n",
    "tt_matrix = ras.read(2)\n",
    "\n",
    "resolution = 'district'\n",
    "\n",
    "if resolution == 'national':\n",
    "    target_shp = yemen_shp\n",
    "elif resolution == 'district':\n",
    "    target_shp = district_shp\n",
    "\n",
    "## First, add on the total population of the district to each district shape\n",
    "\n",
    "mask_pop = np.ma.masked_where(pop > (200000), pop).mask\n",
    "\n",
    "base_pop = zonalStats(target_shp, \n",
    "                        inraster, \n",
    "                        bandNum = 1,\n",
    "                        mask_A = mask_pop,\n",
    "                        reProj = False, \n",
    "                        minVal = 0,\n",
    "                        maxVal = np.inf, \n",
    "                        verbose = True, \n",
    "                        rastType='N')\n",
    "\n",
    "cols = ['total_pop','min','max','mean']\n",
    "\n",
    "temp_df = pd.DataFrame(base_pop, columns = cols)\n",
    "\n",
    "target_shp['total_pop'] = temp_df['total_pop']\n",
    "target_shp['total_pop'].loc[target_shp['total_pop'] == -1] = 0\n",
    "\n",
    "## Now, calculate the population within a range of time thresholds from the destination set\n",
    "for time_thresh in [30,60,120, 240]:\n",
    "    \n",
    "    mask_obj = np.ma.masked_where(tt_matrix > (time_thresh), tt_matrix).mask\n",
    "\n",
    "    raw = zonalStats(target_shp, \n",
    "                        inraster, \n",
    "                        bandNum = 1,\n",
    "                        mask_A = mask_obj,\n",
    "                        reProj = False, \n",
    "                        minVal = 0,\n",
    "                        maxVal = np.inf, \n",
    "                        verbose = True, \n",
    "                        rastType='N')\n",
    "\n",
    "    cols = ['pop_%s' % time_thresh,'min','max','mean']\n",
    "\n",
    "    temp_df = pd.DataFrame(raw, columns = cols)\n",
    "\n",
    "    target_shp['pop_%s' % time_thresh] = temp_df['pop_%s' % time_thresh]\n",
    "    target_shp['pop_%s' % time_thresh].loc[target_shp['pop_%s' % time_thresh] == -1] = 0\n",
    "    target_shp['frac_%s' % time_thresh] = (target_shp['pop_%s' % time_thresh]) / (target_shp['total_pop']).fillna(0)\n",
    "    target_shp['frac_%s' % time_thresh].replace([np.inf, -np.inf], 0)\n",
    "    target_shp['frac_%s' % time_thresh] = target_shp['frac_%s' % time_thresh].fillna(0)\n",
    "    \n",
    "# Save to file\n",
    "                  \n",
    "if resolution == 'national':\n",
    "    print(out_fn)\n",
    "    print(target_shp.frac_30.head(1))\n",
    "    print(target_shp.frac_60.head(1))\n",
    "    print(target_shp.frac_120.head(1))\n",
    "    print(target_shp.frac_240.head(1))\n",
    "else:\n",
    "    target_shp['abs_pop_iso'] = target_shp['total_pop'] - target_shp['pop_30']\n",
    "    target_shp.to_file(os.path.join(basepth, 'output_layers','webmap_batch2','%s_zonal_%s.shp' % (subset, resolution)), driver = 'ESRI Shapefile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Cfox2)",
   "language": "python",
   "name": "cfox2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
