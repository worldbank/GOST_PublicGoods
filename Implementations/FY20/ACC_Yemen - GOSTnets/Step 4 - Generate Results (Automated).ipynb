{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - Generate Results (Automated)\n",
    "\n",
    "This script is a fully automated version of the previous script (Step 4). It has precisely the same mechanics. As such, to learn how it works, go through the non-automated version first - and then approach this version. \n",
    "\n",
    "The only difference / complexity is the use of settings dictionaries to run scenarios in bulk in loop fashion - see the end of the script for examples of this.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peartree version: 0.6.1 \n",
      "networkx version: 2.3 \n",
      "matplotlib version: 3.0.3 \n",
      "osmnx version: 0.9 \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os, sys\n",
    "sys.path.append(r'C:\\Users\\charl\\Documents\\GitHub\\GOST_PublicGoods\\GOSTNets\\GOSTNets')\n",
    "sys.path.append(r'C:\\Users\\charl\\Documents\\GitHub\\GOST')\n",
    "import GOSTnet as gn\n",
    "import importlib\n",
    "import geopandas as gpd\n",
    "import rasterio as rt\n",
    "from rasterio import features\n",
    "from shapely.wkt import loads\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from shapely.geometry import box, Point, Polygon\n",
    "import warnings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pth = r'C:\\Users\\charl\\Documents\\GOST\\Yemen\\graphtool'\n",
    "basepth = r'C:\\Users\\charl\\Documents\\GOST\\Yemen'\n",
    "util_path = r'C:\\Users\\charl\\Documents\\GOST\\Yemen\\util_files'\n",
    "srtm_pth = r'C:\\Users\\charl\\Documents\\GOST\\Yemen\\SRTM'\n",
    "services = ['ALL',\n",
    "            'Antenatal',\n",
    "            'BEmONC',\n",
    "            'CEmONC',\n",
    "            'Under_5',\n",
    "            'Emergency_Surgery',\n",
    "            'Immunizations',\n",
    "            'Malnutrition',\n",
    "            'Int_Outreach']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to add elevation to a point GeoDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_elevation(df, x, y, srtm_pth):\n",
    "    # walk all tiles, find path\n",
    "    \n",
    "    tiles = []\n",
    "    for root, folder, files in os.walk(os.path.join(srtm_pth,'high_res')):\n",
    "        for f in files:\n",
    "            if f[-3:] == 'hgt':\n",
    "                tiles.append(f[:-4])\n",
    "\n",
    "    # load dictionary of tiles\n",
    "    arrs = {}\n",
    "    for t in tiles:\n",
    "        arrs[t] = rt.open(srtm_pth+r'\\high_res\\{}.hgt\\{}.hgt'.format(t, t), 'r')\n",
    "\n",
    "    # assign a code\n",
    "    uniques = []\n",
    "    df['code'] = 'placeholder'\n",
    "    def tile_code(z):\n",
    "        E = str(z[x])[:2]\n",
    "        N = str(z[y])[:2]\n",
    "        return 'N{}E0{}'.format(N, E)\n",
    "    df['code'] = df.apply(lambda z: tile_code(z), axis = 1)\n",
    "    unique_codes = list(set(df['code'].unique()))\n",
    "    \n",
    "    z = {}\n",
    "    # Match on High Precision Elevation\n",
    "    property_name = 'elevation'\n",
    "    for code in unique_codes:\n",
    "        \n",
    "        df2 = df.copy()\n",
    "        df2 = df2.loc[df2['code'] == code]\n",
    "        dataset = arrs[code]\n",
    "        b = dataset.bounds\n",
    "        datasetBoundary = box(b[0], b[1], b[2], b[3])\n",
    "        selKeys = []\n",
    "        selPts = []\n",
    "        for index, row in df2.iterrows():\n",
    "            if Point(row[x], row[y]).intersects(datasetBoundary):\n",
    "                selPts.append((row[x],row[y]))\n",
    "                selKeys.append(index)\n",
    "        raster_values = list(dataset.sample(selPts))\n",
    "        raster_values = [x[0] for x in raster_values]\n",
    "\n",
    "        # generate new dictionary of {node ID: raster values}\n",
    "        z.update(zip(selKeys, raster_values))\n",
    "        \n",
    "    elev_df = pd.DataFrame.from_dict(z, orient='index')\n",
    "    elev_df.columns = ['elevation']\n",
    "    \n",
    "    missing = elev_df.copy()\n",
    "    missing = missing.loc[missing.elevation < 0]\n",
    "    if len(missing) > 0:\n",
    "        missing_df = df.copy()\n",
    "        missing_df = missing_df.loc[missing.index]\n",
    "        low_res_tifpath = os.path.join(srtm_pth, 'clipped', 'clipped_e20N40.tif')\n",
    "        dataset = rt.open(low_res_tifpath, 'r')\n",
    "        b = dataset.bounds\n",
    "        datasetBoundary = box(b[0], b[1], b[2], b[3])\n",
    "        selKeys = []\n",
    "        selPts = []\n",
    "        for index, row in missing_df.iterrows():\n",
    "            if Point(row[x], row[y]).intersects(datasetBoundary):\n",
    "                selPts.append((row[x],row[y]))\n",
    "                selKeys.append(index)\n",
    "        raster_values = list(dataset.sample(selPts))\n",
    "        raster_values = [x[0] for x in raster_values]\n",
    "        z.update(zip(selKeys, raster_values))\n",
    "\n",
    "        elev_df = pd.DataFrame.from_dict(z, orient='index')\n",
    "        elev_df.columns = ['elevation']\n",
    "    df['point_elev'] = elev_df['elevation']\n",
    "    df = df.drop('code', axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to convert distances to walk times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_walktimes(df, start = 'point_elev', end = 'node_elev', dist = 'NN_dist', max_walkspeed = 6, min_speed = 0.1):\n",
    "    # Tobler's hiking function: https://en.wikipedia.org/wiki/Tobler%27s_hiking_function\n",
    "    def speed(incline_ratio, max_speed):\n",
    "        walkspeed = max_speed * np.exp(-3.5 * abs(incline_ratio + 0.05)) \n",
    "        return walkspeed\n",
    "\n",
    "    speeds = {}\n",
    "    times = {}\n",
    "\n",
    "    for index, data in df.iterrows():\n",
    "        if data[dist] > 0:\n",
    "            delta_elevation = data[end] - data[start]\n",
    "            incline_ratio = delta_elevation / data[dist]\n",
    "            speed_kmph = speed(incline_ratio = incline_ratio, max_speed = max_walkspeed)\n",
    "            speed_kmph = max(speed_kmph, min_speed)\n",
    "            speeds[index] = (speed_kmph)\n",
    "            times[index] = (data[dist] / 1000 * 3600 / speed_kmph)\n",
    "\n",
    "    speed_df = pd.DataFrame.from_dict(speeds, orient = 'index')\n",
    "    time_df = pd.DataFrame.from_dict(times, orient = 'index')\n",
    "\n",
    "    df['walkspeed'] = speed_df[0]\n",
    "    df['walk_time'] = time_df[0]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Intersect points with merged districts shapefile, identify relationship\n",
    "\n",
    "def AggressiveSpatialIntersect(points, polygons):\n",
    "    import osmnx as ox\n",
    "    spatial_index = points.sindex\n",
    "    container = {}\n",
    "    cut_geoms = []\n",
    "    for index, row in polygons.iterrows():\n",
    "        polygon = row.geometry\n",
    "        if polygon.area > 0.5:\n",
    "            geometry_cut = ox.quadrat_cut_geometry(polygon, quadrat_width=0.5)\n",
    "            cut_geoms.append(geometry_cut)\n",
    "            print('cutting geometry %s into %s pieces' % (index, len(geometry_cut)))\n",
    "            index_list = []\n",
    "            for P in geometry_cut:\n",
    "                possible_matches_index = list(spatial_index.intersection(P.bounds))\n",
    "                possible_matches = points.iloc[possible_matches_index]\n",
    "                precise_matches = possible_matches[possible_matches.intersects(P)]\n",
    "                if len(precise_matches) > 0:\n",
    "                    index_list.append(precise_matches.index)\n",
    "                flat_list = [item for sublist in index_list for item in sublist]\n",
    "                container[index] = list(set(flat_list))\n",
    "        else:\n",
    "            possible_matches_index = list(spatial_index.intersection(polygon.bounds))\n",
    "            possible_matches = points.iloc[possible_matches_index]\n",
    "            precise_matches = possible_matches[possible_matches.intersects(polygon)]\n",
    "            if len(precise_matches) > 0:\n",
    "                container[index] = list(precise_matches.index)\n",
    "    return container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Zonal Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def zonalStats(inShp, inRaster, bandNum=1, mask_A = None, reProj = False, minVal = '', maxVal = '', verbose=False , rastType='N', unqVals=[]):\n",
    "    import sys, os, inspect, logging, json\n",
    "    import rasterio, affine\n",
    "\n",
    "    import pandas as pd\n",
    "    import geopandas as gpd\n",
    "    import numpy as np\n",
    "\n",
    "    from collections import Counter\n",
    "    from shapely.geometry import box\n",
    "    from affine import Affine\n",
    "    from rasterio import features\n",
    "    from rasterio.mask import mask\n",
    "    from rasterio.features import rasterize\n",
    "    from rasterio.warp import reproject, Resampling\n",
    "    from osgeo import gdal\n",
    "    \n",
    "    ''' Run zonal statistics against an input shapefile\n",
    "    \n",
    "    INPUT VARIABLES\n",
    "    inShp [string or geopandas object] - path to input shapefile\n",
    "    inRaster [string or rasterio object] - path to input raster\n",
    "    \n",
    "    OPTIONAL\n",
    "    bandNum [integer] - band in raster to analyze\n",
    "    reProj [boolean] -  whether to reproject data to match, if not, raise an error\n",
    "    minVal [number] - if defined, will only calculation statistics on values above this number\n",
    "    verbose [boolean] - whether to be loud with responses\n",
    "    rastType [string N or C] - N is numeric and C is categorical. Categorical returns counts of numbers\n",
    "    unqVals [array of numbers] - used in categorical zonal statistics, tabulates all these numbers, will report 0 counts\n",
    "    mask_A [numpy boolean mask] - mask the desired band using an identical shape boolean mask. Useful for doing conditional zonal stats\n",
    "    \n",
    "    RETURNS\n",
    "    array of arrays, one for each feature in inShp\n",
    "    '''   \n",
    "    if isinstance(inShp, str):\n",
    "        inVector = gpd.read_file(inShp) \n",
    "    else:\n",
    "        inVector = inShp\n",
    "    if isinstance(inRaster, str):\n",
    "        curRaster = rasterio.open(inRaster, 'r+')\n",
    "    else:\n",
    "        curRaster = inRaster\n",
    "        \n",
    "    # If mask is not none, apply mask \n",
    "    if mask_A is not None:\n",
    "        \n",
    "        curRaster.write_mask(np.invert(mask_A))\n",
    "    \n",
    "    outputData=[]\n",
    "    if inVector.crs != curRaster.crs:\n",
    "        if reProj:\n",
    "            inVector = inVector.to_crs(curRaster.crs)\n",
    "        else:\n",
    "            raise ValueError(\"Input CRS do not match\")\n",
    "    fCount = 0\n",
    "    tCount = len(inVector['geometry'])\n",
    "    #generate bounding box geometry for raster bbox\n",
    "    b = curRaster.bounds\n",
    "    rBox = box(b[0], b[1], b[2], b[3])\n",
    "    for geometry in inVector['geometry']:\n",
    "        #This test is used in case the geometry extends beyond the edge of the raster\n",
    "        #   I think it is computationally heavy, but I don't know of an easier way to do it\n",
    "        if not rBox.contains(geometry):\n",
    "            geometry = geometry.intersection(rBox)            \n",
    "        try:\n",
    "            fCount = fCount + 1\n",
    "            if fCount % 1000 == 0 and verbose:\n",
    "                tPrint(\"Processing %s of %s\" % (fCount, tCount) )\n",
    "            # get pixel coordinates of the geometry's bounding box\n",
    "            ul = curRaster.index(*geometry.bounds[0:2])\n",
    "            lr = curRaster.index(*geometry.bounds[2:4])\n",
    "            '''\n",
    "            TODO: There is a problem with the indexing - if the shape falls outside the boundaries, it errors\n",
    "                I want to change it to just grab what it can find, but my brain is wrecked and I cannot figure it out\n",
    "            print(geometry.bounds)\n",
    "            print(curRaster.shape)\n",
    "            print(lr)\n",
    "            print(ul)\n",
    "            lr = (max(lr[0], 0), min(lr[1], curRaster.shape[1]))\n",
    "            ul = (min(ul[0], curRaster.shape[0]), min(ul[1]))\n",
    "            '''\n",
    "            # read the subset of the data into a numpy array\n",
    "            window = ((float(lr[0]), float(ul[0]+1)), (float(ul[1]), float(lr[1]+1)))\n",
    "            \n",
    "            if mask is not None:\n",
    "                data = curRaster.read(bandNum, window=window, masked = True)\n",
    "            else:\n",
    "                data = curRaster.read(bandNum, window=window, masked = False)\n",
    "            \n",
    "            # create an affine transform for the subset data\n",
    "            t = curRaster.transform\n",
    "            shifted_affine = Affine(t.a, t.b, t.c+ul[1]*t.a, t.d, t.e, t.f+lr[0]*t.e)\n",
    "\n",
    "            # rasterize the geometry\n",
    "            mask = rasterize(\n",
    "                [(geometry, 0)],\n",
    "                out_shape=data.shape,\n",
    "                transform=shifted_affine,\n",
    "                fill=1,\n",
    "                all_touched=False,\n",
    "                dtype=np.uint8)\n",
    "\n",
    "            # create a masked numpy array\n",
    "            masked_data = np.ma.array(data=data, mask=mask.astype(bool))\n",
    "            if rastType == 'N':                \n",
    "                if minVal != '' or maxVal != '':\n",
    "                    if minVal != '':\n",
    "                        masked_data = np.ma.masked_where(masked_data < minVal, masked_data)\n",
    "                    if maxVal != '':\n",
    "                        masked_data = np.ma.masked_where(masked_data > maxVal, masked_data)                    \n",
    "                    if masked_data.count() > 0:                        \n",
    "                        results = [masked_data.sum(), masked_data.min(), masked_data.max(), masked_data.mean()]\n",
    "                    else :\n",
    "                        results = [-1, -1, -1, -1]                \n",
    "                else:\n",
    "                    results = [masked_data.sum(), masked_data.min(), masked_data.max(), masked_data.mean()]\n",
    "            if rastType == 'C':\n",
    "                if len(unqVals) > 0:                          \n",
    "                    xx = dict(Counter(data.flatten()))\n",
    "                    results = [xx.get(i, 0) for i in unqVals]                \n",
    "                else:\n",
    "                    results = np.unique(masked_data, return_counts=True)                    \n",
    "            outputData.append(results)\n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "            outputData.append([-1, -1, -1, -1])            \n",
    "    return outputData   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Main Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Process(settings_dict):\n",
    "    \n",
    "    print(settings_dict)\n",
    "    walking = settings_dict['walking']\n",
    "    conflict = settings_dict['conflict']\n",
    "    zonal_stats = settings_dict['zonal_stats']\n",
    "    facility_type = settings_dict['facility_type']\n",
    "    year = settings_dict['year']\n",
    "    service_index = settings_dict['service_index']\n",
    "    path_mod = settings_dict['path_mod']\n",
    "    warfronts = settings_dict['warfronts']\n",
    "    \n",
    "    if walking == 1:\n",
    "        type_tag = 'walking'\n",
    "        net_name = r'walk_graph.pickle'\n",
    "    else:\n",
    "        type_tag = 'driving'\n",
    "        net_name = r'G_salty_time_conflict_adj.pickle'\n",
    "\n",
    "    if conflict == 1: \n",
    "        conflict_tag = 'ConflictAdj'\n",
    "        OD_name = r'OD_Jan24th_%s_%s.csv' % (type_tag, year)\n",
    "    else:\n",
    "        conflict_tag = 'NoConflict'\n",
    "        OD_name = r'OD_normal_%s_%s.csv' % (type_tag, year)\n",
    "\n",
    "    OD_pth = pth\n",
    "    net_pth = pth\n",
    "\n",
    "    WGS = {'init':'epsg:4326'}\n",
    "    measure_crs = {'init':'epsg:32638'}\n",
    "\n",
    "    subset = r'%s_HERAMS_%s_%s_%s_%s' % (type_tag, facility_type, services[service_index], conflict_tag, year)\n",
    "    \n",
    "    if warfronts == 0 and conflict == 1:\n",
    "        subset = subset+'_no_warfronts'\n",
    "        \n",
    "    print(\"Output files will have name: \", subset)\n",
    "    print(\"network: \",net_name)\n",
    "    print(\"OD Matrix: \",OD_name)\n",
    "    print(\"Conflict setting: \",conflict_tag)\n",
    "\n",
    "    offroad_speed = 4\n",
    "    \n",
    "    OD = pd.read_csv(os.path.join(OD_pth, OD_name))\n",
    "    OD = OD.rename(columns = {'Unnamed: 0':'O_ID'})\n",
    "    OD = OD.set_index('O_ID')\n",
    "    OD = OD.replace([np.inf, -np.inf], np.nan)\n",
    "    OD_original = OD.copy()\n",
    "    \n",
    "    \n",
    "    ### Optional: Subset to Accepted Nodes\n",
    "    \n",
    "    acceptable_df = pd.read_csv(os.path.join(OD_pth, 'HeRAMS 2018 April_snapped.csv'))    \n",
    "\n",
    "    # Adjust for facility type\n",
    "    if facility_type == 'HOS':\n",
    "        acceptable_df = acceptable_df.loc[acceptable_df['Health Facility Type Coded'].isin(['1',1])]\n",
    "    elif facility_type == 'PHC':\n",
    "        acceptable_df = acceptable_df.loc[acceptable_df['Health Facility Type Coded'].isin([2,'2',3,'3'])]\n",
    "    elif facility_type == 'ALL':\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError('unacceptable facility_type entry!')\n",
    "\n",
    "    # Adjust for functionality in a given year\n",
    "    acceptable_df = acceptable_df.loc[acceptable_df['Functioning %s' % year].isin(['1','2',1,2])]\n",
    "    \n",
    "    Yemn_bound = gpd.read_file(os.path.join(util_path, 'Yemen_bound.shp'))\n",
    "    Yemn_bound = Yemn_bound.to_crs(WGS)\n",
    "    Yemn_bound = Yemn_bound.geometry.iloc[0]\n",
    "\n",
    "    # Adjust for availability of service\n",
    "\n",
    "    SERVICE_DICT = {'Antenatal_2018':'ANC 2018',\n",
    "                   'Antenatal_2016':'Antenatal Care (P422) 2016',\n",
    "                   'BEmONC_2018':'Basic emergency obstetric care 2018',\n",
    "                   'BEmONC_2016':'Basic Emergency Obsteteric Care (P424) 2016',\n",
    "                   'CEmONC_2018':'Comprehensive emergency obstetric care 2018',\n",
    "                   'CEmONC_2016':'Comprehensive Emergency Obstetric Care (S424) 2016',\n",
    "                   'Under_5_2018':'Under 5 clinics 2018',\n",
    "                   'Under_5_2016':'Under-5 clinic services (P23) 2016',\n",
    "                   'Emergency_Surgery_2018':'Emergency and elective surgery 2018',\n",
    "                   'Emergency_Surgery_2016':'Emergency and Elective Surgery (S14) 2016',\n",
    "                   'Immunizations_2018':'EPI 2018',\n",
    "                   'Immunizations_2016':'EPI (P21a) 2016',\n",
    "                   'Malnutrition_2018':'Malnutrition services 2018',\n",
    "                   'Malnutrition_2016':'Malnutrition services (P25) 2016',\n",
    "                   'Int_Outreach_2018':'Integrated outreach (IMCI+EPI+ANC+Nutrition_Services) 2018',\n",
    "                   'Int_Outreach_2016':'Integrated Outreach (P22) 2016'}\n",
    "\n",
    "    if service_index == 0:\n",
    "        pass\n",
    "    else:\n",
    "        acceptable_df = acceptable_df.loc[acceptable_df[SERVICE_DICT['%s_%s' % (services[service_index],year)]].isin(['1',1])]\n",
    "\n",
    "    len(acceptable_df)\n",
    "    \n",
    "    acceptable_df['geometry'] = acceptable_df['geometry'].apply(loads)\n",
    "    acceptable_gdf = gpd.GeoDataFrame(acceptable_df, geometry = 'geometry', crs = {'init':'epsg:4326'})\n",
    "    acceptable_gdf = acceptable_gdf.loc[acceptable_gdf['geometry'].intersects(Yemn_bound)]\n",
    "    accepted_facilities = list(set(list(acceptable_gdf.NN)))\n",
    "    accepted_facilities_str = [str(i) for i in accepted_facilities]\n",
    "    OD = OD_original[accepted_facilities_str]\n",
    "    acceptable_df.to_csv(os.path.join(basepth,'output_layers','Round 3',path_mod,'%s.csv' % subset))\n",
    "    \n",
    "    ### Add elevation for destination nodes\n",
    "    \n",
    "    dest_df = acceptable_df[['NN','NN_dist','Latitude','Longitude']]\n",
    "    dest_df = add_elevation(dest_df, 'Longitude','Latitude', srtm_pth).set_index('NN')\n",
    "    \n",
    "    ### Add elevation from graph nodes (reference)\n",
    "    \n",
    "    G = nx.read_gpickle(os.path.join(OD_pth, net_name))\n",
    "    G_node_df = gn.node_gdf_from_graph(G)\n",
    "    G_node_df = add_elevation(G_node_df, 'x', 'y', srtm_pth)\n",
    "    match_node_elevs = G_node_df[['node_ID','point_elev']].set_index('node_ID')\n",
    "    match_node_elevs.loc[match_node_elevs.point_elev < 0] = 0\n",
    "    \n",
    "    \n",
    "    ### Match on node elevations for dest_df; calculate travel times to nearest node\n",
    "    dest_df['node_elev'] = match_node_elevs['point_elev']\n",
    "    dest_df = generate_walktimes(dest_df, start = 'node_elev', end = 'point_elev', dist = 'NN_dist', max_walkspeed = offroad_speed)\n",
    "    dest_df = dest_df.sort_values(by = 'walk_time', ascending = False)\n",
    "    \n",
    "    \n",
    "    ### Add Walk Time to all travel times in OD matrix\n",
    "    \n",
    "    dest_df = dest_df[['walk_time']]\n",
    "    dest_df.index = dest_df.index.map(str)\n",
    "\n",
    "    d_f = OD.transpose()\n",
    "\n",
    "    for i in d_f.columns:\n",
    "        dest_df[i] = d_f[i]\n",
    "\n",
    "    for i in dest_df.columns:\n",
    "        if i == 'walk_time':\n",
    "            pass\n",
    "        else:\n",
    "            dest_df[i] = dest_df[i] + dest_df['walk_time']\n",
    "\n",
    "    dest_df = dest_df.drop('walk_time', axis = 1)\n",
    "\n",
    "    dest_df = dest_df.transpose()\n",
    "    \n",
    "    \n",
    "    ### Import Shapefile Describing Regions of Control\n",
    "    \n",
    "    if warfronts == 1:\n",
    "        conflict_file = r'merged_dists.shp'\n",
    "    elif warfronts == 0:\n",
    "        conflict_file = r'NoConflict.shp'\n",
    "    merged_dists = gpd.read_file(os.path.join(util_path, conflict_file))\n",
    "    if merged_dists.crs != {'init':'epsg:4326'}:\n",
    "        merged_dists = merged_dists.to_crs({'init':'epsg:4326'})\n",
    "    merged_dists = merged_dists.loc[merged_dists.geometry.type == 'Polygon']\n",
    "    \n",
    "    ### Factor in lines of Control - Import Areas of Control Shapefile\n",
    "    \n",
    "    graph_node_gdf = gn.node_gdf_from_graph(G)\n",
    "    gdf = graph_node_gdf.copy()\n",
    "    gdf = gdf.set_index('node_ID')\n",
    "    possible_snap_nodes = AggressiveSpatialIntersect(graph_node_gdf, merged_dists)\n",
    "    print('**bag of possible node snapping locations has been successfully generated**')\n",
    "\n",
    "    \n",
    "    # Match on network time from origin node (time travelling along network + walking to destination)\n",
    "    \n",
    "    ### Load Grid\n",
    "    if year == 2018:\n",
    "        year_raster = 2018\n",
    "    elif year == 2016:\n",
    "        year_raster = 2015\n",
    "    grid_name = r'origins_1km_%s_snapped.csv' % year_raster\n",
    "    grid = pd.read_csv(os.path.join(OD_pth, grid_name))\n",
    "    grid = grid.rename({'Unnamed: 0':'PointID'}, axis = 1)\n",
    "    grid['geometry'] = grid['geometry'].apply(loads)\n",
    "    grid_gdf = gpd.GeoDataFrame(grid, crs = WGS, geometry = 'geometry')\n",
    "    grid_gdf = grid_gdf.set_index('PointID')\n",
    "    \n",
    "    ### Adjust Nearest Node snapping for War\n",
    "    \n",
    "    origin_container = AggressiveSpatialIntersect(grid_gdf, merged_dists)\n",
    "    print('**bag of possible origins locations has been successfully generated**')\n",
    "\n",
    "    bundle = []\n",
    "    for key in origin_container.keys():\n",
    "        origins = origin_container[key]\n",
    "        possible_nodes = graph_node_gdf.loc[possible_snap_nodes[key]]\n",
    "        origin_subset = grid_gdf.loc[origins]\n",
    "        origin_subset_snapped = gn.pandana_snap_points(origin_subset, \n",
    "                                    possible_nodes, \n",
    "                                    source_crs = 'epsg:4326', \n",
    "                                    target_crs = 'epsg:32638', \n",
    "                                    add_dist_to_node_col = True)\n",
    "        bundle.append(origin_subset_snapped)\n",
    "\n",
    "    grid_gdf_adjusted = pd.concat(bundle)\n",
    "    \n",
    "    grid_gdf = grid_gdf_adjusted\n",
    "    \n",
    "    ### Adjust acceptable destinations for each node for the war\n",
    "    \n",
    "    gdf = graph_node_gdf.copy()\n",
    "    gdf['node_ID'] = gdf['node_ID'].astype('str')\n",
    "    gdf = gdf.loc[gdf.node_ID.isin(list(dest_df.columns))]\n",
    "    gdf = gdf.set_index('node_ID')\n",
    "\n",
    "    dest_container = AggressiveSpatialIntersect(gdf, merged_dists)\n",
    "\n",
    "    gdf = graph_node_gdf.copy()\n",
    "    gdf = gdf.loc[gdf.node_ID.isin(list(dest_df.index))]\n",
    "    gdf = gdf.set_index('node_ID')\n",
    "\n",
    "    origin_snap_container = AggressiveSpatialIntersect(gdf, merged_dists)\n",
    "\n",
    "    bundle = []\n",
    "    for key in origin_snap_container.keys():\n",
    "        origins = origin_snap_container[key]\n",
    "        destinations = dest_container[key]\n",
    "        Q = dest_df[destinations].loc[origins]\n",
    "        Q['min_time'] = Q.min(axis = 1)\n",
    "        Q2 = Q[['min_time']]\n",
    "        bundle.append(Q2)\n",
    "    Q3 = pd.concat(bundle)\n",
    "\n",
    "    dest_df['min_time'] = Q3['min_time']\n",
    "    \n",
    "    ### Return to Normal Process\n",
    "    grid_gdf = grid_gdf.rename(columns = {'NN':'O_ID'})\n",
    "    \n",
    "    ### Merge on min Time\n",
    "    \n",
    "    grid_gdf = grid_gdf.reset_index()\n",
    "    grid_gdf = grid_gdf.set_index(grid_gdf['O_ID'])\n",
    "    grid_gdf['on_network_time'] = dest_df['min_time']\n",
    "    grid_gdf = grid_gdf.set_index('PointID')\n",
    "    \n",
    "    \n",
    "    # Add origin node distance to network - walking time\n",
    "    grid = grid_gdf\n",
    "    grid = add_elevation(grid, 'Longitude','Latitude', srtm_pth)\n",
    "    grid = grid.reset_index()\n",
    "    grid = grid.set_index('O_ID')\n",
    "    grid['node_elev'] = match_node_elevs['point_elev']\n",
    "    grid = grid.set_index('PointID')\n",
    "    grid = generate_walktimes(grid, start = 'point_elev', end = 'node_elev', dist = 'NN_dist', max_walkspeed = offroad_speed)\n",
    "    grid = grid.rename({'node_elev':'nr_node_on_net_elev', \n",
    "                        'walkspeed':'walkspeed_to_net', \n",
    "                        'walk_time':'walk_time_to_net',\n",
    "                       'NN_dist':'NN_dist_to_net'}, axis = 1)\n",
    "    grid['total_time_net'] = grid['on_network_time'] + grid['walk_time_to_net']\n",
    "    \n",
    "    ### Calculate Direct Walking Time (not using road network), vs. network Time\n",
    "    \n",
    "    bundle = []\n",
    "    W = graph_node_gdf.copy()\n",
    "    W['node_ID'] = W['node_ID'].astype(str)\n",
    "    W = W.set_index('node_ID')\n",
    "\n",
    "    locations_gdf = gpd.GeoDataFrame(acceptable_df, geometry = 'geometry', crs = {'init':'epsg:4326'})\n",
    "    locations_container = AggressiveSpatialIntersect(locations_gdf, merged_dists)\n",
    "\n",
    "    for key in origin_container.keys():\n",
    "        origins = origin_container[key]\n",
    "        origin_subset = grid.copy()\n",
    "        origin_subset = origin_subset.loc[origins]\n",
    "        locations = locations_gdf.loc[locations_container[key]]\n",
    "        if len(locations) < 1:\n",
    "            origin_subset['NN'] = None\n",
    "            origin_subset['NN_dist'] = None\n",
    "            bundle.append(origin_subset)\n",
    "        else:\n",
    "            origin_subset_snapped = gn.pandana_snap_points(origin_subset, \n",
    "                                    locations, \n",
    "                                    source_crs = 'epsg:4326', \n",
    "                                    target_crs = 'epsg:32638', \n",
    "                                    add_dist_to_node_col = True)\n",
    "            bundle.append(origin_subset_snapped)\n",
    "\n",
    "    grid_gdf_adjusted = pd.concat(bundle)\n",
    "    \n",
    "    grid = grid_gdf_adjusted\n",
    "    \n",
    "    Y = grid.copy()\n",
    "    objs = []\n",
    "    if len(Y.loc[Y['NN'].isnull() == True]) > 0:\n",
    "        Y2 = Y.loc[Y['NN'].isnull() == True]\n",
    "        Y2['walkspeed_direct'] = 0\n",
    "        Y2['walk_time_direct'] = 9999999\n",
    "        Y2['NN_dist_direct'] = 9999999\n",
    "        objs.append(Y2)\n",
    "\n",
    "    location_elevs = add_elevation(locations_gdf, 'Longitude','Latitude', srtm_pth)\n",
    "    Y1 = Y.loc[Y['NN'].isnull() == False]\n",
    "    Y1['NN'] = Y1['NN'].astype(int)\n",
    "    Y1 = Y1.set_index('NN')\n",
    "    Y1['dest_NN_elev'] = location_elevs['point_elev']\n",
    "\n",
    "    Y1 = Y1.reset_index()\n",
    "    Y1 = generate_walktimes(Y1, start = 'point_elev', end = 'dest_NN_elev', dist = 'NN_dist', max_walkspeed = offroad_speed).reset_index()\n",
    "    Y1 = Y1.rename({'walkspeed':'walkspeed_direct', \n",
    "                        'walk_time':'walk_time_direct',\n",
    "                       'NN_dist':'NN_dist_direct'}, axis = 1)\n",
    "    objs.append(Y1)\n",
    "\n",
    "    grid = pd.concat(objs)\n",
    "\n",
    "    grid['PLOT_TIME_SECS'] = grid[['walk_time_direct','total_time_net']].min(axis = 1)\n",
    "    grid['PLOT_TIME_MINS'] = grid['PLOT_TIME_SECS'] / 60\n",
    "    \n",
    "    ### Burn Raster\n",
    "    if year == 2018:\n",
    "        yr = 18\n",
    "    elif year == 2016:\n",
    "        yr = 15\n",
    "    \n",
    "    rst_fn = os.path.join(pth,'pop18_resampled_clipped.tif')\n",
    "    out_fn = os.path.join(basepth,'output_layers','Round 3',path_mod,'%s.tif' % subset)\n",
    "\n",
    "    # Update metadata\n",
    "    rst = rt.open(rst_fn, 'r')\n",
    "    meta = rst.meta.copy()\n",
    "    D_type = rt.float64\n",
    "    meta.update(compress='lzw', dtype = D_type, count = 2)\n",
    "\n",
    "    with rt.open(out_fn, 'w', **meta) as out:\n",
    "        with rt.open(rst_fn, 'r') as pop:\n",
    "\n",
    "            # this is where we create a generator of geom, value pairs to use in rasterizing\n",
    "            shapes = ((geom,value) for geom, value in zip(grid.geometry, grid.PLOT_TIME_MINS))\n",
    "\n",
    "            population = pop.read(1).astype(D_type)\n",
    "            cpy = population.copy()\n",
    "\n",
    "            travel_times = features.rasterize(shapes=shapes, fill=0, out=cpy, transform=out.transform)\n",
    "\n",
    "            out.write_band(1, population)\n",
    "            out.write_band(2, travel_times)\n",
    "    \n",
    "    if zonal_stats == 0:\n",
    "        pass\n",
    "    else:\n",
    "        for resolution in ['national','district']:\n",
    "            out_fn = os.path.join(r'C:\\Users\\charl\\Documents\\GOST\\Yemen\\output_layers','Round 3',path_mod,'%s.tif' % subset)\n",
    "\n",
    "            sys.path.append(r'C:\\Users\\charl\\Documents\\GitHub\\GOST\\GOSTRocks')\n",
    "\n",
    "            utils = r'C:\\Users\\charl\\Documents\\GOST\\Yemen\\util_files'\n",
    "\n",
    "            yemen_shp_name = os.path.join(utils, r'Yemen_bound.shp')\n",
    "            yemen_shp = gpd.read_file(yemen_shp_name)\n",
    "\n",
    "            if yemen_shp.crs != {'init': 'epsg:4326'}:\n",
    "                yemen_shp = yemen_shp.to_crs({'init': 'epsg:4326'})\n",
    "\n",
    "            district_shp_name = os.path.join(r'C:\\Users\\charl\\Documents\\GOST\\Yemen\\VulnerabilityMatrix', r'VM.shp')\n",
    "            district_shp = gpd.read_file(district_shp_name)\n",
    "\n",
    "            if district_shp.crs != {'init': 'epsg:4326'}:\n",
    "                district_shp = district_shp.to_crs({'init': 'epsg:4326'})\n",
    "\n",
    "            inraster = out_fn\n",
    "            ras = rt.open(inraster, mode = 'r+')\n",
    "            pop = ras.read(1)\n",
    "            tt_matrix = ras.read(2)\n",
    "\n",
    "            if resolution == 'national':\n",
    "                target_shp = yemen_shp\n",
    "            elif resolution == 'district':\n",
    "                target_shp = district_shp\n",
    "\n",
    "            ## First, add on the total population of the district to each district shape\n",
    "\n",
    "            mask_pop = np.ma.masked_where(pop > (200000), pop).mask\n",
    "\n",
    "            base_pop = zonalStats(target_shp, \n",
    "                                    inraster, \n",
    "                                    bandNum = 1,\n",
    "                                    mask_A = mask_pop,\n",
    "                                    reProj = False, \n",
    "                                    minVal = 0,\n",
    "                                    maxVal = np.inf, \n",
    "                                    verbose = True, \n",
    "                                    rastType='N')\n",
    "\n",
    "            cols = ['total_pop','min','max','mean']\n",
    "\n",
    "            temp_df = pd.DataFrame(base_pop, columns = cols)\n",
    "\n",
    "            target_shp['total_pop'] = temp_df['total_pop']\n",
    "            target_shp['total_pop'].loc[target_shp['total_pop'] == -1] = 0\n",
    "\n",
    "            ## Now, calculate the population within a range of time thresholds from the destination set\n",
    "            for time_thresh in [30,60,120, 240]:\n",
    "\n",
    "                mask_obj = np.ma.masked_where(tt_matrix > (time_thresh), tt_matrix).mask\n",
    "\n",
    "                raw = zonalStats(target_shp, \n",
    "                                    inraster, \n",
    "                                    bandNum = 1,\n",
    "                                    mask_A = mask_obj,\n",
    "                                    reProj = False, \n",
    "                                    minVal = 0,\n",
    "                                    maxVal = np.inf, \n",
    "                                    verbose = True, \n",
    "                                    rastType='N')\n",
    "\n",
    "                cols = ['pop_%s' % time_thresh,'min','max','mean']\n",
    "\n",
    "                temp_df = pd.DataFrame(raw, columns = cols)\n",
    "\n",
    "                target_shp['pop_%s' % time_thresh] = temp_df['pop_%s' % time_thresh]\n",
    "                target_shp['pop_%s' % time_thresh].loc[target_shp['pop_%s' % time_thresh] == -1] = 0\n",
    "                target_shp['frac_%s' % time_thresh] = (target_shp['pop_%s' % time_thresh]) / (target_shp['total_pop']).fillna(0)\n",
    "                target_shp['frac_%s' % time_thresh].replace([np.inf, -np.inf], 0)\n",
    "                target_shp['frac_%s' % time_thresh] = target_shp['frac_%s' % time_thresh].fillna(0)\n",
    "\n",
    "            # Save to file\n",
    "\n",
    "            if resolution == 'national':\n",
    "                print('saving national')\n",
    "                outter = target_shp[['total_pop','pop_30','frac_30','pop_60','frac_60','pop_120','frac_120','pop_240','frac_240']]\n",
    "                outter.to_csv(os.path.join(basepth, 'output_layers','Round 3',path_mod,'%s_zonal_%s.csv'% (subset, resolution)))\n",
    "            else:\n",
    "                print('saving district')\n",
    "                target_shp['abs_pop_iso'] = target_shp['total_pop'] - target_shp['pop_30']\n",
    "                target_shp.to_file(os.path.join(basepth, 'output_layers','Round 3',path_mod,'%s_zonal_%s.shp' % (subset, resolution)), driver = 'ESRI Shapefile')\n",
    "\n",
    "\n",
    "    print('\\n**process complete**')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_global_bundle = [\n",
    "         {'walking':0,'conflict':0,'zonal_stats':1,'facility_type':'HOS','year':2018,'service_index':0,'path_mod':'global','warfronts':0},\n",
    "         {'walking':0,'conflict':0,'zonal_stats':1,'facility_type':'HOS','year':2016,'service_index':0,'path_mod':'global','warfronts':0},\n",
    "         {'walking':0,'conflict':0,'zonal_stats':1,'facility_type':'PHC','year':2018,'service_index':0,'path_mod':'global','warfronts':0},          \n",
    "         {'walking':0,'conflict':0,'zonal_stats':1,'facility_type':'PHC','year':2016,'service_index':0,'path_mod':'global','warfronts':0},\n",
    "         {'walking':0,'conflict':1,'zonal_stats':1,'facility_type':'HOS','year':2018,'service_index':0,'path_mod':'global','warfronts':1},\n",
    "         {'walking':0,'conflict':1,'zonal_stats':1,'facility_type':'PHC','year':2018,'service_index':0,'path_mod':'global','warfronts':1},\n",
    "         {'walking':0,'conflict':1,'zonal_stats':1,'facility_type':'HOS','year':2018,'service_index':0,'path_mod':'global','warfronts':0},\n",
    "         {'walking':0,'conflict':1,'zonal_stats':1,'facility_type':'PHC','year':2018,'service_index':0,'path_mod':'global','warfronts':0}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "service_2018_conflict_bundle = []\n",
    "for i in range(1,9):\n",
    "    S = {'walking':0,        \n",
    "          'conflict':1,\n",
    "          'zonal_stats':1,\n",
    "          'facility_type':'ALL',\n",
    "          'year':2018,\n",
    "          'service_index':i,\n",
    "          'path_mod':'Services_2018_conflictadj',\n",
    "          'warfronts':1}\n",
    "    service_2018_conflict_bundle.append(S)\n",
    "\n",
    "service_2018_noconflict_bundle = []\n",
    "for i in range(1,9):\n",
    "    S = {'walking':0,        \n",
    "          'conflict':0,\n",
    "          'zonal_stats':1,\n",
    "          'facility_type':'ALL',\n",
    "          'year':2018,\n",
    "          'service_index':i,\n",
    "          'path_mod':'Services_2018_noconflict',\n",
    "          'warfronts':0}\n",
    "    service_2018_noconflict_bundle.append(S)\n",
    "    \n",
    "service_2016_bundle = []\n",
    "for i in range(1,9):\n",
    "    S = {'walking':0,        \n",
    "          'conflict':0,\n",
    "          'zonal_stats':1,\n",
    "          'facility_type':'ALL',\n",
    "          'year':2016,\n",
    "          'service_index':i,\n",
    "          'path_mod':'Services_2016_noconflict',\n",
    "          'warfronts':0}\n",
    "    service_2016_bundle.append(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'walking': 0, 'conflict': 1, 'zonal_stats': 1, 'facility_type': 'HOS', 'year': 2018, 'service_index': 0, 'path_mod': 'global', 'warfronts': 1}\n",
      "Output files will have name:  driving_HERAMS_HOS_ALL_ConflictAdj_2018\n",
      "network:  G_salty_time_conflict_adj.pickle\n",
      "OD Matrix:  OD_Jan24th_driving_2018.csv\n",
      "Conflict setting:  ConflictAdj\n",
      "cutting geometry 56 into 121 pieces\n",
      "cutting geometry 59 into 47 pieces\n",
      "cutting geometry 78 into 236 pieces\n",
      "cutting geometry 79 into 16 pieces\n",
      "**bag of possible node snapping locations has been successfully generated**\n",
      "cutting geometry 56 into 121 pieces\n",
      "cutting geometry 59 into 47 pieces\n",
      "cutting geometry 78 into 236 pieces\n",
      "cutting geometry 79 into 16 pieces\n",
      "**bag of possible origins locations has been successfully generated**\n",
      "cutting geometry 56 into 121 pieces\n",
      "cutting geometry 59 into 47 pieces\n",
      "cutting geometry 78 into 236 pieces\n",
      "cutting geometry 79 into 16 pieces\n",
      "cutting geometry 56 into 121 pieces\n",
      "cutting geometry 59 into 47 pieces\n",
      "cutting geometry 78 into 236 pieces\n",
      "cutting geometry 79 into 16 pieces\n",
      "cutting geometry 56 into 121 pieces\n",
      "cutting geometry 59 into 47 pieces\n",
      "cutting geometry 78 into 236 pieces\n",
      "cutting geometry 79 into 16 pieces\n",
      "saving national\n",
      "saving district\n",
      "\n",
      "**process complete**\n",
      "{'walking': 0, 'conflict': 1, 'zonal_stats': 1, 'facility_type': 'PHC', 'year': 2018, 'service_index': 0, 'path_mod': 'global', 'warfronts': 1}\n",
      "Output files will have name:  driving_HERAMS_PHC_ALL_ConflictAdj_2018\n",
      "network:  G_salty_time_conflict_adj.pickle\n",
      "OD Matrix:  OD_Jan24th_driving_2018.csv\n",
      "Conflict setting:  ConflictAdj\n",
      "cutting geometry 56 into 121 pieces\n",
      "cutting geometry 59 into 47 pieces\n",
      "cutting geometry 78 into 236 pieces\n",
      "cutting geometry 79 into 16 pieces\n",
      "**bag of possible node snapping locations has been successfully generated**\n",
      "cutting geometry 56 into 121 pieces\n",
      "cutting geometry 59 into 47 pieces\n",
      "cutting geometry 78 into 236 pieces\n",
      "cutting geometry 79 into 16 pieces\n",
      "**bag of possible origins locations has been successfully generated**\n",
      "cutting geometry 56 into 121 pieces\n",
      "cutting geometry 59 into 47 pieces\n",
      "cutting geometry 78 into 236 pieces\n",
      "cutting geometry 79 into 16 pieces\n",
      "cutting geometry 56 into 121 pieces\n",
      "cutting geometry 59 into 47 pieces\n",
      "cutting geometry 78 into 236 pieces\n",
      "cutting geometry 79 into 16 pieces\n",
      "cutting geometry 56 into 121 pieces\n",
      "cutting geometry 59 into 47 pieces\n",
      "cutting geometry 78 into 236 pieces\n",
      "cutting geometry 79 into 16 pieces\n",
      "saving national\n",
      "saving district\n",
      "\n",
      "**process complete**\n"
     ]
    }
   ],
   "source": [
    "warnings.simplefilter('ignore')\n",
    "for bundle in [new_global_bundle[4],new_global_bundle[5]]:\n",
    "    Process(bundle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "warnings.simplefilter('ignore')\n",
    "for bundle in service_2018_conflict_bundle:\n",
    "    Process(bundle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "warnings.simplefilter('ignore')\n",
    "for bundle in service_2018_noconflict_bundle:\n",
    "    Process(bundle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "warnings.simplefilter('ignore')\n",
    "for bundle in service_2016_bundle:\n",
    "    Process(bundle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
