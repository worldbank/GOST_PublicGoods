{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Final Net Preparation\n",
    "\n",
    "In this script we take the cleaned road network and finish our preparations ahead of the OD matrix generation. We do the following: \n",
    "- Subset the graph to the largest two subgraphs (one for the mainland, and one for Socotra)\n",
    "- Salt the network to 2km (break up edges longer than 2km into 2km chunks)\n",
    "- Add travel time for each edge\n",
    "- Add a particular road which is missing but clearly visible from satellite imagery\n",
    "- Perform snapping of both origins and destinations, and send copies to the graphtool folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the usual suspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "networkx version: 2.1 \n",
      "osmnx version: 0.7.4 \n",
      "networkx version: 2.1 \n",
      "osmnx version: 0.7.4 \n"
     ]
    }
   ],
   "source": [
    "import os, sys, time\n",
    "sys.path.append(r'C:\\Users\\charl\\Documents\\GitHub\\GOST_PublicGoods\\GOSTNets\\GOSTNets')\n",
    "import importlib\n",
    "import GOSTnet as gn\n",
    "importlib.reload(gn)\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from shapely.wkt import loads\n",
    "from shapely.geometry import Point, LineString, MultiLineString\n",
    "import networkx as nx\n",
    "import osmnx as ox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the folder paths and filenames that we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network path and network name. npickle must be a gpickle type object\n",
    "npath = r'C:\\Users\\charl\\Documents\\GOST\\Yemen\\YEM\\Round 3\\output'\n",
    "npickle = 'YEM_processed.pickle'\n",
    "\n",
    "# basepath\n",
    "bpath = r'C:\\Users\\charl\\Documents\\GOST\\Yemen'\n",
    "\n",
    "# origin path \n",
    "opath = os.path.join(bpath, 'origins')\n",
    "\n",
    "# destinations path\n",
    "dpath = os.path.join(bpath, 'facility_files')\n",
    "\n",
    "# write path for outputs\n",
    "wpath = r'C:\\Users\\charl\\Documents\\GOST\\Yemen\\YEM\\Round 3\\graphtool'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loads saved graph from pickle\n",
    "In this section we take the largest two subgraphs of the network (for the mainland and the island of Socotra respectively) and keep only these roads going forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290862\n"
     ]
    }
   ],
   "source": [
    "# load in our graph as G\n",
    "G = nx.read_gpickle(os.path.join(npath, npickle))\n",
    "\n",
    "# make a big list of all the strongly connected components in G\n",
    "list_of_subgraphs = list(nx.strongly_connected_component_subgraphs(G))\n",
    "\n",
    "# G_2 is our biggest graph. We abstract him separately as our basegraph\n",
    "G_2 = list_of_subgraphs[0]\n",
    "\n",
    "# out list of small subgraphs is everything else \n",
    "list_of_small_subgraphs = list_of_subgraphs[1:]\n",
    "\n",
    "# we open some empty lists for the nodes and edges of the small subgraphs that we want to retain\n",
    "node_bunch = []\n",
    "edge_bunch = []\n",
    "\n",
    "# moving through each subgraph, \n",
    "for subgraph in list_of_small_subgraphs:\n",
    "    \n",
    "    # if the subgraphs have more than 50 nodes (and we aren't for some reason looking at G_2 our big boy)\n",
    "    if subgraph.number_of_nodes() > 50 and subgraph.number_of_nodes() != G_2.number_of_nodes():\n",
    "        \n",
    "        # add the nodes to the nodebunch\n",
    "        for u, data in subgraph.nodes(data = True):\n",
    "            node_bunch.append((u, data))\n",
    "        \n",
    "        # add the edges to the edgebunch\n",
    "        for u, v, data in subgraph.edges(data = True):\n",
    "            edge_bunch.append((u, v, data))\n",
    "\n",
    "# add to G_2, our big base boy, the additional edges            \n",
    "G_2.add_nodes_from(node_bunch)\n",
    "G_2.add_edges_from(edge_bunch)\n",
    "\n",
    "# reset G, our graph, to our modified G_2\n",
    "G = G_2\n",
    "\n",
    "# print edge count and save down\n",
    "print(G.number_of_edges())\n",
    "gn.save(G, 'G', wpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Salting\n",
    "Here, we break up long edges of the graph into sections of length no more than 'thresh'.\n",
    "We do this to significantly improve the accuracy of snapping points off-network on to the network at the closest point. The GOSTnets function 'salt_long_lines' automatically makes these changes to the network for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: \"factor behavior has changed! now divides rather than multiplies. This change brings gn.salt_long_lines into line with gn.convert_network_to_time\" \n",
      "Identified 6063 unique edge(s) longer than 2000. \n",
      "Beginning new node creation...\n",
      "44286 new edges added and 12125 removed to bring total edges to 323023\n",
      "16080 new nodes added to bring total nodes to 123976\n",
      "check: salting process has left number of connected components unchanged\n",
      "2  |  2\n"
     ]
    }
   ],
   "source": [
    "# call the sale long lines function which does the hard work for us\n",
    "G_salty = gn.salt_long_lines(G, \n",
    "                             'epsg:4326', \n",
    "                             'epsg:32638', \n",
    "                             thresh = 2000, \n",
    "                             factor = 1000, \n",
    "                             attr_list = ['infra_type','id','country','osm_id','Type'])\n",
    "\n",
    "# reconvert the nodes IDs to intergers (just useful to do from time to time, and v quick)\n",
    "G_salty = nx.convert_node_labels_to_integers(G_salty)\n",
    "\n",
    "# this is important. We want the salting process to have not disrupted connectivity! Check that here. \n",
    "print('check: salting process has left number of connected components unchanged')\n",
    "print(len(list(nx.strongly_connected_component_subgraphs(G))),\n",
    "      ' | ', \n",
    "      len(list(nx.strongly_connected_component_subgraphs(G_salty))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Travel Time for each edge\n",
    "\n",
    "The travel time can be added to the graph by correctly calling GOSTnets' convert_network_to_time() function with a speed limit dictionary, detailing the choice of speed limits for each type of road. As Yemen is in pretty bad shape, these speeds are kept low (all values in km/h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding traverse time edge property...\n",
      "(0, 35636, {'Wkt': 'LINESTRING (44.2165745 15.3646484, 44.2167032 15.3659494, 44.2167203 15.3663506, 44.2167763 15.3673335, 44.2167973 15.3681592)', 'id': 24386, 'infra_type': 'secondary', 'osm_id': 108470243, 'country': 'YEM', 'key': 'edge_24386', 'length': 389.3715222365589, 'Type': 'legitimate', 'time': 28.03474960103224, 'mode': 'drive'})\n",
      "(0, 74084, {'Wkt': 'LINESTRING (44.2167973 15.3681592, 44.2168027 15.3690043, 44.2168968 15.3701596)', 'id': 24387, 'infra_type': 'secondary', 'osm_id': 108470243, 'country': 'YEM', 'key': 'edge_24387', 'length': 221.74928659443168, 'Type': 'legitimate', 'time': 15.96594863479908, 'mode': 'drive'})\n"
     ]
    }
   ],
   "source": [
    "print('adding traverse time edge property...')\n",
    "# define speed limit dictionary\n",
    "speed_dict = {\n",
    "    'residential':25,\n",
    "    'unclassified':15,\n",
    "    'track':15,\n",
    "    'tertiary':40,\n",
    "    'secondary':50,\n",
    "    'primary':60,\n",
    "    'trunk':50,\n",
    "    'service':15,\n",
    "    'road':15,\n",
    "    'trunk_link':50,\n",
    "    'secondary_link':50,\n",
    "    'primary_link':60,\n",
    "    'tertiary_link':40}\n",
    "\n",
    "# add traverse time property into 'time' edge attribute \n",
    "G_salty_time = gn.convert_network_to_time(G_salty, \n",
    "                                          distance_tag = 'length', \n",
    "                                          road_col = 'infra_type', \n",
    "                                          graph_type = 'drive', \n",
    "                                          speed_dict = speed_dict, \n",
    "                                          walk_speed = 4,\n",
    "                                          factor = 1000\n",
    "                                         )\n",
    "\n",
    "# print out an example edge to check that it has worked as intended\n",
    "gn.example_edge(G_salty_time, 2)\n",
    "\n",
    "# save down the resultant graph with a name that describes the processes that have been done to it\n",
    "gn.save(G_salty_time, 'G_salty_time', wpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Addition of Missing Roads\n",
    "In the specific Yemen context, the border cuts off one crucial road link in the desert towards the northernmost border. Here, we manually re-add this road back to the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "missed_edges = []\n",
    "\n",
    "# NOTE: these start and end node IDs were identified by manual observation of the road network in QGIS\n",
    "st_node = 21793\n",
    "end_node = 114778\n",
    "st_point = Point(G_salty_time.nodes()[st_node]['x'],G_salty_time.nodes()[st_node]['y'])\n",
    "end_point = Point(G_salty_time.nodes()[end_node]['x'],G_salty_time.nodes()[end_node]['y'])\n",
    "lin = LineString([st_point,end_point])\n",
    "\n",
    "# again, a measurement made in QGIS\n",
    "real_length = 115\n",
    "\n",
    "data = {'Wkt':lin,\n",
    "       'id':max(nx.get_edge_attributes(G_salty_time,'id').values())+1,\n",
    "       'infra_type':'service',\n",
    "       'country':'YEM',\n",
    "       'key':'manual_edge_1',\n",
    "       'length':real_length, \n",
    "       'Type':'manual_edge',\n",
    "       'time':float((real_length / 15)), # we assume a speed of 15km/h across this road\n",
    "       'mode':'drive'}\n",
    "\n",
    "# we need to add a bidirectional edge - so we add the same data, but flip the start and end node positions\n",
    "missed_edges.append((st_node, end_node, data))\n",
    "missed_edges.append((end_node, st_node, data))\n",
    "\n",
    "# add on the missed edges\n",
    "G_salty_time.add_edges_from(missed_edges)\n",
    "\n",
    "# re-ID the nodes\n",
    "G_salty = nx.convert_node_labels_to_integers(G_salty)\n",
    "\n",
    "# save\n",
    "gn.save(G_salty_time, 'G_salty_time', wpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snap Destination Files to the Network\n",
    "standard call of GOSTnets' Pandana Snap. soem controls are done on the GPS coordinates to ensure that no facilities with wildly off GPS coordinates are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5042\n",
      "5042\n",
      "4411\n",
      "time elapsed: 12 seconds\n"
     ]
    }
   ],
   "source": [
    "dfiles = ['HeRAMS 2018 April.csv']\n",
    "\n",
    "for dfile in dfiles:\n",
    "    \n",
    "    # Read in\n",
    "    dest_df = pd.read_csv(os.path.join(os.path.join(dpath, dfile)), encoding = \"ISO-8859-1\")\n",
    "    \n",
    "    # Ensure coordinates are floats\n",
    "    dest_df.Longitude = dest_df.Longitude.astype(float)\n",
    "    dest_df.Latitude = dest_df.Latitude.astype(float)\n",
    "    \n",
    "    # Drop entries with no coordinates    \n",
    "    dest_df2 = dest_df.copy()\n",
    "    dest_df2 = dest_df2.loc[(dest_df2.Longitude != 0)]\n",
    "    dest_df2 = dest_df2.loc[(dest_df2.Longitude != None)]\n",
    "    print(len(dest_df))\n",
    "    \n",
    "    # drop all hospitals with GPS coordiantes outside Yemen / outside this mortal plane\n",
    "    dest_df2 = dest_df2.loc[(dest_df2.Longitude <= 60)]\n",
    "    dest_df2 = dest_df2.loc[(dest_df2.Longitude >= 35)]\n",
    "    dest_df2 = dest_df2.loc[(dest_df2.Latitude <= 30)]\n",
    "    dest_df2 = dest_df2.loc[(dest_df2.Latitude >= 5)]\n",
    "    dest_df = dest_df2\n",
    "    print(len(dest_df))\n",
    "    \n",
    "    # Generate Geometries\n",
    "    dest_df['geometry'] = list(zip(dest_df.Longitude, dest_df.Latitude))\n",
    "    dest_df['geometry'] = dest_df['geometry'].apply(Point)\n",
    "    dest_df = gpd.GeoDataFrame(dest_df, geometry = 'geometry', crs = {'init':'espg:4326'})\n",
    "    \n",
    "    # Perform snap\n",
    "    time.ctime()\n",
    "    start = time.time()\n",
    "    df = gn.pandana_snap(G_salty_time, dest_df, 'epsg:4326','epsg:32638', add_dist_to_node_col = True)\n",
    "    \n",
    "    # Save to file\n",
    "    df.to_csv(os.path.join(wpath, dfile.replace('.csv', '_snapped.csv')))\n",
    "    df.to_csv(os.path.join(dpath, dfile.replace('.csv', '_snapped.csv')))\n",
    "\n",
    "    print('time elapsed: %d seconds' % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snap Origin Points to Networks\n",
    "This time, we snap the origins to the network. This takes a lot longer (there are far more origins than destinations!). Be patient - this only has to be done once for each file in the workflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning snap\n",
      "Time elapsed: 161 seconds\n",
      "Beginning snap\n",
      "Time elapsed: 164 seconds\n"
     ]
    }
   ],
   "source": [
    "ofile_A = r'origins_1km_2015.csv'\n",
    "ofile_B = r'origins_1km_2018.csv'\n",
    "ofiles = [ofile_A,ofile_B]\n",
    "\n",
    "for ofile in ofiles:\n",
    "    \n",
    "    # Read in\n",
    "    dest_df = pd.read_csv(os.path.join(os.path.join(opath, ofile)), encoding = \"ISO-8859-1\")\n",
    "    dest_df['geometry'] = dest_df['geometry'].apply(loads)\n",
    "    dest_df = gpd.GeoDataFrame(dest_df, geometry = 'geometry', crs = {'init':'espg:4326'})\n",
    "    \n",
    "    # Perform snap\n",
    "    print('Beginning snap')\n",
    "    \n",
    "    time.ctime()\n",
    "    start = time.time()\n",
    "    df = gn.pandana_snap(G_salty_time, dest_df, 'epsg:4326','epsg:32638', add_dist_to_node_col = True)\n",
    "    \n",
    "    # Save to file\n",
    "    df.to_csv(os.path.join(wpath, ofile.replace('.csv', '_snapped.csv')))\n",
    "    df.to_csv(os.path.join(opath, ofile.replace('.csv', '_snapped.csv')))\n",
    "\n",
    "\n",
    "    print('Time elapsed: %d seconds' % (time.time() - start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
