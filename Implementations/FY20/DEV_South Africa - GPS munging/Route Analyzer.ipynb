{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Route Analyzer\n",
    "The objective of this script was to identify the Origin-Destination matrix out of Khayelitsha, Cape Town, South Africa, from GPS traces of vehicles operating in the area. \n",
    "\n",
    "Data was organized into DataFrames where the unit of analysis was trips, and data where the unit of analysis was the passenger (i.e. multiple passengers pere trip). These could be tied together using a key field, which was a unique trip ID. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the usual suspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os, time\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, LineString\n",
    "from shapely.ops import unary_union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Raw File Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# file path\n",
    "p = r'C:\\Users\\charl\\Documents\\GOST\\South Africa\\Connectivity\\Submission_15_12_2018'\n",
    "\n",
    "# file name\n",
    "q = r'Recon_December.xlsx'\n",
    "\n",
    "# We import different sheets of the MS Excel document as different DataFrames\n",
    "Trips = pd.read_excel(os.path.join(p,q), sheet_name = 'Trips')\n",
    "Passengers = pd.read_excel(os.path.join(p,q), sheet_name = 'Passengers')\n",
    "\n",
    "# A separate doc, the RouteMaster, is also brought in. This MS Excel included data on the geometry of the routes\n",
    "# themselves, but little else. All trips were made using standardized routes. \n",
    "RouteMaster = pd.read_excel(os.path.join(p,r'Route O-D_Master_21December2018.xlsx'))\n",
    "\n",
    "# We only really need the origin and destination for each route to build our OD\n",
    "RouteMaster = RouteMaster[[\"UNIQUE ROUTE ID's\",'ORIGIN','DESTINATION']].set_index(\"UNIQUE ROUTE ID's\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match on useful fields from Trips frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write a function to convert the timestamp field into an int, expressed in minutes\n",
    "def convert(x):\n",
    "    try:\n",
    "        return (x.minute + x.hour * 60 + x.second / 60)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# apply this to the timestamp field, 'Travel Time', to get back a new field, expressed in minutes\n",
    "Passengers['TT_minutes'] = Passengers['Travel Time'].apply(lambda x: convert(x))\n",
    "\n",
    "# Using the key field of the Trip ID, we move on the fields of interest from the Trips DataFrame to the Passenger DataFrame. \n",
    "Trips = Trips.set_index('Trip ID')\n",
    "Passengers = Passengers.set_index('Trip ID')\n",
    "for i in ['Route Description','Start Coordinate', 'End Coordinate', 'Revenue', 'Start Time', 'Total Passengers', 'Distance']:\n",
    "          Passengers['Trip %s' % i] = Trips[i]\n",
    "        \n",
    "# We reset the index to release the Trip ID after this process has been completed. \n",
    "Passengers = Passengers.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match on useful fields from RouteMaster frame\n",
    "Similarly, before we get started in earnest, we want to consolidate all of the useful fields into a single DataFrame that can be the basis of analysis for the rest of the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To differentiate between the origin of the passenger and the origin of the trip itself,\n",
    "# we generate a new column 'Trip Origin' and a new column 'Trip Destination'.\n",
    "Passengers = Passengers.set_index('Trip Route Description')\n",
    "Passengers['Trip Origin'] = RouteMaster['ORIGIN']\n",
    "Passengers['Trip Destination'] = RouteMaster['DESTINATION']\n",
    "Passengers = Passengers.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Handling: Remove Passengers where travel time, fare or trip distance values are erroneous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The thresholds can of course be flexxed. I used 1000 minutes as a too-long journey, and discarded any trips which either didn't have a fare or a fare more than 1,000 rand. A distance one was also added and can be un-commented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Passengers = Passengers.loc[(Passengers['TT_minutes'] < 1000) & (Passengers['TT_minutes'] > 0)]\n",
    "Passengers = Passengers.loc[(Passengers['Fare'] < 1000) & (Passengers['Fare'] > 0)]\n",
    "#Passengers = Passengers.loc[(Passengers['Trip Distance'] < 1000) & (Passengers['Trip Distance'] > 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add fare per minute / unit distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Passengers['Fare_per_minute'] = Passengers['Fare'] / Passengers['TT_minutes']\n",
    "Passengers['Fare_per_mile'] = Passengers['Fare'] / Passengers['Trip Distance']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passengers: Add on ward details for boarding / alightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we import the ward shapefile. The unit of analysis we want to summaize to is the ward, a city level administrative unit. \n",
    "shp_pth = r'C:\\Users\\charl\\Documents\\GOST\\South Africa\\Connectivity\\Fwd__SAL\\1_Residential_SAL'\n",
    "ward_shp = gpd.read_file(os.path.join(shp_pth, 'wards_4326.shp'))\n",
    "ward_shp = ward_shp[['SL_WARD_KE','SL_SUB_CNC','geometry']]\n",
    "\n",
    "# We generate a new unique passenger ID field from the current DataFrame index\n",
    "Passengers['P_ID'] = Passengers.index\n",
    "\n",
    "# we want to know the ward of boarding and ward of alighting for each passenger. \n",
    "# so, we create a lightweight dataframe with just the essentials:\n",
    "Bind_P = Passengers[['P_ID','Boarding Location','Alighting Location']]\n",
    "\n",
    "def convert_to_point(x):\n",
    "    l = x.split(', ')\n",
    "    return Point(float(l[0]), float(l[1]))\n",
    "    \n",
    "Bind_P['Boarding Point'] = Bind_P['Boarding Location'].apply(lambda x: convert_to_point(x))\n",
    "Bind_P['Alighting Point'] = Bind_P['Alighting Location'].apply(lambda x: convert_to_point(x))\n",
    "\n",
    "# make it a GeoDataFrame...\n",
    "Q = gpd.GeoDataFrame(Bind_P, crs = ward_shp.crs, geometry = 'Boarding Point')\n",
    "\n",
    "# and then spatially join this to the ward shapefile.\n",
    "# this is fairly quick as it is a point to poylgon join, but could be made faster via a spatial index if necessary\n",
    "boarding_join = gpd.sjoin(Q, ward_shp, how = 'left')\n",
    "\n",
    "# we drop the geometry info post-join to just retain the passenger ID, and the boarding ward's identifying info\n",
    "boarding_join = boarding_join[['P_ID','SL_WARD_KE','SL_SUB_CNC']]\n",
    "\n",
    "# rename for visibility's sake\n",
    "boarding_join.columns = ['P_ID','Board_WARD_KE','Board_SUB_CNC']\n",
    "\n",
    "# set unique passenger ID as index, and wait. Leave to cool for 20s. \n",
    "boarding_join = boarding_join.set_index('P_ID')\n",
    "\n",
    "# we now repeat the same process, but for alightning. Note the tactical choice of geometry field in the Q2 line:\n",
    "Q2 = gpd.GeoDataFrame(Bind_P, crs = ward_shp.crs, geometry = 'Alighting Point')\n",
    "alighting_join = gpd.sjoin(Q2, ward_shp, how = 'left')\n",
    "alighting_join = alighting_join[['P_ID','SL_WARD_KE','SL_SUB_CNC']]\n",
    "alighting_join.columns = ['P_ID','Alight_WARD_KE','Alight_SUB_CNC']\n",
    "alighting_join = alighting_join.set_index('P_ID')\n",
    "\n",
    "# Now, we have two super light DFs - the alighting_join, and the boarding join - which we match back on to the main Passenger DF:\n",
    "Passengers = Passengers.set_index('P_ID')\n",
    "Passengers['Alight_WARD_KE'] = alighting_join['Alight_WARD_KE']\n",
    "Passengers['Alight_SUB_CNC'] = alighting_join['Alight_SUB_CNC']\n",
    "Passengers['Board_WARD_KE'] = boarding_join['Board_WARD_KE']\n",
    "Passengers['Board_SUB_CNC'] = boarding_join['Board_SUB_CNC']\n",
    "\n",
    "# finish up by reseting the index. Nice. \n",
    "Passengers = Passengers.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passengers: Fare Summary Output Tables\n",
    "Until now, we have concerned ourselves with data preparation and manipulation; this is the first bit of analysis we do. \n",
    " \n",
    "\n",
    "We generate a number of different tables, summarized by various demographic factors, e.g. ethnicity, age, gender, etc. \n",
    "\n",
    "Pandas' groupby functionality allows summarzing across multiple dimensions at once - so we can compare stats for white, elderly, men against young, female people of color if we so choose.\n",
    "\n",
    "Scroll to the right to see how we group each table - that is the key bit here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ethnicity_table = Passengers[['Ethnicity', 'Age Group', 'Gender', 'Fare', 'Fare_per_minute', 'Fare_per_mile', 'TT_minutes']].groupby(['Ethnicity']).median()\n",
    "age_table = Passengers[['Ethnicity', 'Age Group', 'Gender', 'Fare', 'Fare_per_minute', 'Fare_per_mile', 'TT_minutes']].groupby(['Age Group']).median()\n",
    "gender_table = Passengers[['Ethnicity', 'Age Group', 'Gender', 'Fare', 'Fare_per_minute', 'Fare_per_mile', 'TT_minutes']].groupby(['Gender']).median()\n",
    "gender_ethnicity_table = Passengers[['Ethnicity', 'Age Group', 'Gender', 'Fare', 'Fare_per_minute', 'Fare_per_mile', 'TT_minutes']].groupby(['Gender','Ethnicity']).median()\n",
    "age_ethnicity_table = Passengers[['Ethnicity', 'Age Group', 'Gender', 'Fare', 'Fare_per_minute', 'Fare_per_mile', 'TT_minutes']].groupby(['Age Group','Ethnicity']).median()\n",
    "full_table = Passengers[['Ethnicity', 'Age Group', 'Gender', 'Fare', 'Fare_per_minute', 'Fare_per_mile', 'TT_minutes']].groupby(['Gender','Age Group','Ethnicity']).median()\n",
    "\n",
    "# we create a bag of our summary tables for ease of keeping them together. \n",
    "tables = [ethnicity_table, age_table, gender_table, gender_ethnicity_table, age_ethnicity_table, full_table]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trip level analytics\n",
    "Having prepared our data and created some basic summary analytics, we can now do some trip-level analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate list of unique trips, via python's built in set() function\n",
    "unique_trips = list(set(Passengers['Trip Route Description']))\n",
    "\n",
    "# Here, I subset this for just the first 10 in the list\n",
    "Selected_trips = unique_trips[:10]\n",
    "\n",
    "# open a blank list to append results to\n",
    "trip_tables = []\n",
    "\n",
    "# for each trip...\n",
    "for t in Selected_trips:\n",
    "    \n",
    "    # pick out all the passengers in the table with the same trip route description \n",
    "    Q = Passengers.loc[Passengers['Trip Route Description'].isin([t])]\n",
    "    \n",
    "    # NOTE - these are the official race brackets as determined by the South African Govt - not my choice\n",
    "    ethnicities = ['BLACK','INDIAN','OTHER','WHITE','COLOURED']\n",
    "    \n",
    "    genders = ['M','F']\n",
    "    ages = ['YOUNG','MIDDLE','OLD']\n",
    "    \n",
    "    # we take a copy of the relevant passenger data for this trip and assign it to 's'.\n",
    "    s = Q.copy()\n",
    "    \n",
    "    # here we are creating a series of flag columns for each of our demographic indicators. \n",
    "    # values are either 1 (true) or 0 (false) for each demographic option. These are mutually exclusive within their lists. \n",
    "    for gend in genders:\n",
    "        s[gend] = 0\n",
    "        s[gend].loc[s['Gender'] == gend] = 1\n",
    "    for ethn in ethnicities:\n",
    "        s[ethn] = 0\n",
    "        s[ethn].loc[s['Ethnicity'] == ethn] = 1\n",
    "    for age in ages:\n",
    "        s[age] = 0\n",
    "        s[age].loc[s['Age Group'] == age] = 1\n",
    "        \n",
    "    # We also create a dummy total column, which is 1 for all persons (everyone either M or F in this dataset)\n",
    "    s['Total'] = s['M'] + s['F']\n",
    "    col_keep = []\n",
    "    \n",
    "    # we add an 'hour of the day' column for the hour in which the trip is started. \n",
    "    s['Trip_st_hour'] = s['Trip Start Time'].apply(lambda x: x.hour)\n",
    "    \n",
    "    # we group records by this value - i.e. the time at which people were starting the trip. \n",
    "    # now, each 'row' represents many people. \n",
    "    # the lambda function we are applying is the sum - so we have the 'count' of people in each bracket undertaking \n",
    "    # the trip at this time (remember we generated these 1 / 0 cols above)\n",
    "    s = s.groupby('Trip_st_hour').sum()\n",
    "    \n",
    "    # we don't have to sort but it is pleasing to do so\n",
    "    s = s.sort_values(by = 'Trip_st_hour', ascending = True)\n",
    "    \n",
    "    # we create fractional columns that allow us to easily see the age, gender and ethnicity breakdown of passengers \n",
    "    # for each hour the trip occured in the master Passenger DataFrame. \n",
    "    for ethn in ethnicities:\n",
    "        s['% {}'.format(ethn)] = s[ethn] / (s['Total'])\n",
    "        col_keep.append('% {}'.format(ethn))\n",
    "    for gend in genders:\n",
    "        s['% {}'.format(gend)] = s[gend] / (s['Total'])\n",
    "        col_keep.append('% {}'.format(gend))\n",
    "    for age in ages:\n",
    "        s['% {}'.format(age)] = s[age] / (s['Total'])\n",
    "        col_keep.append('% {}'.format(age)) \n",
    "        \n",
    "    # We subset the DF to the columns we are interested in using col_keep\n",
    "    s = s[['M','F',*col_keep,]]\n",
    "    \n",
    "    # we append the results DataFrame to trip_tables\n",
    "    trip_tables.append(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Route Summary: Initial Generation\n",
    "Now, instead of looking at individual trips, we move up to the route level - and analyze the data at the level of routes (multiple trips).\n",
    "\n",
    "We perform much the same analysis as the above block here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# we generate a route summary by grouping Passengers by Trip Description, again. We take the median values and sort by fare per minutes\n",
    "route_summary = Passengers[['Trip Route Description', 'Fare', 'Fare_per_minute', 'Fare_per_mile', 'TT_minutes']].groupby('Trip Route Description').median().sort_values(by = 'Fare_per_minute', ascending = False)\n",
    "route_summary['Origin'] = RouteMaster['ORIGIN']\n",
    "route_summary['Destination'] = RouteMaster['DESTINATION']\n",
    "\n",
    "# we set up a slice of the passengers DF\n",
    "s = Passengers[['Trip Route Description', 'Gender','Ethnicity','Age Group']]\n",
    "\n",
    "# Import the demographic buckets discussed above\n",
    "ethnicities = ['BLACK','INDIAN','OTHER','WHITE','COLOURED']\n",
    "genders = ['M','F']\n",
    "ages = ['YOUNG','MIDDLE','OLD']\n",
    "\n",
    "# again, set up flag columns for each option\n",
    "for gend in genders:\n",
    "    s[gend] = 0\n",
    "    s[gend].loc[s['Gender'] == gend] = 1\n",
    "for ethn in ethnicities:\n",
    "    s[ethn] = 0\n",
    "    s[ethn].loc[s['Ethnicity'] == ethn] = 1\n",
    "for age in ages:\n",
    "    s[age] = 0\n",
    "    s[age].loc[s['Age Group'] == age] = 1\n",
    "\n",
    "# sum them up again by Trip this time, NOT by hour. Hence, one row = one route    \n",
    "s = s.groupby('Trip Route Description').sum()\n",
    "s['Total'] = s['M'] + s['F']\n",
    "\n",
    "# set up the fractional columns for our demographic info\n",
    "col_keep = []\n",
    "for ethn in ethnicities:\n",
    "    s['% {}'.format(ethn)] = s[ethn] / (s['Total'])\n",
    "    col_keep.append('% {}'.format(ethn))\n",
    "for gend in genders:\n",
    "    s['% {}'.format(gend)] = s[gend] / (s['Total'])\n",
    "    col_keep.append('% {}'.format(gend))\n",
    "for age in ages:\n",
    "    s['% {}'.format(age)] = s[age] / (s['Total'])\n",
    "    col_keep.append('% {}'.format(age))\n",
    "    \n",
    "# subset to just the columns we are interested in\n",
    "s = s[col_keep]\n",
    "\n",
    "# build a single route summary DataFrame with all routes in it (contrast to above block - one DF per route, by hour)\n",
    "for i in col_keep:\n",
    "    route_summary[i] = s[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Route Summary: Match on Route WKT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# our route geometries are expressed as 'kmls' (groan). We modify our fiona instance in this script by adding KML drivers. \n",
    "# Fiona is the underlying library that GeoPandas uses to import KMLs. Unless we make this adjustment, it will not read in KMLs\n",
    "import geopandas as gpd\n",
    "import fiona\n",
    "fiona.drvsupport.supported_drivers['kml'] = 'rw' # enable KML support which is disabled by default\n",
    "fiona.drvsupport.supported_drivers['KML'] = 'rw' # enable KML support which is disabled by default\n",
    "\n",
    "# we pick up our route summary DF, and make a list of all of the unique trips\n",
    "route_summary = route_summary.reset_index()\n",
    "set_of_routes = list(set(route_summary['Trip Route Description']))\n",
    "\n",
    "# we walk the folder with the kmls in it, which adds the file names to 'files' list\n",
    "for root, folder, files in os.walk(r'C:\\Users\\charl\\Documents\\GOST\\South Africa\\Connectivity\\Submission_15_12_2018\\COCT_Dec'):\n",
    "    pass\n",
    "\n",
    "# now, we walk through the files, and check whether the filename corresponds to anything in the Trip Route Description bucket\n",
    "# if it matches, we add load it as a geopandas dataframe, and then append the geoemetry specifically to a dictionary, geom_dict\n",
    "gathered = []\n",
    "geom_dict = {}\n",
    "\n",
    "# go through files...\n",
    "for f in files:\n",
    "    \n",
    "    # split on underscore, take the first bit (this should be a string which matches a route_description object)\n",
    "    route = f.split('_')[0]\n",
    "    \n",
    "    # check to see if it is, and that we haven't already gathered it\n",
    "    if route in set_of_routes and route not in gathered:\n",
    "        \n",
    "        # if it's a kml\n",
    "        if f.split('_')[2] == 'trip.kml':\n",
    "            # read it in\n",
    "            f = gpd.read_file(os.path.join(root, f))\n",
    "            \n",
    "            # add it to our geom_dict, with key being the route identifier (important)\n",
    "            geom_dict[route] = f.geometry.iloc[0]\n",
    "            \n",
    "            # append it also to the gathered list - we don't want multiple geometries recorded for each route ID\n",
    "            gathered.append(route)\n",
    "\n",
    "# we convert this dictionary of geometries to a GeoDataFrame, with the key as the geometry dict keys...which are our route descriptions....\n",
    "geom_df = pd.DataFrame({'geometry':list(geom_dict.values())}-, index = geom_dict.keys())\n",
    "\n",
    "# allowing us to then match this geometry onto the route_summary df via the Trip Route Description field!\n",
    "route_summary = route_summary.set_index('Trip Route Description')\n",
    "route_summary['WKT'] = geom_df['geometry']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Route Summary: Match on Start / End Ward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we can use the kml geometry to also match on the route's start and end ward, using an identical process to before. \n",
    "route_summary = route_summary.reset_index()\n",
    "route_summary = route_summary.loc[route_summary.WKT.apply(type) != float]\n",
    "\n",
    "# we make an origin point out of the first coordinate in the string\n",
    "route_summary['Origin_Point'] = route_summary.WKT.apply(lambda x: Point((x.coords[0])))\n",
    "\n",
    "# and a destination point out of the last coordinate in the string\n",
    "route_summary['Destination_Point'] = route_summary.WKT.apply(lambda x: Point((x.coords[-1])))\n",
    "\n",
    "# subset the route summary df\n",
    "trip_origin_df = route_summary[['Trip Route Description','Origin_Point']]\n",
    "\n",
    "# generate a GDF where the geometry is the origin point\n",
    "trip_origin_df = gpd.GeoDataFrame(trip_origin_df, geometry = 'Origin_Point', crs = ward_shp.crs)\n",
    "\n",
    "# perform spatial join\n",
    "trip_origin_join = gpd.sjoin(trip_origin_df, ward_shp, how = 'left')\n",
    "\n",
    "# prep for joining based on Trip Route Description\n",
    "trip_origin_join = trip_origin_join.set_index('Trip Route Description')\n",
    "\n",
    "# same again, using Destination Point as the geometry\n",
    "trip_dest_df = route_summary[['Trip Route Description','Destination_Point']]\n",
    "trip_dest_df = gpd.GeoDataFrame(trip_dest_df, geometry = 'Destination_Point', crs = ward_shp.crs)\n",
    "trip_dest_join = gpd.sjoin(trip_dest_df, ward_shp, how = 'left')\n",
    "trip_dest_join = trip_dest_join.set_index('Trip Route Description')\n",
    "\n",
    "# now, join on the ward info using the key field, Trip Route Description\n",
    "route_summary = route_summary.set_index('Trip Route Description')\n",
    "route_summary['dest_ward_KE'] = trip_dest_join['SL_WARD_KE']\n",
    "route_summary['dest_SUB_CNC'] = trip_dest_join['SL_SUB_CNC']\n",
    "route_summary['origin_ward_KE'] = trip_origin_join['SL_WARD_KE']\n",
    "route_summary['origin_SUB_CNC'] = trip_origin_join['SL_SUB_CNC']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time of Day: Generate Analytics\n",
    "This is very similar to the trip-level analytics, but here we want to analyze the behaviour and characteristics of all passengers, across all jounrneys - with the only differentiator being the hour of travel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Once again, create a fresh copy of the all-important Passengers DF\n",
    "s = Passengers.copy()\n",
    "\n",
    "# generate a boarding hour field, which we will use to summarize the data later\n",
    "s['Boarding Hour'] = s['Boarding Time'].apply(lambda x: x.hour)\n",
    "\n",
    "# Using this familiar process, set up demographic flag columns (see above for more detailed walkthrough of same process)\n",
    "col_keep = []\n",
    "for gend in genders:\n",
    "    s[gend] = 0\n",
    "    s[gend].loc[s['Gender'] == gend] = 1\n",
    "    col_keep.append(gend)\n",
    "for ethn in ethnicities:\n",
    "    s[ethn] = 0\n",
    "    s[ethn].loc[s['Ethnicity'] == ethn] = 1\n",
    "    col_keep.append(ethn)\n",
    "for age in ages:\n",
    "    s[age] = 0\n",
    "    s[age].loc[s['Age Group'] == age] = 1\n",
    "    col_keep.append(age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We want to do two processes here - average for values within each hour like fare, and count people. \n",
    "# So, we set up two DFs - one for averaging and one for counting (both are copies of the passengers DF)\n",
    "data_averageing = s[['Boarding Hour','Fare','Fare_per_mile','Fare_per_minute','TT_minutes']]\n",
    "data_counting = s[['Boarding Hour',*col_keep]]\n",
    "\n",
    "# Here we actually apply the averaging - .median() for data_averaging, .sum()\n",
    "data_averageing = data_averageing.groupby('Boarding Hour').median()\n",
    "data_counting = data_counting.groupby('Boarding Hour').sum()\n",
    "\n",
    "# now, we join them on to each other\n",
    "data = data_averageing.join(data_counting)\n",
    "\n",
    "# adjust the column naming a little\n",
    "data.columns = ['Median Fare','Median Fare per Mile', 'Median Fare per Minute', 'Median Trip Time (minutes)', *col_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here we take a copy of the above data DF, add those classic % demographic columns\n",
    "s = data.copy()\n",
    "s['Total'] = s['M'] + s['F']\n",
    "for ethn in ethnicities:\n",
    "    s['% {}'.format(ethn)] = s[ethn] / (s['Total'])\n",
    "for gend in genders:\n",
    "    s['% {}'.format(gend)] = s[gend] / (s['Total'])\n",
    "for age in ages:\n",
    "    s['% {}'.format(age)] = s[age] / (s['Total'])\n",
    "time_of_day = s.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Out\n",
    "Here, the client was interested in results in excel format. As such, we send to excel some of the data tables we have generated above. note the use of the sheet_name parameter, which allows us to generate a single excel workbook, with multiple sheets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We set up an excel writer object first to allow us to write multiple objects to the same file\n",
    "with pd.ExcelWriter(os.path.join(p, 'summary.xlsx')) as writer:\n",
    "    route_summary.to_excel(writer, sheet_name = 'RouteSummary')\n",
    "    time_of_day.to_excel(writer, sheet_name = 'HourlyAnalysis')\n",
    "    counter = 1\n",
    "    for table in tables:\n",
    "        table.to_excel(writer, sheet_name = 'table_%s' % counter)\n",
    "        counter +=1\n",
    "    counter = 1\n",
    "    for table in trip_tables:\n",
    "        table.to_excel(writer, sheet_name = '%s' % Selected_trips[counter - 1])\n",
    "        counter +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyis of Khayelitsha\n",
    "Whilst the preceeding analysis has covered journeys across the city, we are now going to geographically bound our activities to just the Khayeltisha wards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bring in the Khayelitsha shapefile as a GDF\n",
    "p_s = r'C:\\Users\\charl\\Documents\\GOST\\South Africa\\Connectivity\\Fwd__SAL'\n",
    "khay = gpd.read_file(os.path.join(p_s, 'Khayelitsha.shp'))\n",
    "\n",
    "# join up the khayelitsha geometries into a single geometry object by using unary_union\n",
    "khay_shp = unary_union(khay.geometry)\n",
    "\n",
    "# make a GDF of this shapely geometry object\n",
    "khay_shp_gdf = gpd.GeoDataFrame({'geometry':khay_shp}, crs = {'init':'epsg:4326'}, index = [1], geometry = 'geometry')\n",
    "\n",
    "# send the single-geometry GDF to file\n",
    "khay_shp_gdf.to_file(os.path.join(p_s, 'Khayelitsha_area.shp'), driver = 'ESRI Shapefile')\n",
    "\n",
    "# project to metres\n",
    "khay_shp_gdf = khay_shp_gdf.to_crs({'init':'epsg:22234'})\n",
    "khay_shp_gdf.to_file(os.path.join(p_s, 'Khayelitsha_area_222234.shp'), driver = 'ESRI Shapefile')\n",
    "\n",
    "# read in whole of Cape Town wards, project, save down once again\n",
    "shp_pth = r'C:\\Users\\charl\\Documents\\GOST\\South Africa\\Connectivity\\Fwd__SAL\\1_Residential_SAL'\n",
    "ward_shp = gpd.read_file(os.path.join(shp_pth, 'wards_4326.shp'))\n",
    "ward_shp = ward_shp.to_crs({'init':'epsg:22234'})\n",
    "ward_shp.to_file(os.path.join(p_s, 'wards_222234.shp'), driver = 'ESRI Shapefile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make geometry objects of the boarding and alighting points\n",
    "Passengers['Boarding Point'] = Passengers['Boarding Location'].apply(lambda x: convert_to_point(x))\n",
    "Passengers['Alighting Point'] = Passengers['Alighting Location'].apply(lambda x: convert_to_point(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here, we generate a Passenger dataframe of JUST the Passengers whose boarding points intersect the Khayelitsha shape\n",
    "khay_p_boarding = gpd.GeoDataFrame(Passengers, crs = khay.crs, geometry = 'Boarding Point')\n",
    "khay_p_boarding = khay_p_boarding.loc[khay_p_boarding.intersects(khay_shp) == True]\n",
    "\n",
    "# we add again the boarding time hour for summary purposes\n",
    "khay_p_boarding['Boarding Time hour'] = khay_p_boarding['Boarding Time'].apply(lambda x: x.hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for testing purposes I send this, projected, to .csv\n",
    "khay_p_boarding = khay_p_boarding.to_crs({'init':'epsg:22234'})\n",
    "khay_p_boarding.to_csv(os.path.join(p, 'khay.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We can subset this set of passengers by gender very easily. \n",
    "# these files allow us to easily identify differences in spatial location of boarding points for men and women\n",
    "\n",
    "# men\n",
    "khay_p_boarding_men = khay_p_boarding.loc[khay_p_boarding['Gender'] == 'M']\n",
    "khay_p_boarding_men.to_csv(os.path.join(p, 'khay_boarding_men.csv'))\n",
    "\n",
    "# women\n",
    "khay_p_boarding_women = khay_p_boarding.loc[khay_p_boarding['Gender'] == 'F']\n",
    "khay_p_boarding_women.to_csv(os.path.join(p, 'khay_boarding_women.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We do precisely the same thing, this time using alighting point (people arriving into Khayeltisha) as the geometry of interest\n",
    "khay_p_alighting = gpd.GeoDataFrame(Passengers, crs = khay.crs, geometry = 'Alighting Point')\n",
    "khay_p_alighting['Alighting Time hour'] = khay_p_alighting['Alighting Time'].apply(lambda x: x.hour)\n",
    "khay_p_alighting = khay_p_alighting.loc[khay_p_alighting.intersects(khay_shp) == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# again, to see if there are differences in disembarkment point, we send these to .csv\n",
    "khay_p_alighting = khay_p_alighting.to_crs({'init':'epsg:22234'})\n",
    "khay_p_alighting_men = khay_p_alighting.loc[khay_p_alighting['Gender'] == 'M']\n",
    "khay_p_alighting_men.to_csv(os.path.join(p, 'khay_alighting_men.csv'))\n",
    "khay_p_alighting_women = khay_p_alighting.loc[khay_p_alighting['Gender'] == 'F']\n",
    "khay_p_alighting_women.to_csv(os.path.join(p, 'khay_alighting_women.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in the ward shapefile\n",
    "shp_pth = r'C:\\Users\\charl\\Documents\\GOST\\South Africa\\Connectivity\\Fwd__SAL\\1_Residential_SAL'\n",
    "ward_shp = gpd.read_file(os.path.join(shp_pth, 'wards_4326.shp'))\n",
    "\n",
    "# create centroids from ward geometries\n",
    "ward_shp['centroid'] = ward_shp.geometry.centroid\n",
    "\n",
    "# For the purposes of visualzing trips between wards, we create a DF of their centroids\n",
    "# These can later be made into lines between centroids\n",
    "match_ward = ward_shp[['SL_WARD_KE','centroid']].set_index('SL_WARD_KE')\n",
    "\n",
    "# adjust a specific centroid which lies in water (part of ward offshore)\n",
    "match_ward['centroid'].loc[5177473] = Point(18.3918, -33.9201)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From manual inspection of the file in QGIS, I know these are the wards that form Khayeltisha\n",
    "khayelitsha_wards = [5177561,5177559, 5177557, 5177555, 5177553, 5177551, 5177549, 5177547, 5177545, 5177543]\n",
    "\n",
    "# As we want to observe the OD matrix at different times of day, I create a times dictionary with associated hour bracketing. \n",
    "times = {'morning':[7,8,9],'midday':[11,12,13],'evening':[16,17,8,19]}\n",
    "\n",
    "# We now repeat a process for both arrivals into and departures from Khayeltisha. \n",
    "# We want one DataFrame for each time of day, so we iterate through 'times'\n",
    "outs = []\n",
    "for t in times:\n",
    "    # pick out our current 'hours' for this time of day\n",
    "    HRS = times[t]\n",
    "    binz = []\n",
    "    # for each ward in Khayelitsha\n",
    "    for ward in khayelitsha_wards:\n",
    "        \n",
    "        # grab all the passengers boarding in this ward at this time of day\n",
    "        Q = khay_p_boarding.copy()\n",
    "        Q = Q.loc[Q['Boarding Time hour'].isin(HRS)]\n",
    "        q = Q.loc[Q.Board_WARD_KE == ward]\n",
    "        \n",
    "        # count the number of people going to each other ward\n",
    "        dest = q.Alight_WARD_KE.value_counts().to_frame()\n",
    "        dest = dest.reset_index()\n",
    "        dest.columns = ['D_ID','count']\n",
    "        \n",
    "        # set an origin ward ID as the current ward\n",
    "        dest['O_ID'] = ward\n",
    "        binz.append(dest)\n",
    "        \n",
    "    # summarize the dataframe at the Khayelithsa level\n",
    "    df2 = pd.concat(binz)\n",
    "    df2['time'] = str(t)\n",
    "    outs.append(df2)\n",
    "\n",
    "# same process, using the passengers alighting in Khayeltisha\n",
    "ins = []\n",
    "for t in times:\n",
    "    HRS = times[t]\n",
    "    binz = []\n",
    "    for ward in khayelitsha_wards:\n",
    "        Q = khay_p_alighting.copy()\n",
    "        Q = Q.loc[Q['Alighting Time hour'].isin(HRS)]\n",
    "        q = Q.loc[Q.Alight_WARD_KE == ward]\n",
    "        orig = q.Board_WARD_KE.value_counts().to_frame()\n",
    "        orig = orig.reset_index()\n",
    "        orig.columns = ['O_ID','count']\n",
    "        orig['D_ID'] = ward\n",
    "        binz.append(orig)\n",
    "    df2 = pd.concat(binz)\n",
    "    df2['time'] = str(t)\n",
    "    ins.append(df2)\n",
    "\n",
    "# Now, we combine the in and out dataframes to work out the net inflows and outflows to each ward from Khayelitsha\n",
    "nets = []\n",
    "\n",
    "# we have to do this for each frame in the ins and outs bucket (i.e. for each time of day)\n",
    "for i in range(0, len(ins)):\n",
    "    # A is a copy of the inflows to Khayelitsha\n",
    "    A = ins[i]\n",
    "    \n",
    "    # define a new column which describes the movement (origin and destination ward IDs)\n",
    "    A['combo'] = A['O_ID'].astype(str) + ' | ' + A['D_ID'].astype(str)\n",
    "    A = A.rename({'count':'inflow'}, axis = 1)\n",
    "    A = A[['inflow','combo']].set_index('combo')\n",
    "    \n",
    "    # B is a copy of the outflows from Khayelitsha\n",
    "    B = outs[i]\n",
    "    \n",
    "    # define a new column which describes the movement (origin and destination ward IDs)\n",
    "    B['combo'] = B['D_ID'].astype(str) + ' | ' + B['O_ID'].astype(str)\n",
    "    B = B.rename({'count':'outflow'}, axis = 1)\n",
    "    B = B[['outflow','combo']].set_index('combo')\n",
    "    \n",
    "    # join the two together, to be able to work out the net\n",
    "    C = A.join(B, how = 'outer')\n",
    "    \n",
    "    # any missing values are filled with 0 - no one did that trip\n",
    "    C['inflow'] = C['inflow'].fillna(0)\n",
    "    C['outflow'] = C['outflow'].fillna(0)\n",
    "    \n",
    "    # work out the net flow between district pairs\n",
    "    C['net'] = C['inflow'] - C['outflow']\n",
    "    \n",
    "    # sort values by the net movement\n",
    "    C = C.sort_values(by = 'net', ascending = False)\n",
    "    \n",
    "    # add on the time of day\n",
    "    C['time'] = list(times.keys())[i]\n",
    "    C = C.reset_index()\n",
    "    \n",
    "    # split the code on the pipe to get the origin and destination ward IDs back\n",
    "    C['O_ID'] = C['combo'].apply(lambda x: x.split(' | ')[0]).astype(float)\n",
    "    C['D_ID'] = C['combo'].apply(lambda x: x.split(' | ')[1]).astype(float)\n",
    "    \n",
    "    # append the resultant dataframe to the bucket of netted-out DFs\n",
    "    nets.append(C)\n",
    "\n",
    "# we want both combined and disaggregated results, so we go through separate processes for each\n",
    "for X in ['combined','disagg']:\n",
    "    \n",
    "    # disaggregated process\n",
    "    if X == 'disagg':\n",
    "        \n",
    "        # for each time of day\n",
    "        for i in range(0, len(nets)):\n",
    "            \n",
    "            # get current dataFrame from nets bucket\n",
    "            df2 = nets[i]\n",
    "            \n",
    "            # add time of day\n",
    "            time_of_day = list(times.keys())[i]\n",
    "            \n",
    "            # take a copy\n",
    "            df = df2.copy()\n",
    "            \n",
    "            # match on the centroids of the destination ward\n",
    "            df = df.set_index('D_ID')\n",
    "            df['D_Point'] = match_ward['centroid']\n",
    "            \n",
    "            # match on the centroid of the origin ward\n",
    "            df = df.reset_index().set_index('O_ID')\n",
    "            df['O_Point'] = match_ward['centroid']\n",
    "            \n",
    "            # reset index\n",
    "            df = df.reset_index()\n",
    "            \n",
    "            # make a human intelligible string\n",
    "            df['journey'] = df['O_ID'].astype(str) + ' to ' + df['D_ID'].astype(str)\n",
    "            \n",
    "            # build a lineString which describes the journey's geometry\n",
    "            df['WKT'] = df.apply(lambda x: LineString([x.O_Point,x.D_Point]), axis = 1)\n",
    "            \n",
    "            # send to csv - all done!\n",
    "            df.to_csv(os.path.join(p, 'Khayelitsha', '{}_{}.csv'.format(time_of_day, X)))\n",
    "    \n",
    "    # for the combined one, the difference is we want to aggregate by origin ID. \n",
    "    # the process is otherwise identical\n",
    "    # combined process\n",
    "    elif X == 'combined':\n",
    "        for i in range(0, len(nets)):\n",
    "            time_of_day = list(times.keys())[i]\n",
    "            df2 = nets[i]\n",
    "            \n",
    "            # these are the only two lines which are different vs. above\n",
    "            df2 = df2[['net','time','D_ID','O_ID']]\n",
    "            \n",
    "            # group by origin ID, sum, reset index\n",
    "            df2 = df2.groupby('O_ID').sum().reset_index()\n",
    "            \n",
    "            # resume above process\n",
    "            df = df2.copy()\n",
    "            df = df.set_index('D_ID')\n",
    "            df['D_Point'] = khay_shp.centroid\n",
    "            df = df.reset_index().set_index('O_ID')\n",
    "            df['O_Point'] = match_ward['centroid']\n",
    "            df = df.reset_index()\n",
    "            df['journey'] = df['O_ID'].astype(str) + ' to ' + df['D_ID'].astype(str)\n",
    "            df['WKT'] = df.apply(lambda x: LineString([x.O_Point,x.D_Point]), axis = 1)\n",
    "            df.to_csv(os.path.join(p, 'Khayelitsha', '{}_{}.csv'.format(time_of_day, X)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
