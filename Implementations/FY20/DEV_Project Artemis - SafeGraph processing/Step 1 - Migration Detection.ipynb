{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Migration Detection\n",
    "\n",
    "This script was designed for use in Project Artemis. The objective was to try to identify instances of migration in the SafeGraph data, which could then be correlated with past famines, and relationships identified. \n",
    "\n",
    "In this script, we work through SafeGraph record summaries to try to define a 'home range' for each user. Large movements in this home range indicate migration. This process generates the data layer, which can the be passed to the visualization script for finalization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import shapely \n",
    "import geopandas as gpd\n",
    "import sys, os, time\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry import MultiPolygon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath = r'C:\\Users\\charl\\Documents\\GOST\\SafeGraph'\n",
    "datapath = r'C:\\Users\\charl\\Documents\\GOST\\SafeGraph\\SafeGraph'\n",
    "WGS84 = {'init' :'epsg:4326'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a list of files by walking through the datapath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(datapath, topdown=False):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimised list flattening function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l, ltypes=(list, tuple)):\n",
    "    ltype = type(l)\n",
    "    l = list(l)\n",
    "    i = 0\n",
    "    while i < len(l):\n",
    "        while isinstance(l[i], ltypes):\n",
    "            if not l[i]:\n",
    "                l.pop(i)\n",
    "                i -= 1\n",
    "                break\n",
    "            else:\n",
    "                l[i:i + 1] = l[i]\n",
    "        i += 1\n",
    "    return ltype(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main function to be run against input dataframes. See in-line comments for details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Main(fil, i, hr_daythresh, hr_distthresh, migr_dist_thresh):    \n",
    "    \n",
    "    # read input .csv\n",
    "    df = pd.read_csv(os.path.join(root, fil))\n",
    "    \n",
    "    # subset to selected column\n",
    "    df = df[['latitude','longitude','horizontal_accuracy','utc_timestamp']]\n",
    "    \n",
    "    # convert UNIX datetime object\n",
    "    df['date'] = pd.to_datetime(df['utc_timestamp'], unit='s')\n",
    "    \n",
    "    # format as day, month\n",
    "    df['date'] = df['date'].dt.strftime('%d.%m')\n",
    "    \n",
    "    # drop all observations where horizonatal accuracy is less than 100m\n",
    "    df = df.loc[df.horizontal_accuracy < 100]\n",
    "    \n",
    "    # generate geomtry objects from longitude and latitude columns\n",
    "    df['Point'] = df.apply(lambda x: Point(x.longitude, x.latitude), axis = 1)\n",
    "    \n",
    "    # grouping function. extracts specific records from passed in dataframe, returns new df.\n",
    "    def grouper(x):\n",
    "        y = pd.DataFrame()\n",
    "        y['minx'] = [x.longitude.min()]\n",
    "        y['maxx'] = [x.longitude.max()]\n",
    "        y['miny'] = [x.latitude.min()]\n",
    "        y['maxy'] = [x.latitude.max()]\n",
    "        y['Pointbag'] = [list(x.Point)]\n",
    "        return y\n",
    "    \n",
    "    # from the larger dataframe, create a dataframe which summarises by day. One row per each day. \n",
    "    daydf = df.groupby('date').apply(lambda x: grouper(x))\n",
    "    \n",
    "    # generate a unique ID for each person\n",
    "    daydf['ID'] = 'person_%s' % i\n",
    "    \n",
    "    # reset index \n",
    "    daydf = daydf.reset_index()\n",
    "    \n",
    "    # break out the day from the date string\n",
    "    daydf['day'] = daydf['date'].apply(lambda x: int(x.split('.')[0]))\n",
    "    \n",
    "    # calculate a rough centroid\n",
    "    daydf['centroid'] = daydf.apply(lambda x: Point(((x['minx'] + x['maxx']) / 2, (x['miny'] + x['maxy']) / 2)), axis = 1)\n",
    "    \n",
    "    # note - not yet projected - dummy column which is reprojected later\n",
    "    daydf['centroid_utm'] = daydf['centroid']\n",
    "    \n",
    "    ### Calculate daily displacement of centroid in metres \n",
    "    # identify correct UTM zone for the median long and lat point\n",
    "    EPSG = 32700-round((45+df.longitude.median())/90,0)*100+round((183+df.latitude.median())/6,0)\n",
    "    epsg = {'init' :'epsg:%s' % int(EPSG)}\n",
    "    \n",
    "    # generate geodataframe\n",
    "    gdaydf = gpd.GeoDataFrame(daydf, geometry = 'centroid_utm', crs = WGS84)\n",
    "    \n",
    "    # project geodataframe to UTM \n",
    "    gdaydf = gdaydf.to_crs(epsg)\n",
    "    \n",
    "    # generate daily change in centroid column ('Disp') - (today  - yesterday)\n",
    "    gdaydf['prev_centroid_utm'] = gdaydf['centroid_utm'].shift(periods = 1).fillna(gdaydf['centroid_utm'])\n",
    "    gdaydf['Disp'] = gdaydf.apply(lambda x: x.centroid_utm.distance(x.prev_centroid_utm), axis = 1)\n",
    "    \n",
    "    # Here we get to the meat of defining a home range.\n",
    "    # We break points into homerange blocks - when days consecutive and movement of centroid are below threshold.\n",
    "    # first value is window size, min_periods prevents smaller windows from occuring\n",
    "    # this is effectively a 'trigger' function whose value changes if the sum is not dividible precisely by the window size.\n",
    "    gdaydf['consec_block'] = gdaydf.rolling(hr_daythresh, min_periods = hr_daythresh).day.sum().fillna(0)  \n",
    "    gdaydf['consec_block'] = gdaydf['consec_block'].mask(gdaydf['consec_block'] % hr_daythresh == 0, 1).mask(gdaydf['consec_block'] % hr_daythresh != 0, 0).astype(int)\n",
    "    \n",
    "    # if displacement is less than the daily movement threshold, return 1, if not, 0\n",
    "    gdaydf['Dispmask'] = gdaydf['Disp'].mask(gdaydf['Disp'] < hr_distthresh, 1).mask(gdaydf['Disp'] > hr_distthresh, 0).astype(int)\n",
    "    \n",
    "    # add together our distance flag and our consecutive block flag to a new series called 'eligible'\n",
    "    gdaydf['eligible'] = (gdaydf['Dispmask'] + gdaydf['consec_block'])\n",
    "    \n",
    "    # convert these back to a binary mask (2 goes to 1, not 2 goes to 0)\n",
    "    gdaydf['eligible'] = gdaydf['eligible'].mask(gdaydf['eligible'] == 2, 1).mask(gdaydf['eligible'] != 2, 0)\n",
    "    \n",
    "    # here we generate a flag function for when eligible blocks change\n",
    "    x = list(gdaydf['eligible'])\n",
    "    y = np.insert(x, 0, 0)\n",
    "    res = (np.diff(y) == 1).cumsum() * x\n",
    "    gdaydf['flag'] = res\n",
    "\n",
    "    # Define shape of homerange(s)\n",
    "    num_homeranges = len(gdaydf['flag'].unique())\n",
    "    \n",
    "    # set up some blank variables\n",
    "    start_loc, end_loc = None, None\n",
    "    detection = 0 \n",
    "    \n",
    "    # create result for being unable to establish homerange:\n",
    "    if num_homeranges == 0: # prev 1\n",
    "        \n",
    "        status = 'unable to establish homerange',\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # create empty list for homeranges\n",
    "        homeranges = []\n",
    "        \n",
    "        # copy over df\n",
    "        hom = gdaydf.copy()\n",
    "        \n",
    "        # for each homerange\n",
    "        for j in range(1, num_homeranges):\n",
    "            \n",
    "            # create a new homerange df called cur_hom\n",
    "            cur_hom = hom.loc[hom['flag'] == j]\n",
    "            \n",
    "            # append the convex hull of the points in that homerange\n",
    "            homeranges.append(gpd.GeoSeries(flatten(list(cur_hom.Pointbag))).unary_union.convex_hull)\n",
    "    \n",
    "        # If only one home range, then by definition no migration\n",
    "        if len(homeranges) <= 1:\n",
    "            \n",
    "            status = '1 homerange established, no migration'\n",
    "        \n",
    "        # if we are in this case, then we have detected at least 2 home ranges - exciting! \n",
    "        else:\n",
    "\n",
    "            # check if homeranges are distant by calculating the distance between homerange centroid\n",
    "            # if this distance is larger than 'migr_dist_thresh', we have detected migration in this model:\n",
    "            homeranges = pd.DataFrame({'geometry' : homeranges})\n",
    "            homeranges = gpd.GeoDataFrame(homeranges, crs = WGS84, geometry = 'geometry')\n",
    "            homeranges['hr_cent'] = homeranges.centroid\n",
    "            homeranges['prev_hr_cent'] = homeranges['hr_cent'].shift(periods = 1).fillna(homeranges['hr_cent'])\n",
    "            homeranges = homeranges.to_crs(epsg)\n",
    "            homeranges['hr_cent_epsg'] = homeranges.centroid\n",
    "            homeranges['prev_hr_cent_epsg'] = homeranges['hr_cent_epsg'].shift(periods = 1).fillna(homeranges['hr_cent_epsg'])\n",
    "            homeranges['Disp'] = homeranges.apply(lambda x: x.hr_cent_epsg.distance(x.prev_hr_cent_epsg), axis = 1)\n",
    "            \n",
    "            # if we are in this case, we have at least 2 homeranges, but they aren't far enough \n",
    "            # away for migration to be declared with certainty:\n",
    "            if homeranges.Disp.max() <= migr_dist_thresh:\n",
    "\n",
    "                status = 'homerange(s) established, no migration',\n",
    "            \n",
    "            # if we are here, then migration has indeed been detected - the distance between homerange centroids exceeds\n",
    "            # our threshold distance for delcaring a migration\n",
    "            else:\n",
    "\n",
    "                migr_homeranges = homeranges.copy()\n",
    "                migr_homeranges = migr_homeranges.loc[homeranges.Disp > migr_dist_thresh]\n",
    "                \n",
    "                migr_homeranges['status'] = 'migration detected'\n",
    "                migr_homeranges['file'] = fil\n",
    "                migr_homeranges['person'] = 'person_%s' % i\n",
    "                migr_homeranges = migr_homeranges.rename(columns = {'hr_cent':'end_loc','prev_hr_cent':'start_loc','Disp':'distance'})\n",
    "                \n",
    "                detection = 1\n",
    "    \n",
    "    # we only want to retun the dataframe migr_homeranges if detection detected\n",
    "    if detection == 1:\n",
    "        return migr_homeranges\n",
    "    \n",
    "    # otherwise, we return a very simple dataframe, with the non-migration detection results. \n",
    "    else:\n",
    "        result = pd.DataFrame({'person': 'person_%s' % i,\n",
    "                           'file':fil,\n",
    "                           'status':status,\n",
    "             }, index = [1])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having defined this fairly complex process for identifying migration via material shifts in homeranges, we apply this to each file in the files list object generated at the start of the script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\charl\\Anaconda3\\envs\\Cfox2\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "outs = []\n",
    "hr_daythresh = 5  # minimum length of time for a homerange to be declared\n",
    "hr_distthresh = 1000  # parameter for homerange distances \n",
    "migr_dist_thresh = 50000 # migration threshold distance - 50km here\n",
    "\n",
    "# process each file in the list, add to the 'outs' list\n",
    "i = 0\n",
    "for fil in files:\n",
    "    outs.append(Main(fil, i, hr_daythresh, hr_distthresh, migr_dist_thresh))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prepare to output this datafile to disk, for visualization in the next script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the outs list into a dataFrame, confusingly also called 'out'\n",
    "out = pd.concat(outs)\n",
    "out = out.reset_index()\n",
    "\n",
    "# choose a subset of columns to output, send to file\n",
    "out = out[['person','status','start_loc','end_loc','distance','file']]\n",
    "out.to_csv(os.path.join(basepath, 'Output', 'output.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Cfox2)",
   "language": "python",
   "name": "cfox2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
