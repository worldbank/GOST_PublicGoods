{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 1\n",
    "\n",
    "This script takes as input a monthly aggregation of safegraph files. Each file contains a CAID (unique user ID), latitude and longitude, and timestamp organized in row format. The purpose of this script is to take the raw input, generate geometry objects, remove dud readings, and try to categorize every journey into various types. \n",
    "\n",
    "It is important to note that this is NOT the script that eventually determined the migration counts in the final layer. Nonethless, it is catalogued here for its techniques, which are applicable to similar data sources / for recycling purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys, os\n",
    "import shapely\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, LineString, box, MultiLineString, MultiPolygon\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set path Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r'D:\\SG'\n",
    "data_file = r'Results_Month_3.csv'\n",
    "workspace = r'C:\\Users\\charl\\Documents\\GOST\\SafeGraph'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame Controls: set the number of rows to import as row_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_count = None  # Control number of rows imported\n",
    "mode = ''       # set mode to test or production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define import function, create unique user counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Import(data_file):\n",
    "    print(\"Commencing Import: %s\" % time.ctime())\n",
    "    \n",
    "    df = pd.read_csv(os.path.join(data_path, data_file), nrows = row_count) # standard import of rows up to 'row_count'\n",
    "    \n",
    "    df['time'] = pd.to_datetime(df['utc_timestamp'], unit='s') # time object generated by converting unix timestamp\n",
    "    \n",
    "    df[\"n\"] = df.groupby('caid').ngroup() # here, we number each user uniquely by grouping records by caid.\n",
    "    \n",
    "    print(\"Import Complete: %s\" % time.ctime())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import function called. May take a while if row_count is large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commencing Import: Mon Sep 17 17:54:24 2018\n",
      "Import Complete: Mon Sep 17 17:54:35 2018\n"
     ]
    }
   ],
   "source": [
    "df = Import(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping Functions for turning points into lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LINEGROUPER(x2):\n",
    "    y = pd.DataFrame()\n",
    "    y['caid'] = [x2.caid.iloc[0]]\n",
    "    y['StartTime'] = x2.time.iloc[0]\n",
    "    y['EndTime'] = x2.time.iloc[-1]\n",
    "    y['time_elapsed'] = [y['EndTime'] - y['StartTime']]\n",
    "    y['bbox_utm'] = [LineString(x2.proj_geom.tolist()).bounds]\n",
    "    y['start_utm'] = [x2.proj_geom.tolist()[0]]\n",
    "    y['end_utm'] = [x2.proj_geom.tolist()[-1]]\n",
    "    y['total_dist'] = [x2.del_dist.sum()]\n",
    "    y['Av.Speed_mph'] = [(x2.del_dist.sum() / x2.del_time_secs.sum()) * 3600 / 1609.34]\n",
    "    y['npoints'] = len(x2)\n",
    "    try:\n",
    "        y['geometry'] = [LineString(x2.geometry.tolist())]\n",
    "    except:\n",
    "        y['geometry'] = 'null'\n",
    "    return y\n",
    "\n",
    "def GROUPER(x):\n",
    "    x = x.groupby(['JourneyID']).apply(lambda x: LINEGROUPER(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main function - runs for each unique ID in DataFrame. Runs sequentially on groups of records with the same caid (here, identified by the new 'n' property, generated upon import)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commencing Main: Mon Sep 17 17:54:35 2018\n",
      "Main Complete: Tue Sep 18 01:49:30 2018\n"
     ]
    }
   ],
   "source": [
    "def Main(df, mode): \n",
    "    \n",
    "    lines, points = [], []\n",
    "\n",
    "    if mode == 'test':\n",
    "        rlist = [1]\n",
    "    else:\n",
    "        rlist = df.n.unique() \n",
    "    print(\"Commencing Main: %s\" % time.ctime())\n",
    "    for record in rlist:\n",
    "        \n",
    "        # Take a single user's records in the master DF. This collapses the dataframe to just a single CAID. \n",
    "        cdf = df.loc[df.n == record]\n",
    "        \n",
    "        # reset index to treat this CAID like its own DF.\n",
    "        cdf = cdf.reset_index()\n",
    "\n",
    "        # Skip if too short to extract useful information\n",
    "        if len(cdf) < 3:\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "\n",
    "            # Find the relevant UTM zone for that user based on first lat and lon coordinates\n",
    "            EPSG = 32700-round((45+cdf.longitude.loc[1])/90,0)*100+round((183+cdf.latitude.loc[1])/6,0)\n",
    "\n",
    "            # Create a geometry object for each point recorded in their records\n",
    "            geom = [Point(xy) for xy in zip(cdf.longitude, cdf.latitude)]\n",
    "\n",
    "            # Use this to create a GeoDataFrame. Initialize with WGS84.\n",
    "            gcdf = gpd.GeoDataFrame(cdf, crs = {'init' :'epsg:4326'}, geometry = geom)\n",
    "\n",
    "            # Reproject to relevant UTM Zone\n",
    "            gcdf = gcdf.to_crs({'init' :'epsg:%d' % EPSG})\n",
    "\n",
    "            # define new lat / lon in projected UTM coordinates\n",
    "            gcdf['lon_UTM'], gcdf['lat_UTM'] = gcdf['geometry'].x, gcdf['geometry'].y\n",
    "\n",
    "            # Generate Time Deltas\n",
    "            gcdf['del_time_secs'] = gcdf['utc_timestamp'].diff()\n",
    "\n",
    "            # Generate Distance Deltas (metres)\n",
    "            gcdf['del_lon'] = gcdf.lon_UTM.diff()\n",
    "            gcdf['del_lat'] = gcdf.lat_UTM.diff() \n",
    "            gcdf['dist'] = np.sqrt(np.square(gcdf['del_lat'])+np.square(gcdf['del_lon'])).fillna(0)\n",
    "\n",
    "            # Create offset column - 'next distance travelled' - flags start of journey\n",
    "            gcdf['next_dist'] = gcdf['dist'].shift(-1).fillna(0)       \n",
    "\n",
    "            # Calculate average speed\n",
    "            gcdf['av.speed (m/s)'] = (gcdf['dist'] / gcdf['del_time_secs'])\n",
    "            gcdf['av.speed (mph)'] = (gcdf['av.speed (m/s)'] * 3600 / 1609.34)\n",
    "\n",
    "            # Remove stationary points - no current movement, not start of new journey, low speed\n",
    "            gcdf = gcdf.loc[(gcdf['next_dist'] != 0) | gcdf['dist'] != 0]\n",
    "            \n",
    "            # having applied the above controls, pass if fewer than 3 records. \n",
    "            if len(gcdf) < 3:\n",
    "                pass\n",
    "            \n",
    "            # otherwise, continue...\n",
    "            else:\n",
    "                \n",
    "                # Regenerate deltas / calculations post cleaning.\n",
    "                # these steps mirror those above, but with stationary points removed. \n",
    "                gcdf = gcdf.reset_index()\n",
    "                gcdf['del_lon'] = gcdf.lon_UTM.diff()\n",
    "                gcdf['del_lat'] = gcdf.lat_UTM.diff() \n",
    "                gcdf['del_dist'] = np.sqrt(np.square(gcdf['del_lat'])+np.square(gcdf['del_lon']))\n",
    "                gcdf['del_time_secs'] = gcdf['utc_timestamp'].diff()\n",
    "                gcdf['av.speed (m/s)'] = gcdf['dist'] / gcdf['del_time_secs'].fillna(0)\n",
    "                gcdf['av.speed (mph)'] = gcdf['av.speed (m/s)'] * 3600 / 1609.34\n",
    "                gcdf['av.speed (mph)'] = gcdf['av.speed (mph)'].fillna(0)\n",
    "\n",
    "                # Create 'new journey start' flag function (not yet applied)\n",
    "                def JourneyFlag(x):\n",
    "                    if x['del_dist'] == 0 and x['next_dist'] > 0 and  x['del_time_secs'] > 60:\n",
    "                        flag = 1\n",
    "                    else:\n",
    "                        flag = 0\n",
    "                    return flag\n",
    "\n",
    "                # Add flag for a new journey (applied above function, row-wise)\n",
    "                gcdf['newjourneyflag'] = gcdf.apply(lambda x: JourneyFlag(x), axis = 1)\n",
    "\n",
    "                # Add counter for journeys done by this caid\n",
    "                gcdf['JourneyID'] = gcdf['newjourneyflag'].cumsum(axis=0)\n",
    "                gcdf['JourneyID'] = '%d_' % record + gcdf['JourneyID'].astype(str)\n",
    "\n",
    "                # Generate bearing function - demarks direction of travel between record pairs. \n",
    "                def BearingCalc(x):\n",
    "                    degs = math.degrees(math.atan2(x['del_lon'], x['del_lat']))\n",
    "                    if degs < 0:\n",
    "                        degs = 360 + degs\n",
    "                    return degs\n",
    "                \n",
    "                # application of bearing function\n",
    "                gcdf['bearing_last'] = gcdf.apply(lambda x: BearingCalc(x), axis = 1).fillna(0)\n",
    "                gcdf['bearing_next'] = gcdf['bearing_last'].shift(-1).fillna(0)\n",
    "\n",
    "                # Reset Geometry to WGS84 base\n",
    "                gcdf['proj_geom'] = gcdf['geometry']\n",
    "                gcdf = gcdf.set_geometry([Point(xy) for xy in zip(gcdf.longitude, gcdf.latitude)])\n",
    "\n",
    "                # Group points into a new Line objects dataframe, reset index\n",
    "                ldf = GROUPER(gcdf)\n",
    "                ldf = ldf.reset_index()\n",
    "\n",
    "                # Remove journeys which are too slow, too fast or don't go anywhere\n",
    "                ldf = ldf.loc[(ldf['Av.Speed_mph'] > 0.1) & \n",
    "                              (ldf['Av.Speed_mph'] < 120) &\n",
    "                              (ldf['total_dist'] > 100)\n",
    "                             ]\n",
    "\n",
    "                if len(ldf) == 0:\n",
    "                    pass\n",
    "                \n",
    "                else:\n",
    "\n",
    "                    # Remove points corresponding to dud journeys\n",
    "                    gcdf = gcdf.loc[gcdf['JourneyID'].isin(ldf.JourneyID)]\n",
    "\n",
    "                    # Add start and end coordinates to line dataframe\n",
    "                    ldf['start_loc'] = ldf['geometry'].apply(lambda x: Point(x.coords[0]))\n",
    "                    ldf['end_loc'] = ldf['geometry'].apply(lambda x: Point(x.coords[len(x.coords)-1]))\n",
    "\n",
    "                    # Add centroid of journey\n",
    "                    ldf['journey_centroid'] = ldf['geometry'].apply(lambda x: x.centroid)\n",
    "\n",
    "                    # MID is maximal intra-trip displacement. This is a measure of the area a journey covers\n",
    "                    ldf['MID'] = ldf['bbox_utm'].apply(lambda x: (np.sqrt(np.square(x[2] - x[0])+np.square(x[3] - x[1]))))\n",
    "\n",
    "                    # Add displacement between start and end of journey\n",
    "                    ldf['Disp'] = ldf.apply(lambda x: x.start_utm.distance(x.end_utm), axis = 1)\n",
    "\n",
    "                    # append lines and points dataframes for this ID to list objects for later concaternation.\n",
    "                    lines.append(ldf), points.append(gcdf)\n",
    "    \n",
    "    # place into master dataframes\n",
    "    linesdf = pd.concat(lines)\n",
    "    pointsdf = pd.concat(points)   \n",
    "    \n",
    "    print(\"Main Complete: %s\" % time.ctime())\n",
    "    \n",
    "    # return both line and point dataframes.\n",
    "    return linesdf, pointsdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we execute the above function. if mode == 'test', then only the first group of records is processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linesdf, pointsdf = Main(df, mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we send the intermediate, processed frames to file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commencing Save: Tue Sep 18 01:49:31 2018\n",
      "Save complete: Tue Sep 18 01:52:11 2018\n"
     ]
    }
   ],
   "source": [
    "print(\"Commencing Save: %s\" % time.ctime())\n",
    "\n",
    "# Drop useless columns\n",
    "for i in ['n','chng','index','Unnamed: 0','del_lon','del_lat','newjourneyflag']:\n",
    "    try: \n",
    "        pointsdf = pointsdf.drop(i, axis = 1)\n",
    "    except: \n",
    "        pass\n",
    "for i in ['level_1']:\n",
    "    linesdf = linesdf.drop(i, axis = 1)\n",
    "\n",
    "# Save down\n",
    "linesdf.to_csv(os.path.join(workspace, 'journeys.csv'))\n",
    "pointsdf.to_csv(os.path.join(workspace, 'points.csv'))\n",
    "\n",
    "print(\"Save complete: %s\" % time.ctime())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have journeys assembled from the points, we can classify their probable types. We do this by looking at:\n",
    "\n",
    "- 'total_dist': the total distance covered in a single uninterrupted journey\n",
    "- 'Disp': the displacement between the start and end of the journey\n",
    "- 'MID': the maximal intra-trip displacement. This is a measure of the area the journey covers.\n",
    "\n",
    "These parameters are approximations generated by Charles Fox based on nothing other than observation. If you have better ideas for new classification rules / better estimates for the parameters, implement them in this classification function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Classify(x):\n",
    "    home_range = x.total_dist/6\n",
    "    \n",
    "    if x.Disp > 100000 and x.total_dist < x.Disp*3:\n",
    "        status = 'Migration'\n",
    "        return status\n",
    "    elif x.total_dist > 25000 and x.total_dist < x.Disp*3:\n",
    "        status = 'LongTrip'\n",
    "        return status\n",
    "    elif x.total_dist > 25000 and x.Disp < home_range and x.MID > home_range:\n",
    "        status = 'LongCommute'\n",
    "        return status\n",
    "    elif x.total_dist < 25000 and x.Disp < home_range and x.MID > home_range:\n",
    "        status = 'LocalCommute'\n",
    "        return status\n",
    "    elif x.total_dist / max(x.Disp,1) > 5:\n",
    "        status = 'LocalMovement'\n",
    "        return status\n",
    "    elif x.total_dist < 25000:\n",
    "        status = 'LocalTrip'\n",
    "        return status\n",
    "    else:\n",
    "        status = 'Other'\n",
    "    return status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block, apply the customized classification function against each row in the lines (i.e. journyes) dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commencing Classification: Tue Sep 18 01:52:11 2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\charl\\AppData\\Local\\Continuum\\anaconda3\\envs\\GGCW\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Complete: Tue Sep 18 01:53:27 2018\n"
     ]
    }
   ],
   "source": [
    "print(\"Commencing Classification: %s\" % time.ctime())\n",
    "\n",
    "sum_lines = linesdf[['StartTime','start_loc','time_elapsed','end_loc','total_dist','Av.Speed_mph','MID','Disp','geometry']]\n",
    "sum_lines['TripType'] = sum_lines.apply(lambda x: Classify(x), axis = 1)\n",
    "sum_lines.TripType.value_counts()\n",
    "sum_lines.to_csv('journeys.csv') # send the classified lines file to .csv as 'journeys.csv'\n",
    "\n",
    "print(\"Classification Complete: %s\" % time.ctime())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we generate a daily summary dataframe. This dataframe looks only at the lines dataframe assembled in 'Main'. It aims to identify, for each user, the daily summary of their activity, for where they make more than one journey per day. It generates a bounding box for their activity, and also plots a centroid. If these centroids move drastically, then a migration may have occured as the home range has changed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commencing Daily Summary: Tue Sep 18 01:53:27 2018\n",
      "Daily Summary Complete: Tue Sep 18 01:56:20 2018\n"
     ]
    }
   ],
   "source": [
    "print(\"Commencing Daily Summary: %s\" % time.ctime())\n",
    "\n",
    "def summary(x):\n",
    "    y = pd.DataFrame()\n",
    "    y['minx'] = [min(x.minx.tolist())]\n",
    "    y['miny'] = [min(x.miny.tolist())]\n",
    "    y['maxx'] = [max(x.maxx.tolist())]\n",
    "    y['maxy'] = [max(x.maxy.tolist())]\n",
    "    y['journey count'] = len(x)\n",
    "    y['total distance'] = [x.total_dist.sum()]\n",
    "    return y\n",
    "\n",
    "ndf = linesdf.copy()\n",
    "ndf = ndf[['caid','JourneyID','StartTime','total_dist', 'journey_centroid','geometry']]\n",
    "ndf['Day'] = ndf['StartTime'].dt.day\n",
    "ndf['Month'] = ndf['StartTime'].dt.month\n",
    "ndf['journey_bounds'] = ndf['geometry'].apply(lambda x: x.bounds)\n",
    "ndf['minx'] = ndf.journey_bounds.apply(lambda x: x[0])\n",
    "ndf['miny'] = ndf.journey_bounds.apply(lambda x: x[1])\n",
    "ndf['maxx'] = ndf.journey_bounds.apply(lambda x: x[2])\n",
    "ndf['maxy'] = ndf.journey_bounds.apply(lambda x: x[3])\n",
    "ndf = ndf.groupby(['caid', 'Day']).apply(lambda x: summary(x))\n",
    "ndf['bbox'] = ndf.apply(lambda x: box(x.minx, x.miny, x.maxx, x.maxy), axis = 1)\n",
    "ndf['centroid'] = ndf.bbox.apply(lambda x: x.centroid)\n",
    "ndf = ndf.drop(['minx','miny','maxx','maxy'], axis = 1)\n",
    "ndf.to_csv(os.path.join(workspace, 'daily_summary.csv'))\n",
    "\n",
    "print(\"Daily Summary Complete: %s\" % time.ctime())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GGCW)",
   "language": "python",
   "name": "ggcw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
