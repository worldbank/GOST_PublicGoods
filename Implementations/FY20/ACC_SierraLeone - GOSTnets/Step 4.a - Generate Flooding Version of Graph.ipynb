{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our Sierra Leone example, we have flooding data provided by the GFDRR team (Fathom maps). Here, we match this data on to our OSM road network, and adjust travel speeds accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "networkx version: 2.3 \n",
      "osmnx version: 0.9 \n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "from shapely.geometry import Point, MultiPoint\n",
    "from shapely.wkt import loads\n",
    "from scipy import spatial\n",
    "from functools import partial\n",
    "import pyproj\n",
    "from shapely.ops import transform\n",
    "sys.path.append(r'C:\\Users\\charl\\Documents\\GitHub\\GOST_PublicGoods\\GOSTNets\\GOSTNets')\n",
    "import GOSTnet as gn\n",
    "import rasterio\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we choose the flood return period, as well as set the file locations to the relevant rasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_return_period = 20\n",
    "\n",
    "pluv_loc = r'C:\\Users\\charl\\Documents\\GOST\\SierraLeone\\Flooding\\SL_pluvial_undefended'\n",
    "\n",
    "pluvial_raster = os.path.join(pluv_loc, r'SL-PU-{}-1.tif'.format(flood_return_period))\n",
    "\n",
    "fluv_loc = r'C:\\Users\\charl\\Documents\\GOST\\SierraLeone\\Flooding\\SL_fluvial_undefended'\n",
    "\n",
    "fluvial_raster = os.path.join(fluv_loc, r'SL-FU-{}-1.tif'.format(flood_return_period))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read in the raster to memory using rasterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fluv = rasterio.open(fluvial_raster)\n",
    "\n",
    "pluv = rasterio.open(pluvial_raster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create two objects of the raw flood depth arrays, and then find the pair-wise maximum of the two arrays to generate a new array, 'max_depth', which describes the maximum expected depth of flood water, from either pluvial or fluvial floods, that you might expect to occur in DRC over a twenty year time span (where twenty years is the 'return period' of the flooding event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fluv_array = fluv.read(1)\n",
    "pluv_array = pluv.read(1)\n",
    "\n",
    "max_depth = np.maximum(fluv_array, pluv_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write this combined raster out to a new raster file, to keep for use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = r'C:\\Users\\charl\\Documents\\GOST\\SierraLeone\\Flooding\\combined'\n",
    "\n",
    "final_flood_path = os.path.join(out_path,'combo_%s.tif' % flood_return_period)\n",
    "\n",
    "meta = pluv.meta.copy()\n",
    "\n",
    "with rasterio.open(final_flood_path, 'w', **meta) as out:\n",
    "\n",
    "    out.write_band(1, max_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we intersect this flood raster with our road network. First we load in the road network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_pth = r'C:\\Users\\charl\\Documents\\GOST\\SierraLeone\\RoadNet'\n",
    "\n",
    "G = nx.read_gpickle(os.path.join(net_pth, 'final_G.pickle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we intersect the graph's nodes with the combined flood layer to match on the values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_flood = gn.sample_raster(G, final_flood_path, property_name = 'flood_depth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to go through and make sure no 'bad values' were matched on. These are values above 10m or below 0m. We set these back to 0m (no disruption). These values occur if nodes lie either in permanent water bodies or out of the range of the raster in the no data areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u, data in G_flood.nodes(data = True):\n",
    "    fl_val = data['flood_depth']\n",
    "    if 0 < fl_val < 10:\n",
    "        pass\n",
    "    else:\n",
    "        data['flood_depth'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although a function exists in GOSTnets for breaking a graph for a given node property at a given threshold, in this exercise, we want to prevent travel after depths of 0.5m, but linearly increase travel time for flood depths below this. As such, we will copy and paste the 'disrupt network' function, but make a few local changes for our use case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disrupt_network(G, attr, thresh, fail_value = 99999999):\n",
    "    \n",
    "    #### Function for disrupting a graph ####\n",
    "    # REQUIRED: G - a graph containing one or more nodes and one or more edges\n",
    "    #           property - the element in the data dictionary for the edges to test\n",
    "    #           thresh - values of data[property] above this value are disrupted\n",
    "    #           fail_value - the data['time'] property is set to this value to simulate the removal of the edge\n",
    "    # RETURNS:  a modified graph with the edited 'time' attribute\n",
    "    # -------------------------------------------------------------------------#\n",
    "\n",
    "    G_copy = G.copy()     # copy the input graph\n",
    "\n",
    "    broken_nodes, disrupted_nodes = [], []    # generate some empty list objects\n",
    "    \n",
    "    disruption_dict = {}   # generate an empty dictionary\n",
    "\n",
    "    for u, data in G_copy.nodes(data = True):   # Iterate through all nodes, one at a time\n",
    "\n",
    "        if data[attr] > thresh:    # if larger than the threshold...\n",
    "\n",
    "            broken_nodes.append(u)    # record this as a broken node\n",
    "            \n",
    "            disruption_dict[u] = data[attr]  # add node ID and corresponding flood depth to a dictionary\n",
    "            \n",
    "        elif 0 < data[attr] < thresh:    # if flood depth is bigger than 0 - but smaller than thresh:\n",
    "            \n",
    "            disrupted_nodes.append(u)   # record this as a disrupted node\n",
    "            \n",
    "            disruption_dict[u] = data[attr]   # add node ID and corresponding flood depth to a dictionary\n",
    "        \n",
    "        else: # If this node is untouched by the flood....\n",
    "            \n",
    "            disruption_dict[u] = 0    # ....set its dictionary value to 0\n",
    "            \n",
    "    i = 0\n",
    "    \n",
    "    for u, v, data in G_copy.edges(data = True):     # starting a second iteration, this time of edges:\n",
    "\n",
    "        if u in broken_nodes or v in broken_nodes:    # if start or end node is a broken node\n",
    "\n",
    "            data['time_adj'] = fail_value    # prevent travel along this edge - set travel time to high value\n",
    "            i+=1\n",
    "        \n",
    "        elif u in disrupted_nodes or v in disrupted_nodes: # if start or end node is a disrupted node\n",
    "            \n",
    "            depth = max(disruption_dict[u], disruption_dict[v]) # flood depth is max of depth of flood at u and v\n",
    "            \n",
    "            orig_time = data['time_adj'] \n",
    "            \n",
    "            data['time_adj'] = orig_time * (1/ ((thresh - depth) / thresh))  # linearly increase travel times \n",
    "\n",
    "    print('edges disrupted: %s' % i)\n",
    "    \n",
    "    return G_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our custom edge disruption function written, we can now apply this to the flood graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edges disrupted: 7208\n"
     ]
    }
   ],
   "source": [
    "G_flood = disrupt_network(G_flood, 'flood_depth', 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we re-save our graph, knowing that travel will be disrupted in some cases in this model. \n",
    "\n",
    "In order to compare the disrupted to the non-disrupted network, we will have to generate to OD-matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gn.save(G_flood, 'final_G_flood', net_pth, , nodes = False,  edges = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Cfox2)",
   "language": "python",
   "name": "cfox2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
