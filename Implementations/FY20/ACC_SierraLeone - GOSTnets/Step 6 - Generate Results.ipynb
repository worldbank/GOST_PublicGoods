{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Results\n",
    "\n",
    "In this notebook, we will import and manipulate the OD matrix - which should have been calculated separately using the graphtool methodology. We assume this has been done, and in turn, that an OD_matrix.csv file exists in the graphtool folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peartree version: 0.6.1 \n",
      "networkx version: 2.3 \n",
      "matplotlib version: 3.0.3 \n",
      "osmnx version: 0.9 \n",
      "peartree version: 0.6.1 \n",
      "networkx version: 2.3 \n",
      "matplotlib version: 3.0.3 \n",
      "osmnx version: 0.9 \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os, sys\n",
    "sys.path.append(r'C:\\Users\\charl\\Documents\\GitHub\\GOST_PublicGoods\\GOSTNets\\GOSTNets')\n",
    "import GOSTnet as gn\n",
    "import importlib\n",
    "importlib.reload(gn)\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio import features\n",
    "from shapely.wkt import loads\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set path locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "basepth = r'C:\\Users\\charl\\Documents\\GOST\\SierraLeone'\n",
    "graphtool_pth = os.path.join(basepth, 'graphtool')\n",
    "net_pth = basepth\n",
    "pckle = r'final_G.pickle'\n",
    "flood = 'flood'\n",
    "if flood == 'flood':\n",
    "    OD_name = r'OD_matrix_flood.csv'\n",
    "else:\n",
    "    OD_name = r'OD_matrix.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings. Subset refers to which destination file we will be using to subset the larger OD matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_speed = 3\n",
    "WGS = {'init':'epsg:4326'}\n",
    "measure_crs = {'init':'epsg:3857'}\n",
    "date = 'May2019'\n",
    "dest_type = 'schools'\n",
    "subset = r'%s_%s_%s' % (dest_type, date, flood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import All-Destination OD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60905, 8309)\n"
     ]
    }
   ],
   "source": [
    "OD = pd.read_csv(os.path.join(graphtool_pth, OD_name))\n",
    "OD = OD.rename(columns = {'Unnamed: 0':'O_ID'})\n",
    "OD = OD.set_index('O_ID')\n",
    "OD = OD.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "OD_original = OD\n",
    "print(OD_original.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Subset to Accepted Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the destination file which we are currently analyzing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "acceptable_df = pd.read_csv(os.path.join(graphtool_pth, '%s_snapped.csv' % dest_type))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert this pandas DataFrame to a geopandas GeoDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "acceptable_df['geometry'] = acceptable_df['geometry'].apply(loads)\n",
    "acceptable_gdf = gpd.GeoDataFrame(acceptable_df, geometry = 'geometry', crs = {'init':'epsg:4326'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a list of the unique snapped-to nodes in this destination file. Subset the entire OD-matrix accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted_facilities = list(set(list(acceptable_df.NN)))\n",
    "accepted_facilities_str = [str(i) for i in accepted_facilities]\n",
    "OD = OD_original[accepted_facilities_str]\n",
    "acceptable_df.to_csv(os.path.join(basepth,'Output','%s.csv' % subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60905, 7470)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OD.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Walk Time from Final Node to Destination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we collapse the acceptable DF (basically, the destinations) to just the nearest node and nearest node distance column. \n",
    "\n",
    "Then, we work out the approximate striaght-line walking time to the node, and then add this value to the OD matrix values (which represents on-network times) to the destination.\n",
    "\n",
    "Note, we do this in a quite clumsy way as more than one destination may snap to a given node (e.g. in a rural area). As such, we move the entire O-D matrix on to the dest_df object using a for-loop, and then min along this axis later once the travel time from network to the destination has been added to each OD matrix value. I know this is horrid but I have tried and failed to find a better way. \n",
    "\n",
    "For large calculations, this may require a lot of RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_df = acceptable_df[['NN','NN_dist']]\n",
    "\n",
    "dest_df = dest_df.set_index('NN')\n",
    "\n",
    "dest_df['NN_dist'] = dest_df['NN_dist'] / 1000 * 3600 / walk_speed\n",
    "\n",
    "dest_df.index = dest_df.index.map(str)\n",
    "\n",
    "d_f = OD.transpose()\n",
    "\n",
    "for i in d_f.columns:\n",
    "    dest_df[i] = d_f[i]\n",
    "    \n",
    "for i in dest_df.columns:\n",
    "    if i == 'NN_dist':\n",
    "        pass\n",
    "    else:\n",
    "        dest_df[i] = dest_df[i] + dest_df['NN_dist']\n",
    "\n",
    "dest_df = dest_df.drop('NN_dist', axis = 1)\n",
    "\n",
    "dest_df = dest_df.transpose()\n",
    "\n",
    "dest_df['min_time'] = dest_df.min(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Origin Layer for Travel Time Binding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have two of three of our component times calculated: \n",
    "\n",
    "1.) Walking time from the exact DESTINATION point, to the network; \n",
    "\n",
    "2.) Drive time on the network from each origin node to the nearest destination.\n",
    "\n",
    "We still need to add the walking time from the ORIGIN to the network.\n",
    "\n",
    "Now, we re-import our origin points, and set the snapped-to node as the index. Bear in mind this WILL involve duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_name = r'origins_100m_snapped.csv'\n",
    "grid = pd.read_csv(os.path.join(graphtool_pth, grid_name))\n",
    "grid = grid.rename(columns = {'NN':'O_ID','NN_dist':'walk_to_road_net'})\n",
    "grid = grid.set_index(grid['O_ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add a field for each origin point called 'on_network_time'. This is us moving the OD matrix on to the grid (or at least, the min() value - i.e. the on network travel time from each origin node to its closest facility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid['on_network_time'] = dest_df['min_time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we add walk time by translating the walk_to_road_net distance column into a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid['walk_to_road_net'] = grid['walk_to_road_net'] / 1000 * 3600 / walk_speed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we add this to the On-network time to get the total travel time from any given origin node to its closest destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid['total_time_net'] = grid['on_network_time'] + grid['walk_to_road_net']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, it may be faster to simply walk directyly to your destination - never using the road network. This prevents spuriously high travel times which assumes a long snapping distance, short on-network travel, then long walk from the nearest node to the final destination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid['geometry'] = grid['geometry'].apply(loads)\n",
    "o_2_d = gpd.GeoDataFrame(grid, crs = {'init':4326}, geometry = 'geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_2_d = gn.pandana_snap_points(o_2_d, \n",
    "                               acceptable_gdf, \n",
    "                               source_crs='epsg:4326',\n",
    "                               target_crs='epsg:3857',\n",
    "                               add_dist_to_node_col = True)\n",
    "\n",
    "o_2_d['walk_time_direct'] = o_2_d['NN_dist'] / 1000 * 3600 / walk_speed\n",
    "\n",
    "grid['walk_time_direct'] = o_2_d['walk_time_direct']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want the time plotted to be the minimum of directly walking to your destination, and using the road network to get there. So, we take the minimum of the 'total network time' and the time it would take if you walked in a straight line to your destination. \n",
    "\n",
    "For ease of organization, we make it abundantly clear what the units are by including them in the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid['PLOT_TIME_SECS'] = grid[['walk_time_direct','total_time_net']].min(axis = 1)\n",
    "grid['PLOT_TIME_MINS'] = grid['PLOT_TIME_SECS'] / 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also useful to know which model is being used for any given cell (whether they are using the network, or not). We record these results in the 'choice' column, as defined below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choice(x):\n",
    "    if x.walk_time_direct < x.total_time_net:\n",
    "        return 'walk'\n",
    "    else:\n",
    "        return 'net'\n",
    "\n",
    "grid['choice'] = grid.apply(lambda x: choice(x), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can whether the majority of people are using straight line walking rather than the network (the assumption being they use the most time efficient method of getting from A to B):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "walk    5727218\n",
       "net     2860687\n",
       "Name: choice, dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid['choice'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Output Raster from Origin Tif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we take the original raster layer, and burn in the travel times as contain in the grid GeoDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "rst_fn = os.path.join(os.path.join(basepth, 'Origins', 'WP', 'SLE14adjv1.tif'))\n",
    "out_fn = os.path.join(basepth,'Output','%s.tif' % subset)\n",
    "\n",
    "# Update metadata\n",
    "rst = rasterio.open(rst_fn, 'r')\n",
    "meta = rst.meta.copy()\n",
    "D_type = rasterio.float64\n",
    "meta.update(compress='lzw', dtype = D_type, count = 2)\n",
    "\n",
    "with rasterio.open(out_fn, 'w', **meta) as out:\n",
    "    with rasterio.open(rst_fn, 'r') as pop:\n",
    "        \n",
    "        # this is where we create a generator of geom, value pairs to use in rasterizing\n",
    "        shapes = ((geom,value) for geom, value in zip(grid.geometry, grid.PLOT_TIME_MINS))\n",
    "\n",
    "        population = pop.read(1).astype(D_type)\n",
    "        cpy = population.copy()\n",
    "\n",
    "        travel_times = features.rasterize(shapes=shapes, fill=0, out=cpy, transform=out.transform)\n",
    "\n",
    "        out.write_band(1, population)\n",
    "        out.write_band(2, travel_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTIONAL: Generate change rasters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have two similar scenarios that we want to compare, it is useful to generate a change raster to help us pick out the major differences. \n",
    "\n",
    "This process generates a tri band raster. The bands are as follows:\n",
    "\n",
    "Band 1 - The basic input population. This is a copy of the original WorldPop information.\n",
    "Band 2 - The travel time delta between the two scenarios.\n",
    "Band 3 - The population weighted change - i.e. the delta multiplied by the population. This allows us to see from a utilitarian perspective where the largest utility changes are occuring between the two scenarios. \n",
    "\n",
    "This block involves a series of custom inputs (e.g. choice of file names and file paths) - check it carefully before executing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for service_type in ['schools']:\n",
    "    for second_type in ['Senior Secondary','Primary','Pre-Primary','Junior Secondary']:\n",
    "        \n",
    "        WGS = {'init':'epsg:4326'}\n",
    "\n",
    "        basepth = r'C:\\Users\\charl\\Documents\\GOST\\SierraLeone\\Output'\n",
    "\n",
    "        subset = r'%s_May2019_%s' % (service_type, second_type)\n",
    "\n",
    "        pre_raster = os.path.join(basepth, '%s_%s' % (subset,'no_flood.tif'))\n",
    "        post_raster = os.path.join(basepth, '%s_%s' % (subset,'flood.tif'))\n",
    "        out_fn = os.path.join(basepth,'%s_%s_change.tif' % (service_type,second_type))\n",
    "\n",
    "        pre = rasterio.open(pre_raster, 'r')\n",
    "        arr_pre = pre.read(2)\n",
    "\n",
    "        post = rasterio.open(post_raster, 'r')\n",
    "        arr_post = post.read(2)\n",
    "\n",
    "        delta = arr_pre - arr_post\n",
    "\n",
    "        # Update metadata\n",
    "        rst_fn = pre_raster\n",
    "        rst = rasterio.open(rst_fn, 'r')\n",
    "        meta = rst.meta.copy()\n",
    "        D_type = rasterio.float64\n",
    "        meta.update(compress='lzw', dtype = D_type, count = 3)\n",
    "\n",
    "        with rasterio.open(out_fn, 'w', **meta) as out:\n",
    "            with rasterio.open(rst_fn, 'r') as pop:\n",
    "\n",
    "                population = pop.read(1).astype(D_type)\n",
    "\n",
    "                out.write_band(1, population)\n",
    "                out.write_band(2, delta)\n",
    "                out.write_band(3, delta * population)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTIONAL: Automated processing\n",
    "\n",
    "This block is a copy of the above, but designed to execute different scenarios in a loop fashion. Only use this when comfortable with the above processing steps. There are no comments below as it is a clone of the above process - follow that, and you can follow this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "basepth = r'C:\\Users\\charl\\Documents\\GOST\\SierraLeone'\n",
    "graphtool_pth = os.path.join(basepth, 'graphtool')\n",
    "net_pth = basepth\n",
    "pckle = r'final_G.pickle'\n",
    "flood = 'no_flood'\n",
    "\n",
    "if flood == 'flood':\n",
    "    OD_name = r'OD_matrix_flood.csv'\n",
    "else:\n",
    "    OD_name = r'OD_matrix.csv'\n",
    "\n",
    "OD = pd.read_csv(os.path.join(graphtool_pth, OD_name))\n",
    "OD = OD.rename(columns = {'Unnamed: 0':'O_ID'})\n",
    "OD = OD.set_index('O_ID')\n",
    "OD = OD.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "walk_speed = 3\n",
    "WGS = {'init':'epsg:4326'}\n",
    "measure_crs = {'init':'epsg:3857'}\n",
    "date = 'May2019'\n",
    "dest_type = 'schools'\n",
    "\n",
    "template_df = pd.read_csv(os.path.join(graphtool_pth, '%s_snapped.csv' % dest_type))\n",
    "template_df['geometry'] = template_df['geometry'].apply(loads)\n",
    "\n",
    "template_grid = pd.read_csv(os.path.join(graphtool_pth, r'origins_100m_snapped.csv'))\n",
    "template_grid = template_grid.rename(columns = {'NN':'O_ID','NN_dist':'walk_to_road_net'})\n",
    "template_grid = template_grid.set_index(template_grid['O_ID'])\n",
    "\n",
    "for sub_type in template_df.SCHOOL_LEVEL.unique():\n",
    "\n",
    "    subset = r'%s_%s_%s_%s' % (dest_type, date, sub_type, flood)\n",
    "\n",
    "    OD_cut = OD.copy()\n",
    "    acceptable_df = template_df.copy()\n",
    "    grid = template_grid.copy()\n",
    "    \n",
    "    acceptable_df = acceptable_df.loc[acceptable_df.SCHOOL_LEVEL == sub_type]\n",
    "    acceptable_gdf = gpd.GeoDataFrame(acceptable_df, geometry = 'geometry', crs = {'init':'epsg:4326'})\n",
    "\n",
    "    accepted_facilities = list(set(list(acceptable_df.NN)))\n",
    "    accepted_facilities_str = [str(i) for i in accepted_facilities]\n",
    "    OD_cut = OD_cut[accepted_facilities_str]\n",
    "    \n",
    "    acceptable_df.to_csv(os.path.join(basepth,'Output','%s.csv' % subset))\n",
    "\n",
    "    dest_df = acceptable_df[['NN','NN_dist']]\n",
    "    dest_df = dest_df.set_index('NN')\n",
    "    dest_df['NN_dist'] = dest_df['NN_dist'] / 1000 * 3600 / walk_speed\n",
    "    dest_df.index = dest_df.index.map(str)\n",
    "    d_f = OD_cut.transpose()\n",
    "\n",
    "    for i in d_f.columns:\n",
    "        dest_df[i] = d_f[i]\n",
    "\n",
    "    for i in dest_df.columns:\n",
    "        if i == 'NN_dist':\n",
    "            pass\n",
    "        else:\n",
    "            dest_df[i] = dest_df[i] + dest_df['NN_dist']\n",
    "\n",
    "    dest_df = dest_df.drop('NN_dist', axis = 1)\n",
    "    dest_df = dest_df.transpose()\n",
    "    dest_df['min_time'] = dest_df.min(axis = 1)\n",
    "\n",
    "    grid['on_network_time'] = dest_df['min_time']\n",
    "    grid['walk_to_road_net'] = grid['walk_to_road_net'] / 1000 * 3600 / walk_speed \n",
    "    grid['total_time_net'] = grid['on_network_time'] + grid['walk_to_road_net']\n",
    "    grid['geometry'] = grid['geometry'].apply(loads)\n",
    "    o_2_d = gpd.GeoDataFrame(grid, crs = {'init':4326}, geometry = 'geometry')\n",
    "\n",
    "    o_2_d = gn.pandana_snap_points(o_2_d, \n",
    "                                   acceptable_gdf, \n",
    "                                   source_crs='epsg:4326',\n",
    "                                   target_crs='epsg:3857',\n",
    "                                   add_dist_to_node_col = True)\n",
    "\n",
    "    o_2_d['walk_time_direct'] = o_2_d['NN_dist'] / 1000 * 3600 / walk_speed\n",
    "\n",
    "    grid['walk_time_direct'] = o_2_d['walk_time_direct']\n",
    "\n",
    "    grid['PLOT_TIME_SECS'] = grid[['walk_time_direct','total_time_net']].min(axis = 1)\n",
    "    grid['PLOT_TIME_MINS'] = grid['PLOT_TIME_SECS'] / 60\n",
    "\n",
    "    def choice(x):\n",
    "        if x.walk_time_direct < x.total_time_net:\n",
    "            return 'walk'\n",
    "        else:\n",
    "            return 'net'\n",
    "\n",
    "    grid['choice'] = grid.apply(lambda x: choice(x), axis = 1)\n",
    "\n",
    "    rst_fn = os.path.join(os.path.join(basepth, 'Origins', 'WP', 'SLE14adjv1.tif'))\n",
    "    out_fn = os.path.join(basepth,'Output','%s.tif' % subset)\n",
    "\n",
    "    rst = rasterio.open(rst_fn, 'r')\n",
    "    meta = rst.meta.copy()\n",
    "    D_type = rasterio.float64\n",
    "    meta.update(compress='lzw', dtype = D_type, count = 2)\n",
    "\n",
    "    with rasterio.open(out_fn, 'w', **meta) as out:\n",
    "        with rasterio.open(rst_fn, 'r') as pop:\n",
    "\n",
    "            # this is where we create a generator of geom, value pairs to use in rasterizing\n",
    "            shapes = ((geom,value) for geom, value in zip(grid.geometry, grid.PLOT_TIME_MINS))\n",
    "\n",
    "            population = pop.read(1).astype(D_type)\n",
    "            cpy = population.copy()\n",
    "\n",
    "            travel_times = features.rasterize(shapes=shapes, fill=0, out=cpy, transform=out.transform)\n",
    "\n",
    "            out.write_band(1, population)\n",
    "            out.write_band(2, travel_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Cfox2)",
   "language": "python",
   "name": "cfox2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
