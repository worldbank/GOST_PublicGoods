{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we calculate some generalized network statistics about the input network, and save the output as a .csv. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the usual suspects for network / data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import os, sys, time\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Main Function\n",
    "In the next block, we define the main process which is applied against an input network, passed to the function. This function does all of the calculations we are interested in. Note the in-line comments for specifics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NetStats(g, ISO):\n",
    "    # g is a passed in path to a networkx object, in string format. \n",
    "    # ISO is the ISO-3 country code for the country. It is used mainly as a device for keeping track of the output. \n",
    "    \n",
    "    # all results will be loaded into a results dictionary. We define an empty dict here. \n",
    "    results = {}\n",
    "    \n",
    "    # load the Graph\n",
    "    G = nx.read_gpickle(g)\n",
    "    \n",
    "    # the first of our results - load the number of nodes and edges in as entries to the results dict\n",
    "    results['number_of_edges'] = G.number_of_edges()\n",
    "    results['number_of_nodes'] = G.number_of_nodes()\n",
    "    \n",
    "    # genertae the list of connected sub-graphs - usually 1, but often more. \n",
    "    Gs = list(nx.strongly_connected_component_subgraphs(G))\n",
    "    \n",
    "    # Here, we identify the sub-graphs worthy of analysis (set thresholds appropriately!)\n",
    "    iterator = 0\n",
    "    \n",
    "    # we create empty buckets for the edges and nodes, and one for the iterator\n",
    "    counts, edges, nodes = [],[],[]\n",
    "    for g in Gs:\n",
    "        counts.append(iterator)\n",
    "        edges.append(g.number_of_edges())\n",
    "        nodes.append(g.number_of_nodes())\n",
    "        iterator+=1\n",
    "        \n",
    "    # After iterating through all sub-graphs, we load into a dataframe the results. \n",
    "    # Each graph is summarized as a line in this df\n",
    "    df = pd.DataFrame({'id':counts,'edges':edges,'nodes':nodes})\n",
    "    \n",
    "    # we sort by the number of edges, largest graph first\n",
    "    df = df.sort_values(by = 'edges', ascending = False)\n",
    "    \n",
    "    # We set the threshold for graph analysis here. The threshold is - half the number of edges of the largest graph. \n",
    "    thresh = df.edges.iloc[0] * 0.5\n",
    "    \n",
    "    # we remove any sub-graphs that don't meet this newly impoed standard\n",
    "    df = df.loc[df.edges >= thresh]\n",
    "    print(df)\n",
    "    \n",
    "    # we generate a list of the graph IDs through which to iterate in the actual summary statistics stage\n",
    "    id_list = list(df.id)\n",
    "    \n",
    "    # now, we are ready to calculate some stats. We do the following process for each interesting subgraph:\n",
    "    for i in range(0, len(id_list)):\n",
    "        \n",
    "        ### Section 1\n",
    "        # note that all results are appended to the dictionary with 'i' - the graph ID - to\n",
    "        # allow multiple results for each major-network\n",
    "        \n",
    "        # set up timing \n",
    "        start = time.time()\n",
    "        i = 0\n",
    "        \n",
    "        # pick out current graph from Gs, the list of graphs\n",
    "        curr_G = Gs[id_list[i]]\n",
    "        \n",
    "        # generate an UNdirected graph from the current graph for this stage of calcs\n",
    "        undirected_G = nx.Graph(curr_G)\n",
    "        \n",
    "        # calculate cyclomatic number\n",
    "        # https://networkx.github.io/documentation/networkx-1.10/reference/generated/networkx.algorithms.cycles.cycle_basis.html\n",
    "        circuits = nx.cycle_basis(undirected_G)\n",
    "        cyclomatic_number = len(circuits)\n",
    "        results['G%s_cyclomatic_number' % i] = cyclomatic_number\n",
    "        \n",
    "        # get simple number of nodes and edges\n",
    "        e = undirected_G.number_of_edges()\n",
    "        v = undirected_G.number_of_nodes()\n",
    "        results['G%s_number_of_edges'% i] = e\n",
    "        results['G%s_number_of_nodes'% i] = v\n",
    "        \n",
    "        # print out elapsed time for the above calculations\n",
    "        print('\\tTime elapsed for Section 1: %s seconds' % (time.time() - start))\n",
    "        \n",
    "        ### Section 2\n",
    "        start = time.time()\n",
    "        \n",
    "        # calculate the network's alpha, beta and gamma as derivatives from the cyclomatic number + number of nodes and edges\n",
    "        results['G%s_alpha'% i] = cyclomatic_number / ((2 * v) - 5)\n",
    "        results['G%s_beta'% i] = e / v\n",
    "        results['G%s_gamma'% i] = e / (3 * (v - 2))\n",
    "        print('\\tTime elapsed for Section 2: %s seconds' % (time.time() - start))\n",
    "    \n",
    "        ### Section 3\n",
    "        # here, you see some cell magic with the '%' symbol. These commands will only work in a jupyter env. \n",
    "        start = time.time()\n",
    "        \n",
    "        # calculate eccentricity\n",
    "        # https://networkx.github.io/documentation/networkx-1.10/reference/generated/networkx.algorithms.distance_measures.eccentricity.html\n",
    "        %time ecc = nx.eccentricity(undirected_G)\n",
    "        \n",
    "        # calculate network diameter\n",
    "        # https://networkx.github.io/documentation/networkx-1.7/reference/generated/networkx.algorithms.distance_measures.diameter.html\n",
    "        %time results['G%s_diameter' %i] = nx.diameter(undirected_G, ecc)\n",
    "        \n",
    "        # calculate network radius\n",
    "        # https://networkx.github.io/documentation/networkx-1.10/reference/generated/networkx.algorithms.distance_measures.radius.html\n",
    "        %time results['G%s_radius' %i] = nx.radius(undirected_G, ecc)\n",
    "        \n",
    "        # calculate average clustering\n",
    "        # https://networkx.github.io/documentation/networkx-1.9/reference/generated/networkx.algorithms.cluster.average_clustering.html\n",
    "        %time results['G%s_average_clustering' %i] = nx.average_clustering(undirected_G)\n",
    "        \n",
    "        # calculate this massive monster of a mouthful\n",
    "        # https://networkx.github.io/documentation/networkx-1.10/reference/generated/networkx.algorithms.assortativity.degree_assortativity_coefficient.html\n",
    "        %time results['G%s_degree_assortativity_coefficient' %i] = nx.degree_assortativity_coefficient(undirected_G)\n",
    "        \n",
    "        # the following two calcs were very time-expensive, so were commented out\n",
    "        #%time results['G%s_global_efficiency' %i] = nx.global_efficiency(undirected_G)\n",
    "        #%time results['G%s_av_node_connectivity' %i] = nx.average_node_connectivity(undirected_G)\n",
    "        print('\\tTime elapsed for Section 3: %s seconds' % (time.time() - start))\n",
    "        \n",
    "        ### Section 4\n",
    "        start = time.time()\n",
    "        \n",
    "        # here, we calculate some by-node stats (mean).\n",
    "        # We also calculate the median, 1st quartile and 3rd quartile of these results\n",
    "        # https://networkx.github.io/documentation/networkx-1.9/reference/generated/networkx.algorithms.centrality.degree_centrality.html\n",
    "        Z = list(nx.degree_centrality(undirected_G).values())\n",
    "        results['G%s_av_degree_centrality' % i] = np.mean(Z)\n",
    "        results['G%s_0.25_degree_centrality' % i] = np.percentile(Z, 25)\n",
    "        results['G%s_0.50_degree_centrality' % i] = np.percentile(Z, 50)\n",
    "        results['G%s_0.75_degree_centrality' % i] = np.percentile(Z, 75)\n",
    "        print('\\tTime elapsed for Section 4: %s seconds' % (time.time() - start))\n",
    "        \n",
    "        ### Section 5\n",
    "        # same idea as for Section 4 here, but with closeness centrality. \n",
    "        # https://networkx.github.io/documentation/networkx-1.10/reference/generated/networkx.algorithms.centrality.closeness_centrality.html\n",
    "        start = time.time()\n",
    "        Z = list(nx.closeness_centrality(undirected_G).values())\n",
    "        results['G%s_av_closeness_centrality' % i] = np.mean(Z)\n",
    "        results['G%s_0.25_closeness_centrality' % i] = np.percentile(Z, 25)\n",
    "        results['G%s_0.50_closeness_centrality' % i] = np.percentile(Z, 50)\n",
    "        results['G%s_0.75_closeness_centrality' % i] = np.percentile(Z, 75)\n",
    "        print('\\tTime elapsed for Section 5: %s seconds' % (time.time() - start))\n",
    "        \n",
    "        # Section 6\n",
    "        # same idea as for Section 4 here, but with betweenness centrality. \n",
    "        # https://networkx.github.io/documentation/networkx-1.10/reference/generated/networkx.algorithms.centrality.betweenness_centrality.html\n",
    "        start = time.time()\n",
    "        Z = list(nx.betweenness_centrality(undirected_G).values())\n",
    "        results['G%s_av_betweenness_centrality' % i] = np.mean(Z)\n",
    "        results['G%s_0.25_betweenness_centrality' % i] = np.percentile(Z, 25)\n",
    "        results['G%s_0.50_betweenness_centrality' % i] = np.percentile(Z, 50)\n",
    "        results['G%s_0.75_betweenness_centrality' % i] = np.percentile(Z, 75)\n",
    "        print('\\tTime elapsed for Section 6: %s seconds' % (time.time() - start))\n",
    "        \n",
    "        # Section 7\n",
    "        # same idea as for Section 4 here, but with eigenvector centrality. \n",
    "        # https://networkx.github.io/documentation/networkx-1.10/reference/generated/networkx.algorithms.centrality.eigenvector_centrality.html\n",
    "        start = time.time()\n",
    "        try:\n",
    "            Z = list(nx.eigenvector_centrality(undirected_G).values())\n",
    "            results['G%s_av_eigenvector_centrality' % i] = np.mean(Z)\n",
    "            results['G%s_0.25_eigenvector_centrality' % i] = np.percentile(Z, 25)\n",
    "            results['G%s_0.50_eigenvector_centrality' % i] = np.percentile(Z, 50)\n",
    "            results['G%s_0.75_eigenvector_centrality' % i] = np.percentile(Z, 75)\n",
    "        except:\n",
    "            pass\n",
    "        print('\\tTime elapsed for Section 7: %s seconds' % (time.time() - start))\n",
    "        \n",
    "        \"\"\"\n",
    "        This section was again commented out as it was slowing the code to a crawl.\n",
    "        # Section 8\n",
    "        start = time.time()\n",
    "        try:\n",
    "            Z = list(nx.communicability_betweenness_centrality(undirected_G).values())\n",
    "            results['G%s_av_communicability_betweenness_centrality' % i] = np.mean(Z)\n",
    "            results['G%s_0.25_av_communicability_betweenness_centrality' % i] = np.percentile(Z, 25)\n",
    "            results['G%s_0.50_av_communicability_betweenness_centrality' % i] = np.percentile(Z, 50)\n",
    "            results['G%s_0.75_av_communicability_betweenness_centrality' % i] = np.percentile(Z, 75)\n",
    "        except:\n",
    "            pass\n",
    "        print('\\tTime elapsed for Section 8: %s seconds' % (time.time() - start))\n",
    "        \"\"\"\n",
    "    \n",
    "    # generate a dataframe of the results we have calculated, by network\n",
    "    df = pd.DataFrame(results, index = ['value'])\n",
    "    \n",
    "    # here the dataframe is transposed, the index reset, and the index \n",
    "    # renamed to 'var_name' to make it a 2D data table that can be efficiently sliced\n",
    "    df = df.transpose().reset_index().rename(columns = {'index':'var_name'})\n",
    "    \n",
    "    # we add the country of interest as an additional column - the only time we use the ISO variable\n",
    "    df['country'] = ISO\n",
    "    \n",
    "    # return the results df\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...processing AGO\n",
      "   id  edges  nodes\n",
      "0   0   7227   2674\n",
      "\tTime elapsed for Section 1: 0.07486414909362793 seconds\n",
      "\tTime elapsed for Section 2: 0.0 seconds\n",
      "Wall time: 14.5 s\n",
      "Wall time: 0 ns\n",
      "Wall time: 0 ns\n",
      "Wall time: 28 ms\n",
      "Wall time: 25 ms\n",
      "\tTime elapsed for Section 3: 14.518999338150024 seconds\n",
      "\tTime elapsed for Section 4: 0.0010001659393310547 seconds\n",
      "\tTime elapsed for Section 5: 14.598000288009644 seconds\n",
      "\tTime elapsed for Section 6: 19.646999835968018 seconds\n",
      "\tTime elapsed for Section 7: 1.1510004997253418 seconds\n",
      "...processing AIA\n",
      "   id  edges  nodes\n",
      "0   0     94     33\n",
      "\tTime elapsed for Section 1: 0.0 seconds\n",
      "\tTime elapsed for Section 2: 0.0 seconds\n",
      "Wall time: 2 ms\n",
      "Wall time: 0 ns\n",
      "Wall time: 0 ns\n",
      "Wall time: 0 ns\n",
      "Wall time: 991 µs\n",
      "\tTime elapsed for Section 3: 0.0050008296966552734 seconds\n",
      "\tTime elapsed for Section 4: 0.0 seconds\n",
      "\tTime elapsed for Section 5: 0.0029993057250976562 seconds\n",
      "\tTime elapsed for Section 6: 0.0029702186584472656 seconds\n",
      "\tTime elapsed for Section 7: 0.014024734497070312 seconds\n",
      "...processing ALB\n",
      "   id  edges  nodes\n",
      "1   1   7084   2642\n",
      "\tTime elapsed for Section 1: 0.028969287872314453 seconds\n",
      "\tTime elapsed for Section 2: 0.0 seconds\n",
      "Wall time: 14 s\n",
      "Wall time: 0 ns\n",
      "Wall time: 0 ns\n",
      "Wall time: 24 ms\n",
      "Wall time: 24 ms\n",
      "\tTime elapsed for Section 3: 14.067030668258667 seconds\n",
      "\tTime elapsed for Section 4: 0.0019719600677490234 seconds\n",
      "\tTime elapsed for Section 5: 14.198027849197388 seconds\n",
      "\tTime elapsed for Section 6: 19.06000018119812 seconds\n",
      "\tTime elapsed for Section 7: 1.147003173828125 seconds\n",
      "...processing AND\n",
      "   id  edges  nodes\n",
      "0   0    254    104\n",
      "\tTime elapsed for Section 1: 0.0009965896606445312 seconds\n",
      "\tTime elapsed for Section 2: 0.0 seconds\n",
      "Wall time: 22 ms\n",
      "Wall time: 0 ns\n",
      "Wall time: 0 ns\n",
      "Wall time: 0 ns\n",
      "Wall time: 994 µs\n",
      "\tTime elapsed for Section 3: 0.02500152587890625 seconds\n",
      "\tTime elapsed for Section 4: 0.0 seconds\n",
      "\tTime elapsed for Section 5: 0.021998882293701172 seconds\n",
      "\tTime elapsed for Section 6: 0.025002241134643555 seconds\n",
      "\tTime elapsed for Section 7: 0.039997100830078125 seconds\n",
      "...processing ARE\n",
      "   id  edges  nodes\n",
      "0   0  26521   7474\n",
      "\tTime elapsed for Section 1: 0.13796687126159668 seconds\n",
      "\tTime elapsed for Section 2: 0.0 seconds\n",
      "Wall time: 2min 5s\n",
      "Wall time: 0 ns\n",
      "Wall time: 0 ns\n",
      "Wall time: 76 ms\n",
      "Wall time: 72 ms\n",
      "\tTime elapsed for Section 3: 125.73052144050598 seconds\n",
      "\tTime elapsed for Section 4: 0.002970457077026367 seconds\n",
      "\tTime elapsed for Section 5: 124.27603268623352 seconds\n",
      "\tTime elapsed for Section 6: 176.96699738502502 seconds\n",
      "\tTime elapsed for Section 7: 4.0009989738464355 seconds\n",
      "...processing ARG\n",
      "   id  edges  nodes\n",
      "0   0  79812  25876\n",
      "\tTime elapsed for Section 1: 0.49899816513061523 seconds\n",
      "\tTime elapsed for Section 2: 0.0 seconds\n",
      "Wall time: 27min 48s\n",
      "Wall time: 0 ns\n",
      "Wall time: 0 ns\n",
      "Wall time: 253 ms\n",
      "Wall time: 264 ms\n",
      "\tTime elapsed for Section 3: 1668.6909997463226 seconds\n",
      "\tTime elapsed for Section 4: 0.010999679565429688 seconds\n",
      "\tTime elapsed for Section 5: 1708.4533495903015 seconds\n",
      "\tTime elapsed for Section 6: 2987.899998664856 seconds\n",
      "\tTime elapsed for Section 7: 17.343995809555054 seconds\n",
      "...processing ARM\n",
      "   id  edges  nodes\n",
      "0   0   8252   3045\n",
      "\tTime elapsed for Section 1: 0.03296828269958496 seconds\n",
      "\tTime elapsed for Section 2: 0.0 seconds\n",
      "Wall time: 19.8 s\n",
      "Wall time: 1 ms\n",
      "Wall time: 0 ns\n",
      "Wall time: 27 ms\n",
      "Wall time: 27 ms\n",
      "\tTime elapsed for Section 3: 19.836999893188477 seconds\n",
      "\tTime elapsed for Section 4: 0.001001119613647461 seconds\n",
      "\tTime elapsed for Section 5: 18.867029905319214 seconds\n",
      "\tTime elapsed for Section 6: 25.49000120162964 seconds\n",
      "\tTime elapsed for Section 7: 1.1409690380096436 seconds\n",
      "...processing ASM\n",
      "   id  edges  nodes\n",
      "0   0     82     34\n",
      "\tTime elapsed for Section 1: 0.0010001659393310547 seconds\n",
      "\tTime elapsed for Section 2: 0.0 seconds\n",
      "Wall time: 2 ms\n",
      "Wall time: 0 ns\n",
      "Wall time: 0 ns\n",
      "Wall time: 0 ns\n",
      "Wall time: 0 ns\n",
      "\tTime elapsed for Section 3: 0.003999471664428711 seconds\n",
      "\tTime elapsed for Section 4: 0.0 seconds\n",
      "\tTime elapsed for Section 5: 0.0029990673065185547 seconds\n",
      "\tTime elapsed for Section 6: 0.0029997825622558594 seconds\n",
      "\tTime elapsed for Section 7: 0.012999773025512695 seconds\n",
      "...processing ATG\n",
      "   id  edges  nodes\n",
      "0   0    320    107\n",
      "\tTime elapsed for Section 1: 0.00099945068359375 seconds\n",
      "\tTime elapsed for Section 2: 0.0 seconds\n",
      "Wall time: 27 ms\n",
      "Wall time: 0 ns\n",
      "Wall time: 0 ns\n",
      "Wall time: 1 ms\n",
      "Wall time: 2 ms\n",
      "\tTime elapsed for Section 3: 0.03099799156188965 seconds\n",
      "\tTime elapsed for Section 4: 0.0 seconds\n",
      "\tTime elapsed for Section 5: 0.024999380111694336 seconds\n",
      "\tTime elapsed for Section 6: 0.028000831604003906 seconds\n",
      "\tTime elapsed for Section 7: 0.04799914360046387 seconds\n",
      "...processing AUS\n",
      "   id   edges  nodes\n",
      "0   0  111187  36012\n",
      "\tTime elapsed for Section 1: 0.767998218536377 seconds\n",
      "\tTime elapsed for Section 2: 0.0 seconds\n",
      "Wall time: 54min 52s\n",
      "Wall time: 0 ns\n",
      "Wall time: 0 ns\n",
      "Wall time: 355 ms\n",
      "Wall time: 374 ms\n",
      "\tTime elapsed for Section 3: 3292.735030889511 seconds\n",
      "\tTime elapsed for Section 4: 0.016970157623291016 seconds\n",
      "\tTime elapsed for Section 5: 3343.9548015594482 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-55e42542665b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'...processing %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mISO\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'{}_processed.pickle'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mISO\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNetStats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mISO\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr'C:\\Users\\charl\\Documents\\CE\\Criticality\\Netstats'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'%s_processed_netstats.csv'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mISO\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-c48e925df723>\u001b[0m in \u001b[0;36mNetStats\u001b[1;34m(g, ISO)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;31m# Section 6\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m         \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbetweenness_centrality\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mundirected_G\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m         \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'G%s_av_betweenness_centrality'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'G%s_0.25_betweenness_centrality'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpercentile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-229>\u001b[0m in \u001b[0;36mbetweenness_centrality\u001b[1;34m(G, k, normalized, weight, endpoints, seed)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\Cfox_2\\lib\\site-packages\\networkx\\utils\\decorators.py\u001b[0m in \u001b[0;36m_random_state\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    462\u001b[0m         \u001b[0mnew_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m         \u001b[0mnew_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrandom_state_index\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 464\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    465\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_random_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\Cfox_2\\lib\\site-packages\\networkx\\algorithms\\centrality\\betweenness.py\u001b[0m in \u001b[0;36mbetweenness_centrality\u001b[1;34m(G, k, normalized, weight, endpoints, seed)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[0mbetweenness\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_accumulate_endpoints\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbetweenness\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m             \u001b[0mbetweenness\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_accumulate_basic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbetweenness\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m     \u001b[1;31m# rescaling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m     betweenness = _rescale(betweenness, len(G), normalized=normalized,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\Cfox_2\\lib\\site-packages\\networkx\\algorithms\\centrality\\betweenness.py\u001b[0m in \u001b[0;36m_accumulate_basic\u001b[1;34m(betweenness, S, P, sigma, s)\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[0mdelta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msigma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mcoeff\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m             \u001b[0mbetweenness\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mbetweenness\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# set the root path to all of the networks which will be abalyzed. \n",
    "root = r'D:\\Criticality II\\country_networks'\n",
    "\n",
    "# we walk the root path, picking out the pickles. This will look different depending on your file structure. \n",
    "Q = []\n",
    "for q, t, folder in os.walk(root):\n",
    "    if q[-6:] == 'output':\n",
    "        Q.append(q)\n",
    "        \n",
    "# we iterate through each of our valid paths\n",
    "for q in Q:\n",
    "    \n",
    "    # from my folder structure we pick out the ISO code. Again, this is user-specific to their file structure\n",
    "    ISO = q[-10:-7]\n",
    "    if ISO not in ['ABW','AFG']:\n",
    "        \n",
    "        print('...processing %s' % ISO)\n",
    "        \n",
    "        # we define g as the path to the pickled networkx object\n",
    "        g = os.path.join(q, '{}_processed.pickle'.format(ISO))\n",
    "        \n",
    "        # we assign D to be the output dataframe from the netstats function\n",
    "        D = NetStats(g, ISO)\n",
    "        \n",
    "        # we save the stats for this network down to the path location as a .csv\n",
    "        path = r'C:\\Users\\charl\\Documents\\CE\\Criticality\\Netstats'\n",
    "        D.to_csv(os.path.join(path, '%s_processed_netstats.csv' % ISO))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
