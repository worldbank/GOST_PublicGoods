{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Building Attribution\n",
    "In this script, we take a building footprint layer, provided by Digital Globe, and attach a range of standardized characteristics to each building footprint polygon. \n",
    "\n",
    "These characteristics include properties such as area, count of buildings within 25m, 50m and 100m, and the average properties of the closest 5 and 25 buildings. \n",
    "\n",
    "The theory behind this is that these characteristics, about both the building itself and its immediate neighbours, can be used by a machine learning model to identify slum areas - if some training shapefiles on slums are also provided. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import sys, os\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KDTree\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set basic defintions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pth = os.getcwd()+'/merged'\n",
    "WGS = {'init':'epsg:4326'}\n",
    "UTM = {'init':'epsg:32629'}\n",
    "save_thresh = 100000 # save progress every [] rows \n",
    "print_thresh = 10000 # print out calculation process every [] rows for each processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block we import the shapefile, ensure it is projected in WGS 84,  reproject to a metres-based projection, and then add area information. \n",
    "\n",
    "We also calculate the centroid here, whils the data is projected - to ensure that distance based measures are returned in relevant units (meters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fil = gpd.read_file(os.path.join(pth, 'merged.shp'))\n",
    "if fil.crs != WGS:\n",
    "    fil = fil.to_crs(WGS)\n",
    "fil = fil.to_crs(UTM) \n",
    "fil['area'] = fil.area\n",
    "fil['centroid'] = fil['geometry'].centroid\n",
    "fil = fil.to_crs(WGS)\n",
    "fil = fil[['PID','centroid','area']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opportunity to shorten DF for testing purposes in the first line. Otherwise, this block builds the KDTree of the underlying GeoDataFrame. As such, it may take a while to generate, depending on the number of objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "short = fil\n",
    "area_dict = dict(zip(list(short.index), list(short['area'])))\n",
    "matrix = list(zip(short.centroid.apply(lambda x: x.x),short.centroid.apply(lambda x: x.y)))\n",
    "matrix_np = np.asarray(matrix, dtype = float, order = None)\n",
    "KD_tree = KDTree(matrix_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block sets up multiprocessing functionality. It splits up the input DataFrame, short, into chunks based on the available number of threads. This allows the calculations to be spread across multiple threads easily. \n",
    "\n",
    "Users should manually adjust the 'threads' parameter (dtype: int) to avoid taking over all of the available resources on the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 rows completed at Wed May 27 16:02:24 2020\n",
      "10000 rows completed at Wed May 27 16:02:26 2020\n",
      "10000 rows completed at Wed May 27 16:02:29 2020\n",
      "10000 rows completed at Wed May 27 16:02:31 2020\n",
      "20000 rows completed at Wed May 27 16:03:09 2020\n",
      "20000 rows completed at Wed May 27 16:03:11 2020\n",
      "20000 rows completed at Wed May 27 16:03:13 2020\n",
      "20000 rows completed at Wed May 27 16:03:16 2020\n",
      "30000 rows completed at Wed May 27 16:03:50 2020\n",
      "30000 rows completed at Wed May 27 16:03:54 2020\n",
      "30000 rows completed at Wed May 27 16:03:55 2020\n",
      "30000 rows completed at Wed May 27 16:03:58 2020\n",
      "40000 rows completed at Wed May 27 16:04:31 2020\n",
      "40000 rows completed at Wed May 27 16:04:34 2020\n",
      "40000 rows completed at Wed May 27 16:04:35 2020\n",
      "40000 rows completed at Wed May 27 16:04:38 2020\n",
      "50000 rows completed at Wed May 27 16:05:09 2020\n",
      "50000 rows completed at Wed May 27 16:05:12 2020\n",
      "50000 rows completed at Wed May 27 16:05:14 2020\n",
      "50000 rows completed at Wed May 27 16:05:16 2020\n",
      "60000 rows completed at Wed May 27 16:05:47 2020\n",
      "60000 rows completed at Wed May 27 16:05:50 2020\n",
      "60000 rows completed at Wed May 27 16:05:52 2020\n",
      "60000 rows completed at Wed May 27 16:05:54 2020\n"
     ]
    }
   ],
   "source": [
    "threads = multiprocessing.cpu_count()  # limit this line if on the JNB to avoid consuming 100% of resources!\n",
    "\n",
    "d = []\n",
    "\n",
    "for i in range(1, (threads+1)):\n",
    "    len_total_df = len(short)\n",
    "    chunk = int(np.ceil(len_total_df / threads))\n",
    "    d_f = short[(chunk*(i-1)):(chunk*i)]\n",
    "    \n",
    "    processor_input_dict = {\n",
    "        'df':d_f,\n",
    "        'thread_no':i,\n",
    "        'print_thresh':print_thresh,\n",
    "        'save_thresh':save_thresh\n",
    "    }\n",
    "    \n",
    "    d.append(processor_input_dict)\n",
    "\n",
    "bundle = []\n",
    "\n",
    "with Pool(threads) as pool:\n",
    "    results = pool.map(Main,d,chunksize=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define 'Main' - the function called by each processor in the Pool. In each case, it expects a dictionary of passed objects (generated in the previous block). Each thread deals with an identically sized chunk of the original input DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query individual rooftop objects against KD Tree, calculate statistics\n",
    "def Main(passed_dict):\n",
    "    # unpack passed dict into local variables for this thread.\n",
    "    short = passed_dict['df']\n",
    "    thread_no = passed_dict['thread_no']\n",
    "    print_thresh = passed_dict['print_thresh']\n",
    "    save_thresh = passed_dict['save_thresh']\n",
    "    \n",
    "    # set up some counters / timings\n",
    "    t = time.time()\n",
    "    counter = 1\n",
    "    \n",
    "    # iterate through each row of the passed DataFrame of housing polygons.\n",
    "    for index, row in short.iterrows():\n",
    "        \n",
    "        # identify the x and y coordinates of the house's centroid\n",
    "        y = row.centroid.y\n",
    "        x = row.centroid.x\n",
    "        \n",
    "        # Query the KD tree for the first 26 objects (1 will be the house itself.)\n",
    "        # this returns a dataframe of the nearest 26 objects, their distances, and their indices. \n",
    "        distances, indices = KD_tree.query([(x,y)], k = 26)\n",
    "\n",
    "        # Distance calculations - closest 5\n",
    "        # here, we subset the distances frame for the first 5 neighbours, and calculate summary stats\n",
    "        nearest_5_distances = list(distances[0])[1:6]  # subset / slice\n",
    "        min_5 = min(nearest_5_distances) # closest neighbour of the 5 closest (min distance to another building)\n",
    "        max_5 = max(nearest_5_distances) # furthest neighbour of the 5 closest (min distance to another building)\n",
    "        mean_5 = np.mean(nearest_5_distances) # average distance of centroids of 5 nearest neighbours\n",
    "        median_5 = np.median(nearest_5_distances) # median distance of centroids of 5 nearest neighbours\n",
    "        dist_5_std = np.std(nearest_5_distances) # standard deviation of centroids of 5 nearest neighbours\n",
    "\n",
    "        # Distance calculations - closest 25\n",
    "        # here, we subset the distances frame for the first 25 neighbours, and calculate summary stats\n",
    "        nearest_25_distances = list(distances[0])[1:]\n",
    "        min_25 = min(nearest_25_distances)\n",
    "        max_25 = max(nearest_25_distances)\n",
    "        mean_25 = np.mean(nearest_25_distances)\n",
    "        median_25 = np.median(nearest_25_distances)\n",
    "        dist_25_std = np.std(nearest_5_distances)\n",
    "\n",
    "        # Areal calculations - closest 5\n",
    "        # here, instead of the distances frame we generated via the KD tree, we use the area_dict \n",
    "        # and query it with the indices from the KD tree step\n",
    "        indices_5 = list(indices[0])[1:6]\n",
    "        areas = [area_dict[x] for x in indices_5] \n",
    "        area_5_mean = np.mean(areas)  # mean area of 5 nearest neighbours\n",
    "        area_5_med = np.median(areas)  # median area of 5 nearest neighbours\n",
    "        area_5_stdev = np.std(areas)   # standard deviation of area of 5 nearest neighbours\n",
    "\n",
    "        # Areal calculations - closest 25\n",
    "        # repeat above block for closest 25\n",
    "        indices_25 = list(indices[0])[1:]\n",
    "        areas = [area_dict[x] for x in indices_25]\n",
    "        area_25_mean = np.mean(areas)\n",
    "        area_25_med = np.median(areas)\n",
    "        area_25_stdev = np.std(areas)\n",
    "\n",
    "        # Count\n",
    "        # here we turn the process on its head, and identify all objects within certain distance thresholds\n",
    "        count_25m = KD_tree.query_radius([(x,y)], r = 25, count_only = True)[0] # count of buildings in 25m radius\n",
    "        count_50m = KD_tree.query_radius([(x,y)], r = 50, count_only = True)[0] # count of buildings in 50m radius\n",
    "        count_100m = KD_tree.query_radius([(x,y)], r = 100, count_only = True)[0] # count of buildings in 100m radius\n",
    "        \n",
    "        # add these stats to a dictionary called 'ans'\n",
    "        ans = {'PID':row.PID,\n",
    "               'area':row.area,\n",
    "              'dist_5_min':min_5,\n",
    "              'dist_5_max':max_5,\n",
    "              'dist_5_mean':mean_5,\n",
    "              'dist_5_med':median_5,\n",
    "              'dist_5_std':dist_5_std,\n",
    "              'area_5_mean':area_5_mean,\n",
    "              'area_5_med':area_5_med,\n",
    "              'area_5_std':area_5_stdev,\n",
    "              'dist_25_min':min_25,\n",
    "              'dist_25_max':max_25,\n",
    "              'dist_25_mean':mean_25,\n",
    "              'dist_25_med':median_25,\n",
    "              'dist_25_std':dist_25_std,\n",
    "              'area_25_mean':area_25_mean,\n",
    "              'area_25_med':area_25_med,\n",
    "              'area_25_std':area_25_stdev,\n",
    "              'count_25m':count_25m,\n",
    "              'count_50m':count_50m,\n",
    "              'count_100m':count_100m\n",
    "              }\n",
    "\n",
    "        bundle.append(ans)\n",
    "        # keep track of progress via this row\n",
    "        if counter % print_thresh == 0:\n",
    "            print('%s rows completed at %s' % (counter, time.ctime()))\n",
    "        \n",
    "        # this functionality saves progress in case the process cannot be finished in one sitting. \n",
    "        # ideally, finish the processing in one sitting. \n",
    "        old = 0\n",
    "        if counter % save_thresh == 0:\n",
    "            saver = pd.DataFrame(bundle)\n",
    "            saver = saver[list(bundle[0].keys())]\n",
    "            if saver.crs != WGS:\n",
    "                saver = saver.to_crs(WGS)\n",
    "            saver = saver.set_index('PID')\n",
    "            saver = saver.set_index('PID')\n",
    "            saver['geometry'] = saver['geometry']\n",
    "            saver = gpd.GeoDataFrame(saver, geometry = 'geometry', crs = WGS)\n",
    "            saver.to_file(os.path.join(pth, 'output_%s_to_%s_thread_%s.shp' % (old, counter, thread_no)), driver = 'ESRI Shapefile')\n",
    "            old = counter\n",
    "        counter+=1\n",
    "    return bundle\n",
    "    print('Task completed in %s seconds' % (time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_bundle = []\n",
    "for sublist in results:\n",
    "    for item in sublist:\n",
    "        final_bundle.append(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Final Layer\n",
    "Here, we rejoin the original geometry onto our statistics DF via the key field 'PID', and output the resultant file as a shapefile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df = pd.DataFrame(final_bundle)\n",
    "\n",
    "orig_fil = gpd.read_file(os.path.join(pth, 'merged.shp'))\n",
    "if orig_fil.crs != WGS:\n",
    "    orig_fil = orig_fil.to_crs(WGS)\n",
    "orig_fil = orig_fil.set_index('PID')\n",
    "\n",
    "out_df = out_df.set_index('PID')\n",
    "out_df['geometry'] = orig_fil['geometry']\n",
    "out_df = gpd.GeoDataFrame(out_df, geometry = 'geometry', crs = WGS)\n",
    "out_df.to_file(os.path.join(pth, 'buildings_altered.shp'), driver = 'ESRI Shapefile')\n",
    "out_df.to_csv(os.path.join(pth, 'buildings_altered.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
