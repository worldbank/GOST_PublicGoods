{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Machine Learning\n",
    "\n",
    "This script is only functional once Step 1: Bamako Building Attribution has been run.\n",
    "\n",
    "In this script, we import training areas, intersect these with the attributed building foot prints file, and then pass this to an automated machine learning library: h2o's AutoML.\n",
    "\n",
    "CREDITS: Sarah Antos for training area shapefiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries, set file paths and file names. Building_df should be attributed building foot print shapefile from step 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time\n",
    "import pandas as pd\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pth = r'C:\\Users\\charl\\Documents\\GOST\\Bamako'\n",
    "base_fil = r'1243_bamako_building_32629_neighbourInfo.shp'\n",
    "building_df = gpd.read_file(os.path.join(pth, base_fil))\n",
    "building_df.crs = {'init':'epsg:4326'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we import the training shapefiles. These are polygons which intersect the building footprints file. \n",
    "\n",
    "We combine these into a new file, df, which includes each polygon, and its income bracket (slum, rich or mid income). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = ['Middle_Class_Hamdallaye.shp',\n",
    "           'Middle_Class_Hippodrome.shp',\n",
    "           'Middle_Class_Sogoniko.shp',\n",
    "           'TFS_slum.shp',\n",
    "           'High Class.shp']\n",
    "\n",
    "shp_list = []\n",
    "for shp in regions: \n",
    "    shp_df = gpd.read_file(os.path.join(pth, shp))\n",
    "    shp_df['type'] = 'blank'\n",
    "    if shp == 'TFS_slum.shp':\n",
    "        shp_df['type'] = 'slum'\n",
    "    elif shp == 'High Class.shp':\n",
    "        shp_df['type'] = 'rich'\n",
    "    else:\n",
    "        shp_df['type'] = 'mid'\n",
    "    shp_list.append(shp_df[['Name','geometry','type']])\n",
    "df = gpd.GeoDataFrame(pd.concat(shp_list), crs = {'init':'epsg:4326'}, geometry = 'geometry')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these known training classifications, we assign the income bracket from the training data onto the main building footprints file. We then drop all non-attributed footprints. \n",
    "\n",
    "The surviving footprints will serve as our model training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\charl\\AppData\\Local\\Continuum\\anaconda3\\envs\\test\\lib\\site-packages\\pandas\\core\\indexing.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "# set property type default as 'unknown'\n",
    "building_df['type'] = 'unknown'\n",
    "\n",
    "# iterate through training area polygons, assign type from training polygon DataFrame (df).\n",
    "for index, row in df.iterrows():\n",
    "    x = row.geometry\n",
    "    y = row.type\n",
    "    building_df['type'].loc[building_df.intersects(x) == True] = y\n",
    "t = building_df.copy()\n",
    "\n",
    "# drop all other footprints outside training polygons\n",
    "t = t.loc[t['type'].isin(['mid','slum','rich'])]\n",
    "\n",
    "# build_df is now our official 'training data' for our ML model. \n",
    "build_df = t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map the types to a numerical, catagorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_df['type'] = build_df['type'].map({'slum':1,\n",
    "                                        'mid':2,\n",
    "                                        'rich':3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import our machine learning library, including helper functions for exchanging between a Pandas DataFrame and an h2o Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "from h2o.frame import H2OFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shut down any existing h2o servers, initiate a new one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are you sure you want to shutdown the H2O instance running at http://127.0.0.1:54321 (Y/N)? Y\n",
      "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "; Java HotSpot(TM) Client VM (build 25.171-b11, mixed mode, sharing)\n",
      "  Starting server from C:\\Users\\charl\\AppData\\Local\\Continuum\\anaconda3\\envs\\test\\lib\\site-packages\\h2o\\backend\\bin\\h2o.jar\n",
      "  Ice root: C:\\Users\\charl\\AppData\\Local\\Temp\\tmpj3a2j57m\n",
      "  JVM stdout: C:\\Users\\charl\\AppData\\Local\\Temp\\tmpj3a2j57m\\h2o_charl_started_from_python.out\n",
      "  JVM stderr: C:\\Users\\charl\\AppData\\Local\\Temp\\tmpj3a2j57m\\h2o_charl_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>01 secs</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>America/New_York</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.22.1.3</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>10 days </td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_charl_9zrjy3</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>247.5 Mb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>0</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>0</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>Algos, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.6.7 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  -------------------------------\n",
       "H2O cluster uptime:         01 secs\n",
       "H2O cluster timezone:       America/New_York\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.22.1.3\n",
       "H2O cluster version age:    10 days\n",
       "H2O cluster name:           H2O_from_python_charl_9zrjy3\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    247.5 Mb\n",
       "H2O cluster total cores:    0\n",
       "H2O cluster allowed cores:  0\n",
       "H2O cluster status:         accepting new members, healthy\n",
       "H2O connection url:         http://127.0.0.1:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         Algos, AutoML, Core V3, Core V4\n",
       "Python version:             3.6.7 final\n",
       "--------------------------  -------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h2o.cluster().shutdown(prompt=True) \n",
    "h2o.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is a bit clumsy, but allows the constructions of an h20 frame type object from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "expi = {'PID':list(build_df['PID']),\n",
    "        'bArea':list(build_df['bArea']),\n",
    "        'distMean':list(build_df['distMean']),\n",
    "        'distMin':list(build_df['distMin']),\n",
    "        'distMax':list(build_df['distMax']),\n",
    "        'areaMean':list(build_df['areaMean']),\n",
    "        'areaMin':list(build_df['areaMin']),\n",
    "        'areaMax':list(build_df['areaMax']),\n",
    "        'type':list(build_df['type'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "frme = H2OFrame(expi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we explicity define the predictor fields, and our dependent variables (response). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = ['bArea','distMean','distMax','distMin','areaMean','areaMin','areaMax','type']\n",
    "response = 'type'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = frme.split_frame(ratios = [.8], seed = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block of code is fairly h2o standard. It trains 20 models on this data, limiting the runtime to 1 hour. At the end of an hour or training 20 models, whichever is first, it returns a DataFrame of predictions as preds, ordered by the quality of their predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoML progress: |████████████████████████████████████████████████████████| 100%\n",
      "stackedensemble prediction progress: |████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "# Identify predictors and response\n",
    "x = predictors\n",
    "y = response\n",
    "\n",
    "# For binary classification, response should be a factor\n",
    "train[y] = train[y].asfactor()\n",
    "valid[y] = valid[y].asfactor()\n",
    "\n",
    "# Run AutoML for 20 base models (limited to 1 hour max runtime by default)\n",
    "aml = H2OAutoML(max_models=20, seed=1)\n",
    "aml.train(x=x, y=y, training_frame=train)\n",
    "\n",
    "# View the AutoML Leaderboard\n",
    "lb = aml.leaderboard\n",
    "lb.head(rows=lb.nrows)  # Print all rows instead of default (10 rows)\n",
    "\n",
    "preds = aml.leader.predict(valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we print out the performance of our top performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ModelMetricsMultinomialGLM: stackedensemble\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 0.22544477423935919\n",
      "RMSE: 0.4748102507732528\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = aml.leader.model_performance(valid)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the model down to its own save location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = h2o.save_model(model=aml.leader, path=r'C:\\Users\\charl\\Documents\\GOST\\Bamako\\model', force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h2o struggled to generate predictions for more than 100,000 rows at a time. Thus, we split the original DataFrame into 100,000 row chunks, run the predictions on the h2o version of the frame, then send these to file. These predictions could be re-aggregated as desired; but this was not required for this proof of concept. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_short = building_df.copy()\n",
    "df_short = df_short[:100000]\n",
    "\n",
    "# convert to h2o frame\n",
    "expi2 = {'PID':list(df_short['PID']),\n",
    "        'bArea':list(df_short['bArea']),\n",
    "        'distMean':list(df_short['distMean']),\n",
    "        'distMin':list(df_short['distMin']),\n",
    "        'distMax':list(df_short['distMax']),\n",
    "        'areaMean':list(df_short['areaMean']),\n",
    "        'areaMin':list(df_short['areaMin']),\n",
    "        'areaMax':list(df_short['areaMax'])}\n",
    "frme2 = H2OFrame(expi2)\n",
    "\n",
    "# generate predictions from top model\n",
    "preds_2 = aml.leader.predict(frme2)\n",
    "\n",
    "# send back to Pandas DataFrame\n",
    "preds_df = preds_all.as_data_frame()\n",
    "preds_df = preds_df.reset_index()\n",
    "preds_df['New_ID'] = preds_df.index\n",
    "preds_df = preds_df.set_index('New_ID')\n",
    "u = df_short.copy()\n",
    "u = u.reset_index()\n",
    "u['New_ID'] = u.index\n",
    "u = u.set_index('New_ID')\n",
    "u['predict'] = preds_df['predict']\n",
    "u.to_file(os.path.join(r'C:\\Users\\charl\\Documents\\GOST\\Bamako','pred_layer_2.shp'), driver = 'ESRI Shapefile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "stackedensemble prediction progress: |████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "df_short = building_df.copy()\n",
    "df_short = df_short[100000:200000]\n",
    "\n",
    "# convert to h2o frame\n",
    "expi3 = {'PID':list(df_short['PID']),\n",
    "        'bArea':list(df_short['bArea']),\n",
    "        'distMean':list(df_short['distMean']),\n",
    "        'distMin':list(df_short['distMin']),\n",
    "        'distMax':list(df_short['distMax']),\n",
    "        'areaMean':list(df_short['areaMean']),\n",
    "        'areaMin':list(df_short['areaMin']),\n",
    "        'areaMax':list(df_short['areaMax'])}\n",
    "frme3 = H2OFrame(expi3)\n",
    "\n",
    "# generate predictions from top model\n",
    "preds_3 = aml.leader.predict(frme3)\n",
    "\n",
    "# send back to Pandas DataFrame\n",
    "preds_df = preds_3.as_data_frame()\n",
    "preds_df = preds_df.reset_index()\n",
    "preds_df['New_ID'] = preds_df.index\n",
    "preds_df = preds_df.set_index('New_ID')\n",
    "u = df_short.copy()\n",
    "u = u.reset_index()\n",
    "u['New_ID'] = u.index\n",
    "u = u.set_index('New_ID')\n",
    "u['predict'] = preds_df['predict']\n",
    "u.to_file(os.path.join(r'C:\\Users\\charl\\Documents\\GOST\\Bamako','pred_layer_3.shp'), driver = 'ESRI Shapefile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "stackedensemble prediction progress: |████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "df_short = building_df.copy()\n",
    "df_short = df_short[200000:]\n",
    "expi4 = {'PID':list(df_short['PID']),\n",
    "        'bArea':list(df_short['bArea']),\n",
    "        'distMean':list(df_short['distMean']),\n",
    "        'distMin':list(df_short['distMin']),\n",
    "        'distMax':list(df_short['distMax']),\n",
    "        'areaMean':list(df_short['areaMean']),\n",
    "        'areaMin':list(df_short['areaMin']),\n",
    "        'areaMax':list(df_short['areaMax'])}\n",
    "frme4 = H2OFrame(expi4)\n",
    "preds_4 = aml.leader.predict(frme4)\n",
    "preds_df = preds_4.as_data_frame()\n",
    "preds_df = preds_df.reset_index()\n",
    "preds_df['New_ID'] = preds_df.index\n",
    "preds_df = preds_df.set_index('New_ID')\n",
    "u = df_short.copy()\n",
    "u = u.reset_index()\n",
    "u['New_ID'] = u.index\n",
    "u = u.set_index('New_ID')\n",
    "u['predict'] = preds_df['predict']\n",
    "u.to_file(os.path.join(r'C:\\Users\\charl\\Documents\\GOST\\Bamako','pred_layer_4.shp'), driver = 'ESRI Shapefile')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (test)",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
