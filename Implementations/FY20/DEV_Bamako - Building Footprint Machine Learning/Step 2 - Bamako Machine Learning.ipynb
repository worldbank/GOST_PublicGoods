{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Machine Learning\n",
    "\n",
    "This script is only functional once Step 1: Building Attribution has been run.\n",
    "\n",
    "In this script, we import training areas, intersect these with the attributed building foot prints file, and then pass this to an automated machine learning library: h2o's AutoML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries, set file paths and file names. Building_df should be attributed building foot print shapefile from step 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time\n",
    "import pandas as pd\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pth = os.getcwd()+'/merged'\n",
    "base_fil = 'buildings_altered.shp'\n",
    "building_df = gpd.read_file(os.path.join(pth, base_fil))\n",
    "building_df.crs = {'init' :'epsg:4326'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we import the training shapefiles. These are polygons which intersect the building footprints file. \n",
    "\n",
    "We combine these into a new file, df, which includes each polygon, and its income bracket (slum, rich or mid income). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = ['commercial.shp',\n",
    "           'industrial.shp',\n",
    "           'informal_low_income.shp',\n",
    "           'residential_high_income.shp',\n",
    "           'residential_middle_income.shp',\n",
    "           'residential_low_income.shp']\n",
    "\n",
    "shp_list = []\n",
    "for shp in regions: \n",
    "    shp_df = gpd.read_file(os.path.join(pth+'/training_zim', shp))\n",
    "    shp_df['type'] = 'blank'\n",
    "    if shp == 'commercial.shp':\n",
    "        shp_df['type'] = 'commercial'\n",
    "    elif shp == 'industrial.shp':\n",
    "        shp_df['type'] = 'industrial'\n",
    "    elif shp == 'informal_low_income.shp':\n",
    "        shp_df['type'] = 'informal_low_income'\n",
    "    elif shp == 'residential_high_income.shp':\n",
    "        shp_df['type'] = 'residential_high_income'\n",
    "    elif shp == 'residential_middle_income.shp':\n",
    "        shp_df['type'] = 'residential_middle_income'\n",
    "    elif shp == 'residential_low_income.shp':\n",
    "        shp_df['type'] = 'residential_low_income'\n",
    "    shp_list.append(shp_df[['Class','geometry','type']])\n",
    "df = gpd.GeoDataFrame(pd.concat(shp_list), crs = {'init':'epsg:4326'}, geometry = 'geometry')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these known training classifications, we assign the income bracket from the training data onto the main building footprints file. We then drop all non-attributed footprints. \n",
    "\n",
    "The surviving footprints will serve as our model training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex.chunet/anaconda3/envs/ML/lib/python3.6/site-packages/pandas/core/indexing.py:671: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "# set property type default as 'unknown'\n",
    "building_df['type'] = 'unknown'\n",
    "\n",
    "# iterate through training area polygons, assign type from training polygon DataFrame (df).\n",
    "for index, row in df.iterrows():\n",
    "    x = row.geometry\n",
    "    y = row.type\n",
    "    building_df['type'].loc[building_df.intersects(x) == True] = y\n",
    "t = building_df.copy()\n",
    "\n",
    "# drop all other footprints outside training polygons\n",
    "t = t.loc[t['type'].isin(['commercial','industrial','informal_low_income','residential_high_income','residential_middle_income',\n",
    "                         'residential_low_income'])]\n",
    "\n",
    "# build_df is now our official 'training data' for our ML model. \n",
    "build_df = t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map the types to a numerical, categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_df['type'] = build_df['type'].map({'commercial':1,\n",
    "                                        'industrial':2,\n",
    "                                        'informal_low_income':3,\n",
    "                                        'residential_high_income':4,\n",
    "                                        'residential_middle_income':5,\n",
    "                                        'residential_low_income':6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_df['PID'] = build_df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import our machine learning library, including helper functions for exchanging between a Pandas DataFrame and an h2o Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "from h2o.frame import H2OFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shut down any existing h2o servers, initiate a new one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321 ..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "  Java Version: java version \"10.0.1\" 2018-04-17; Java(TM) SE Runtime Environment 18.3 (build 10.0.1+10); Java HotSpot(TM) 64-Bit Server VM 18.3 (build 10.0.1+10, mixed mode)\n",
      "  Starting server from /Users/alex.chunet/anaconda3/envs/ML/lib/python3.6/site-packages/h2o/backend/bin/h2o.jar\n",
      "  Ice root: /var/folders/hv/sntp5ybs2bl6hqtgl4pb1p2r0000gn/T/tmpa5b0exba\n",
      "  JVM stdout: /var/folders/hv/sntp5ybs2bl6hqtgl4pb1p2r0000gn/T/tmpa5b0exba/h2o_alex_chunet_started_from_python.out\n",
      "  JVM stderr: /var/folders/hv/sntp5ybs2bl6hqtgl4pb1p2r0000gn/T/tmpa5b0exba/h2o_alex_chunet_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n",
      "Warning: Your H2O cluster version is too old (9 months and 12 days)! Please download and install the latest version from http://h2o.ai/download/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>02 secs</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>Europe/Paris</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.28.1.2</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>9 months and 12 days !!!</td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_alex_chunet_yd4q3e</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>2 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>{'http': None, 'https': None}</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.6.10 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ------------------------------------------------------------------\n",
       "H2O cluster uptime:         02 secs\n",
       "H2O cluster timezone:       Europe/Paris\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.28.1.2\n",
       "H2O cluster version age:    9 months and 12 days !!!\n",
       "H2O cluster name:           H2O_from_python_alex_chunet_yd4q3e\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    2 Gb\n",
       "H2O cluster total cores:    4\n",
       "H2O cluster allowed cores:  4\n",
       "H2O cluster status:         accepting new members, healthy\n",
       "H2O connection url:         http://127.0.0.1:54321\n",
       "H2O connection proxy:       {'http': None, 'https': None}\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4\n",
       "Python version:             3.6.10 final\n",
       "--------------------------  ------------------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#h2o.cluster().shutdown(prompt=True)\n",
    "h2o.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is a bit clumsy, but allows the constructions of an h20 frame type object from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "expi_f = {'PID':list(build_df['PID']),\n",
    "        'area':list(build_df['area']),\n",
    "        'dist_5_min':list(build_df['dist_5_min']),\n",
    "        'dist_5_max':list(build_df['dist_5_max']),\n",
    "        'dist_5_mea':list(build_df['dist_5_mea']),\n",
    "        'dist_5_med':list(build_df['dist_5_med']),\n",
    "        'dist_5_std':list(build_df['dist_5_std']),\n",
    "        'area_5_mea':list(build_df['area_5_mea']),\n",
    "        'area_5_med':list(build_df['area_5_med']),\n",
    "        'area_5_std':list(build_df['area_5_std']),\n",
    "        'dist_25_mi':list(build_df['dist_25_mi']),\n",
    "        'dist_25_ma':list(build_df['dist_25_ma']),\n",
    "        'dist_25_me':list(build_df['dist_25_me']),\n",
    "        'dist_25__1':list(build_df['dist_25__1']),\n",
    "        'dist_25_st':list(build_df['dist_25_st']),\n",
    "        'area_25_me':list(build_df['area_25_me']),\n",
    "        'area_25__1':list(build_df['area_25__1']),\n",
    "        'area_25_st':list(build_df['area_25_st']),\n",
    "        'count_25m':list(build_df['count_25m']),\n",
    "        'count_50m':list(build_df['count_50m']),\n",
    "        'count_100m':list(build_df['count_100m']),\n",
    "        'type':list(build_df['type'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "frme = H2OFrame(expi_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we explicity define the predictor fields, and our dependent variables (response). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = ['area', 'dist_5_min', 'dist_5_max', 'dist_5_mea', 'dist_5_med',\n",
    "       'dist_5_std', 'area_5_mea', 'area_5_med', 'area_5_std', 'dist_25_mi',\n",
    "       'dist_25_ma', 'dist_25_me', 'dist_25__1', 'dist_25_st', 'area_25_me',\n",
    "       'area_25__1', 'area_25_st', 'count_25m', 'count_50m', 'count_100m']\n",
    "response = 'type'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = frme.split_frame(ratios = [.8], seed = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block of code is fairly h2o standard. It trains 20 models on this data, limiting the runtime to 1 hour. At the end of an hour or training 20 models, whichever is first, it returns a DataFrame of predictions as preds, ordered by the quality of their predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoML progress: |████████████████████████████████████████████████████████| 100%\n",
      "stackedensemble prediction progress: |████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "# Identify predictors and response\n",
    "x = predictors\n",
    "y = response\n",
    "\n",
    "# For binary classification, response should be a factor\n",
    "train[y] = train[y].asfactor()\n",
    "valid[y] = valid[y].asfactor()\n",
    "\n",
    "# Run AutoML for 20 base models (limited to 1 hour max runtime by default)\n",
    "aml = H2OAutoML(max_models=20, seed=1)\n",
    "aml.train(x=x, y=y, training_frame=train)\n",
    "\n",
    "# View the AutoML Leaderboard\n",
    "lb = aml.leaderboard\n",
    "lb.head(rows=lb.nrows)  # Print all rows instead of default (10 rows)\n",
    "\n",
    "preds = aml.leader.predict(valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we print out the performance of our top performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ModelMetricsMultinomialGLM: stackedensemble\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 0.12220657961186379\n",
      "RMSE: 0.3495805767085234\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = aml.leader.model_performance(valid)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix: Row labels: Actual class; Column labels: Predicted class\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>Error</th>\n",
       "      <th>Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.374101</td>\n",
       "      <td>52 / 139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.0</td>\n",
       "      <td>399.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.101351</td>\n",
       "      <td>45 / 444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.138686</td>\n",
       "      <td>19 / 137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>437.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.111789</td>\n",
       "      <td>55 / 492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.270758</td>\n",
       "      <td>75 / 277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>463.0</td>\n",
       "      <td>0.049281</td>\n",
       "      <td>24 / 487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>99.0</td>\n",
       "      <td>448.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>536.0</td>\n",
       "      <td>267.0</td>\n",
       "      <td>499.0</td>\n",
       "      <td>0.136640</td>\n",
       "      <td>270 / 1,976</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      1      2      3      4      5      6     Error         Rate\n",
       "0  87.0   39.0    0.0   11.0    2.0    0.0  0.374101     52 / 139\n",
       "1  12.0  399.0    1.0   27.0    5.0    0.0  0.101351     45 / 444\n",
       "2   0.0    0.0  118.0   10.0    4.0    5.0  0.138686     19 / 137\n",
       "3   0.0    8.0    4.0  437.0   37.0    6.0  0.111789     55 / 492\n",
       "4   0.0    2.0    0.0   48.0  202.0   25.0  0.270758     75 / 277\n",
       "5   0.0    0.0    4.0    3.0   17.0  463.0  0.049281     24 / 487\n",
       "6  99.0  448.0  127.0  536.0  267.0  499.0  0.136640  270 / 1,976"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the model down to its own save location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = h2o.save_model(model=aml.leader, path='/Users/alex.chunet/Documents/Repositories/GEO_ML/Bamako_building attribution', force=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h2o struggled to generate predictions for more than 100,000 rows at a time. Thus, we split the original DataFrame into 100,000 row chunks, run the predictions on the h2o version of the frame, then send these to file. These predictions could be re-aggregated as desired; but this was not required for this proof of concept. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_df['PID'] = building_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "bef = [0,100000,200000,300000,400000,500000,600000,700000,800000,900000,1000000,1100000,1200000]\n",
    "af = [100000,200000,300000,400000,500000,600000,700000,800000,900000,1000000,1100000,1200000,1300000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 100000\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "stackedensemble prediction progress: |████████████████████████████████████| 100%\n",
      "100000 200000\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "stackedensemble prediction progress: |████████████████████████████████████| 100%\n",
      "200000 300000\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "stackedensemble prediction progress: |████████████████████████████████████| 100%\n",
      "300000 400000\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "stackedensemble prediction progress: |████████████████████████████████████| 100%\n",
      "400000 500000\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "stackedensemble prediction progress: |████████████████████████████████████| 100%\n",
      "500000 600000\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "stackedensemble prediction progress: |████████████████████████████████████| 100%\n",
      "600000 700000\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "stackedensemble prediction progress: |████████████████████████████████████| 100%\n",
      "700000 800000\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "stackedensemble prediction progress: |████████████████████████████████████| 100%\n",
      "800000 900000\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "stackedensemble prediction progress: |████████████████████████████████████| 100%\n",
      "900000 1000000\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "stackedensemble prediction progress: |████████████████████████████████████| 100%\n",
      "1000000 1100000\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "stackedensemble prediction progress: |████████████████████████████████████| 100%\n",
      "1100000 1200000\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "stackedensemble prediction progress: |████████████████████████████████████| 100%\n",
      "1200000 1300000\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "stackedensemble prediction progress: |████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "for x,y in zip(bef, af):\n",
    "    print(x,y)\n",
    "    df_short = building_df.copy()\n",
    "    df_short = df_short[x:y]\n",
    "\n",
    "    # convert to h2o frame\n",
    "    expi = {'PID':list(df_short['PID']),\n",
    "            'area':list(df_short['area']),\n",
    "            'dist_5_min':list(df_short['dist_5_min']),\n",
    "            'dist_5_max':list(df_short['dist_5_max']),\n",
    "            'dist_5_mea':list(df_short['dist_5_mea']),\n",
    "            'dist_5_med':list(df_short['dist_5_med']),\n",
    "            'dist_5_std':list(df_short['dist_5_std']),\n",
    "            'area_5_mea':list(df_short['area_5_mea']),\n",
    "            'area_5_med':list(df_short['area_5_med']),\n",
    "            'area_5_std':list(df_short['area_5_std']),\n",
    "            'dist_25_mi':list(df_short['dist_25_mi']),\n",
    "            'dist_25_ma':list(df_short['dist_25_ma']),\n",
    "            'dist_25_me':list(df_short['dist_25_me']),\n",
    "            'dist_25__1':list(df_short['dist_25__1']),\n",
    "            'dist_25_st':list(df_short['dist_25_st']),\n",
    "            'area_25_me':list(df_short['area_25_me']),\n",
    "            'area_25__1':list(df_short['area_25__1']),\n",
    "            'area_25_st':list(df_short['area_25_st']),\n",
    "            'count_25m':list(df_short['count_25m']),\n",
    "            'count_50m':list(df_short['count_50m']),\n",
    "            'count_100m':list(df_short['count_100m'])}\n",
    "    frme = H2OFrame(expi)\n",
    "\n",
    "    # generate predictions from top model\n",
    "    preds = aml.leader.predict(frme)\n",
    "\n",
    "    # send back to Pandas DataFrame\n",
    "    preds_df = preds.as_data_frame()\n",
    "    preds_df = preds_df.reset_index()\n",
    "    preds_df['New_ID'] = preds_df.index\n",
    "    preds_df = preds_df.set_index('New_ID')\n",
    "    u = df_short.copy()\n",
    "    u = u.reset_index()\n",
    "    u['New_ID'] = u.index\n",
    "    u = u.set_index('New_ID')\n",
    "    u['predict'] = preds_df['predict']\n",
    "    u.to_file(os.path.join('/Users/alex.chunet/Documents/Repositories/GEO_ML/Bamako_building attribution/output_zim','pred_layer_%s_%s.shp' % (x, y)), driver = 'ESRI Shapefile')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_in=\"\"\n",
    "path_out =\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred2 = gpd.read_file(path_in+'pred_layer_2.shp')\n",
    "pred3 = gpd.read_file(path_in+'pred_layer_3.shp')\n",
    "pred4 = gpd.read_file(path_in+'pred_layer_4.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.concat([pred2, pred3, pred4], join=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.to_file(os.path.join(path,'merged.shp'), driver = 'ESRI Shapefile')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
