{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - Generate Results\n",
    "\n",
    "This script has gone through a lot of development, and is now in two version - with the 'operational' one being the automated version also labelled as Step 4 in this folder. \n",
    "\n",
    "This is really where most of the complexity lies in the Yemen analysis, as we have been asked to do cuts and analyses for the WHO and UNICEF teams on the ground in Yemen that no other team has (as of yet!) requested from the GOST team. \n",
    "\n",
    "This is a good script for learning what happens in the even more complicated Automated version. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the usual suspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peartree version: 0.6.1 \n",
      "networkx version: 2.3 \n",
      "matplotlib version: 3.0.3 \n",
      "osmnx version: 0.9 \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os, sys\n",
    "sys.path.append(r'C:\\Users\\charl\\Documents\\GitHub\\GOST_PublicGoods\\GOSTNets\\GOSTNets')\n",
    "sys.path.append(r'C:\\Users\\charl\\Documents\\GitHub\\GOST')\n",
    "import GOSTnet as gn\n",
    "import importlib\n",
    "import geopandas as gpd\n",
    "import rasterio as rt\n",
    "from rasterio import features\n",
    "from shapely.wkt import loads\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from shapely.geometry import box, Point, Polygon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings\n",
    "\n",
    "Here, we build our scenario for this run through of Generate Results (which must be run multiple times for multiple scenarios). \n",
    "\n",
    "It is this bit which is effectively automated in the 'automated version' of the script. Here, we set the 'scenario variables':\n",
    "- whether or not to model access to a motor vehicle for driving along roads; \n",
    "- whether to incorporate a version of the graph with adjustments made for conflict (i.e. road closures, conflicts); \n",
    "- the year of the analysis;\n",
    "- whether we are looking at access to hosptials, Primary HealthCare facilities (PHCs) or both, \n",
    "- the range of services which we are analyzing access to (a subset of the facilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "walking = 0 # set to 1 for walking, 0 for driving.\n",
    "\n",
    "conflict = 1 # set to 1 to prevent people from crossing warfronts, and to incorporate road closures per UN logistics cluster data\n",
    "\n",
    "facility_type = 'ALL'   # Options: 'HOS' or 'PHC' or 'ALL'\n",
    "\n",
    "year = 2018   # default = 2018; can be 2016 if 2016 origin layer prepared\n",
    "\n",
    "service_index = 8 # Set to 0 for all services / access to hospitals. A choice from the next list.\n",
    "\n",
    "services = ['ALL',\n",
    "            'Antenatal',\n",
    "            'BEmONC',\n",
    "            'CEmONC',\n",
    "            'Under_5',\n",
    "            'Emergency_Surgery',\n",
    "            'Immunizations',\n",
    "            'Malnutrition',\n",
    "            'Int_Outreach']\n",
    "\n",
    "zonal_stats = 1 # set to 1 to produce summary zonal stats layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import All-Destination OD\n",
    "\n",
    "The OD will have on one axis all of the uniquely snapped-to nodes for the origin points, and on the other, all of the uniquely snapped-to nodes amongst the destinations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "basepth = r'C:\\Users\\charl\\Documents\\GOST\\Yemen'\n",
    "\n",
    "# path to the graphtool folder for the files that generated the OD matrix\n",
    "pth = os.path.join(basepth, 'graphtool')\n",
    "\n",
    "# path for utility files, e.g. admin boundaries, warfront file\n",
    "pth = os.path.join(basepth, 'util_files')\n",
    "\n",
    "# path to the SRTM tiles used for matching on node elevation\n",
    "pth = os.path.join(basepth, 'SRTM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block, we translate some of the settings elected above into file name suffixes to help us tell different outputs apart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'walking' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3d0cdcc8ec98>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# in the event walking is on, we want the walk graph, not the normal driving graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[0mwalking\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mtype_tag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'walking'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mnet_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr'walk_graph.pickle'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'walking' is not defined"
     ]
    }
   ],
   "source": [
    "# in the event walking is on, we want the walk graph, not the normal driving graph. \n",
    "if walking == 1:\n",
    "    type_tag = 'walking'\n",
    "    net_name = r'walk_graph.pickle'\n",
    "else:\n",
    "    type_tag = 'driving'\n",
    "    # this network has both conflict and non-conflict drive times on its edges. \n",
    "    net_name = r'G_salty_time_conflict_adj.pickle'\n",
    "\n",
    "# if we are looking at conflict, at the ConflictAdj suffix to output files\n",
    "if conflict == 1: \n",
    "    conflict_tag = 'ConflictAdj'\n",
    "else:\n",
    "    conflict_tag = 'NoConflict'\n",
    "\n",
    "# our path to our OD matrix should be the graphtool folder (most of the time!). Adjust if necessary\n",
    "OD_pth = pth\n",
    "OD_name = r'output_Jan24th_%s.csv' % type_tag\n",
    "\n",
    "# same is also true of our network path - adjust if necessary\n",
    "net_pth = pth\n",
    "\n",
    "# Define our CRS for the analysis - WGS84 and projected. \n",
    "WGS = {'init':'epsg:4326'}\n",
    "measure_crs = {'init':'epsg:32638'}\n",
    "\n",
    "# Here we build the whole filename based on our choice of settings, and print those beneath\n",
    "subset = r'%s_24th_HERAMS_%s_%s_%s_%s' % (type_tag, facility_type, services[service_index], conflict_tag, year)\n",
    "print(\"Output files will have name: \", subset)\n",
    "print(\"network: \",net_name)\n",
    "print(\"OD Matrix: \",OD_name)\n",
    "print(\"Conflict setting: \",conflict_tag)\n",
    "\n",
    "# set the speed at which people are assumed to move over open ground with no paths\n",
    "# e.g. when walking from desitnation node to actual destination. KEY ASSUMPTION!!\n",
    "offroad_speed = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in OD Matrix\n",
    "Here, we read in our OD matrix, and do a bit of housekeeping on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OD = pd.read_csv(os.path.join(OD_pth, OD_name))\n",
    "OD = OD.rename(columns = {'Unnamed: 0':'O_ID'})\n",
    "OD = OD.set_index('O_ID')\n",
    "OD = OD.replace([np.inf, -np.inf], np.nan)\n",
    "OD_original = OD.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset to Accepted Nodes\n",
    "In this block, we do simila operations to the HeRAMS file, including:\n",
    "- removing facilities which aren't of the selected type\n",
    "- removing non-operational facilities\n",
    "- If selected, dropping facilities which don't offer the required services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2373"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initial read in\n",
    "acceptable_df = pd.read_csv(os.path.join(OD_pth, 'HeRAMS 2018 April_snapped.csv'))\n",
    "\n",
    "# Adjust for facility type - drop facilities as necessary. 1 = hospital, 2/3 = PHC\n",
    "if facility_type == 'HOS':\n",
    "    acceptable_df = acceptable_df.loc[acceptable_df['Health Facility Type Coded'].isin(['1',1])]\n",
    "elif facility_type == 'PHC':\n",
    "    acceptable_df = acceptable_df.loc[acceptable_df['Health Facility Type Coded'].isin([2,'2',3,'3'])]\n",
    "elif facility_type == 'ALL':\n",
    "    pass\n",
    "else:\n",
    "    raise ValueError('unacceptable facility_type entry!')\n",
    "\n",
    "# Adjust for functionality in a given year. Functioning facilities given by a 1 or a 2.\n",
    "acceptable_df = acceptable_df.loc[acceptable_df['Functioning %s' % year].isin(['1','2',1,2])]\n",
    "\n",
    "# Adjust for availability of service.\n",
    "# first we build a dictionary of all the columns we care about with a standardized KEY (note: not value!)\n",
    "# we use the keys themselves to then subset the Pandas DF\n",
    "SERVICE_DICT = {'Antenatal_2018':'ANC 2018',\n",
    "               'Antenatal_2016':'Antenatal Care (P422) 2016',\n",
    "               'BEmONC_2018':'Basic emergency obstetric care 2018',\n",
    "               'BEmONC_2016':'Basic Emergency Obsteteric Care (P424) 2016',\n",
    "               'CEmONC_2018':'Comprehensive emergency obstetric care 2018',\n",
    "               'CEmONC_2016':'Comprehensive Emergency Obstetric Care (S424) 2016',\n",
    "               'Under_5_2018':'Under 5 clinics 2018',\n",
    "               'Under_5_2016':'Under-5 clinic services (P23) 2016',\n",
    "               'Emergency_Surgery_2018':'Emergency and elective surgery 2018',\n",
    "               'Emergency_Surgery_2016':'Emergency and Elective Surgery (S14) 2016',\n",
    "               'Immunizations_2018':'EPI 2018',\n",
    "               'Immunizations_2016':'EPI (P21a) 2016',\n",
    "               'Malnutrition_2018':'Malnutrition services 2018',\n",
    "               'Malnutrition_2016':'Malnutrition services (P25) 2016',\n",
    "               'Int_Outreach_2018':'Integrated outreach (IMCI+EPI+ANC+Nutrition_Services) 2018',\n",
    "               'Int_Outreach_2016':'Integrated Outreach (P22) 2016'}\n",
    "\n",
    "if service_index == 0:\n",
    "    pass\n",
    "else:\n",
    "    # take this line slowly...\n",
    "    acceptable_df = acceptable_df.loc[acceptable_df[SERVICE_DICT['%s_%s' % (services[service_index],year)]].isin(['1',1])]\n",
    "\n",
    "# print out the length of the acceptable_df - which is, at this point, the list of valid destinations for this analysis. \n",
    "len(acceptable_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OD-Matrix slicing for valid destinations\n",
    "\n",
    "In this section, we pick out the snapped-to nodes for the remaining valid destinations, and then slice the larger OD matrix accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36624, 3824)\n",
      "(36624, 2182)\n"
     ]
    }
   ],
   "source": [
    "# load the geometry column, make it a GeoDataFrame\n",
    "acceptable_df['geometry'] = acceptable_df['geometry'].apply(loads)\n",
    "acceptable_gdf = gpd.GeoDataFrame(acceptable_df, geometry = 'geometry', crs = {'init':'epsg:4326'})\n",
    "\n",
    "# convert types of nearest node (currently stored as int) into string, find unique set. \n",
    "accepted_facilities = list(set(list(acceptable_df.NN)))\n",
    "accepted_facilities_str = [str(i) for i in accepted_facilities]\n",
    "\n",
    "# keep ONLY the columns in the OD-Matrix relating to nodes snapped to by these destinations. \n",
    "# Massive reduction in size of the OD-matrix in most cases\n",
    "OD = OD_original[accepted_facilities_str]\n",
    "\n",
    "# Send to file the destination .csv - used for generating outputs as the facility locations in this case\n",
    "acceptable_df.to_csv(os.path.join(basepth,'output_layers','Round 3','%s.csv' % subset))\n",
    "\n",
    "# print out dimensions to make sure we are on track. \n",
    "print(OD_original.shape)\n",
    "print(OD.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to add elevation to a point GeoDataFrame\n",
    "\n",
    "This function takes the work in Step 3.a, and functionalizes it - allowing us to add an elevation field to a point GeoDataFrame, assuming we have a path to all the SRTM tiles and we have denoted x and y Lat / Long columns in WGS84. See Step 3.a for a detailed walkthrough of this function's methodology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_elevation(df, x, y, srtm_pth):\n",
    "    \n",
    "    # walk all tiles, find path\n",
    "    tiles = []\n",
    "    for root, folder, files in os.walk(os.path.join(srtm_pth,'high_res')):\n",
    "        for f in files:\n",
    "            if f[-3:] == 'hgt':\n",
    "                tiles.append(f[:-4])\n",
    "\n",
    "    # load dictionary of tiles\n",
    "    arrs = {}\n",
    "    for t in tiles:\n",
    "        arrs[t] = rt.open(srtm_pth+r'\\high_res\\{}.hgt\\{}.hgt'.format(t, t), 'r')\n",
    "\n",
    "    # assign a code\n",
    "    uniques = []\n",
    "    df['code'] = 'placeholder'\n",
    "    def tile_code(z):\n",
    "        E = str(z[x])[:2]\n",
    "        N = str(z[y])[:2]\n",
    "        return 'N{}E0{}'.format(N, E)\n",
    "    df['code'] = df.apply(lambda z: tile_code(z), axis = 1)\n",
    "    unique_codes = list(set(df['code'].unique()))\n",
    "    \n",
    "    z = {}\n",
    "    \n",
    "    # Match on High Precision Elevation\n",
    "    property_name = 'elevation'\n",
    "    for code in unique_codes:\n",
    "        \n",
    "        df2 = df.copy()\n",
    "        df2 = df2.loc[df2['code'] == code]\n",
    "        dataset = arrs[code]\n",
    "        b = dataset.bounds\n",
    "        datasetBoundary = box(b[0], b[1], b[2], b[3])\n",
    "        selKeys = []\n",
    "        selPts = []\n",
    "        for index, row in df2.iterrows():\n",
    "            if Point(row[x], row[y]).intersects(datasetBoundary):\n",
    "                selPts.append((row[x],row[y]))\n",
    "                selKeys.append(index)\n",
    "        raster_values = list(dataset.sample(selPts))\n",
    "        raster_values = [x[0] for x in raster_values]\n",
    "\n",
    "        # generate new dictionary of {node ID: raster values}\n",
    "        z.update(zip(selKeys, raster_values))\n",
    "        \n",
    "    elev_df = pd.DataFrame.from_dict(z, orient='index')\n",
    "    elev_df.columns = ['elevation']\n",
    "    \n",
    "    # match on low-precision elevation\n",
    "    missing = elev_df.copy()\n",
    "    missing = missing.loc[missing.elevation < 0]\n",
    "    if len(missing) > 0:\n",
    "        missing_df = df.copy()\n",
    "        missing_df = missing_df.loc[missing.index]\n",
    "        low_res_tifpath = os.path.join(srtm_pth, 'clipped', 'clipped_e20N40.tif')\n",
    "        dataset = rt.open(low_res_tifpath, 'r')\n",
    "        b = dataset.bounds\n",
    "        datasetBoundary = box(b[0], b[1], b[2], b[3])\n",
    "        selKeys = []\n",
    "        selPts = []\n",
    "        for index, row in missing_df.iterrows():\n",
    "            if Point(row[x], row[y]).intersects(datasetBoundary):\n",
    "                selPts.append((row[x],row[y]))\n",
    "                selKeys.append(index)\n",
    "        raster_values = list(dataset.sample(selPts))\n",
    "        raster_values = [x[0] for x in raster_values]\n",
    "        z.update(zip(selKeys, raster_values))\n",
    "\n",
    "        elev_df = pd.DataFrame.from_dict(z, orient='index')\n",
    "        elev_df.columns = ['elevation']\n",
    "    df['point_elev'] = elev_df['elevation']\n",
    "    df = df.drop('code', axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to convert distances to walk times\n",
    "\n",
    "Once again, the Tobler's hiking function reappears from Step 3.a to help us generate elevation-adjusted walk times for our network. This verion is modified to return a walk time for the distances between:\n",
    "- an origin / destination coordinate pair (the raw lat/long in the WHO's HeRAMS file or an origin centroid in WorldPop), and\n",
    "- the nearest node on the network. \n",
    "This explains why the 'dist' argument is set by default to 'NN_dist' - or the distance to the nearest node for each point in the frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_walktimes(df, start = 'point_elev', end = 'node_elev', dist = 'NN_dist', max_walkspeed = 6, min_speed = 0.1):\n",
    "    \n",
    "    def speed(incline_ratio, max_speed):\n",
    "        walkspeed = max_speed * np.exp(-3.5 * abs(incline_ratio + 0.05)) \n",
    "        return walkspeed\n",
    "\n",
    "    speeds = {}\n",
    "    times = {}\n",
    "\n",
    "    for index, data in df.iterrows():\n",
    "        if data[dist] > 0:\n",
    "            delta_elevation = data[end] - data[start]\n",
    "            incline_ratio = delta_elevation / data[dist]\n",
    "            speed_kmph = speed(incline_ratio = incline_ratio, max_speed = max_walkspeed)\n",
    "            speed_kmph = max(speed_kmph, min_speed)\n",
    "            speeds[index] = (speed_kmph)\n",
    "            times[index] = (data[dist] / 1000 * 3600 / speed_kmph)\n",
    "\n",
    "    speed_df = pd.DataFrame.from_dict(speeds, orient = 'index')\n",
    "    time_df = pd.DataFrame.from_dict(times, orient = 'index')\n",
    "\n",
    "    df['walkspeed'] = speed_df[0]\n",
    "    df['walk_time'] = time_df[0]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add elevation for destination nodes\n",
    "\n",
    "What it says on the tin - adding an elevation column for the dest_df, the destinations GeoDataFrame. Note we add the elevation for the _points themselves_, NOT their nearest nodes - which we will do soon.\n",
    "\n",
    "We sneakily also set the index to 'Nearest Node' (which will always here be referred to as 'NN')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\charl\\Anaconda3\\envs\\Cfox2\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\charl\\Anaconda3\\envs\\Cfox2\\lib\\site-packages\\ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\charl\\Anaconda3\\envs\\Cfox2\\lib\\site-packages\\ipykernel_launcher.py:71: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "dest_df = acceptable_df[['NN','NN_dist','Latitude','Longitude']]\n",
    "dest_df = add_elevation(dest_df, 'Longitude','Latitude', srtm_pth).set_index('NN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add elevation from graph nodes\n",
    "\n",
    "It will come in useful to also have a dataframe of our nodes with their elevation matched on. We generate this here using tried and tested methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G = nx.read_gpickle(os.path.join(OD_pth, net_name))\n",
    "G_node_df = gn.node_gdf_from_graph(G)\n",
    "G_node_df = add_elevation(G_node_df, 'x', 'y', srtm_pth)\n",
    "match_node_elevs = G_node_df[['node_ID','point_elev']].set_index('node_ID')\n",
    "match_node_elevs.loc[match_node_elevs.point_elev < 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match on node elevations for dest_df nearest nodes; calculate travel times to nearest node\n",
    "\n",
    "Here, having earlier set the dest_df index to nearest node, and having generate above a reference frame for all node elevations, we match on the elevation of each destination's _nearest node_. This will allow us to calculate walk times to the network with accuracy for each destination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# match on NN elevation (not actual destination elevation!)\n",
    "dest_df['node_elev'] = match_node_elevs['point_elev']\n",
    "\n",
    "# with all our constituent fields generated, now we can generate our network-to-actual-destination walktimes.\n",
    "dest_df = generate_walktimes(dest_df, start = 'node_elev', end = 'point_elev', dist = 'NN_dist', max_walkspeed = offroad_speed)\n",
    "\n",
    "# we sort, for fun, mainly. \n",
    "dest_df = dest_df.sort_values(by = 'walk_time', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Walk Time to all travel times in OD matrix\n",
    "\n",
    "We have now generated one component of three of a given travel time between an origin and a destination. \n",
    "\n",
    "Due to the organization of the OD matrix (we only have the unique snapped-to origins nodes, NOT all 560,000 origin nodes in there...) it becomes easier to adjust every journey time to add on required walk time at the end from network to destination. \n",
    "\n",
    "We do that here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# subset the destination DataFrame into one that is just equal to the walk-time from network. index = NN. \n",
    "dest_df = dest_df[['walk_time']]\n",
    "\n",
    "# convert to type that will work with the OD matrix\n",
    "dest_df.index = dest_df.index.map(str)\n",
    "\n",
    "# flip the OD-matrix! Now, origins are the columns, not the rows. \n",
    "d_f = OD.transpose()\n",
    "\n",
    "# for each origin node column, \n",
    "for i in d_f.columns:\n",
    "    \n",
    "    # match on this column to the destination DataFrame (bear with me, I know this is funky)\n",
    "    dest_df[i] = d_f[i]\n",
    "\n",
    "# now, we add the walk time to each and every value, laterally:\n",
    "for i in dest_df.columns:\n",
    "    if i == 'walk_time':\n",
    "        pass\n",
    "    # executed here\n",
    "    else:\n",
    "        dest_df[i] = dest_df[i] + dest_df['walk_time']\n",
    "\n",
    "# before dropping the walk_time - it has been added everywhere, afterall. \n",
    "dest_df = dest_df.drop('walk_time', axis = 1)\n",
    "\n",
    "# then we flip our OD-matrix back the right way up, with rows once again equal to origin nodes, and columns = destination nodes. \n",
    "dest_df = dest_df.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the hardest bit to follow in this script. It has to be done this way with our limited computing resources (and is actually fairly efficient). Nonetheless, it isn't pretty. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Shapefile Describing Regions of Control\n",
    "\n",
    "In this block, we import one of two shapefiles. If conflict is OFF, we have the NoConflict.shp - which is a single shape, with no warfronts in it. Otherwise, w pick up the merged_dists.shp, generated in the Prep A workbook. \n",
    "\n",
    "If you intend to do a scenario involving conflict, go and run that first and come back when merged_dists.shp exists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# choose relevant file\n",
    "if conflict == 1:\n",
    "    conflict_file = r'merged_dists.shp'\n",
    "elif conflict == 0:\n",
    "    conflict_file = r'NoConflict.shp'\n",
    "    \n",
    "# read in relevant file\n",
    "merged_dists = gpd.read_file(os.path.join(util_path, conflict_file))\n",
    "\n",
    "# project if necessary\n",
    "if merged_dists.crs != {'init':'epsg:4326'}:\n",
    "    merged_dists = merged_dists.to_crs({'init':'epsg:4326'})\n",
    "    \n",
    "# only keep Polygons (nothing else should be in there anyway)\n",
    "merged_dists = merged_dists.loc[merged_dists.geometry.type == 'Polygon']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factor in lines of Control - Import Areas of Control Shapefile\n",
    "\n",
    "This function helps us intersect GeoDataFrames and the warfronts file. \n",
    "\n",
    "It returns a dictionary where:\n",
    "- the index is the polygon index in a GeoDataFrame containing polygons, \n",
    "- the value is a list of objects which fall within that polygon\n",
    "\n",
    "It does this very quickly by leveraging spatial indices to speed up the process. Further, we cut up polygons which are too big into smaller blocks and iterate through those to further turbocharge the intersection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Intersect points with merged districts shapefile, identify relationship\n",
    "def AggressiveSpatialIntersect(points, polygons):\n",
    "    # points must be a GeoDataFrame containing points\n",
    "    # polygons must be a GeoDataFrame containing polygons\n",
    "    \n",
    "    import osmnx as ox\n",
    "    \n",
    "    # make a spatial index of the points to be intersected\n",
    "    spatial_index = points.sindex\n",
    "    \n",
    "    # set up the dictionary to be returned\n",
    "    container = {}\n",
    "    cut_geoms = []\n",
    "    \n",
    "    # iterate through each polygon\n",
    "    for index, row in polygons.iterrows():\n",
    "        \n",
    "        # pick out the shapely object\n",
    "        polygon = row.geometry\n",
    "        \n",
    "        # the polygon is big, \n",
    "        if polygon.area > 0.5:\n",
    "            \n",
    "            # cut the geometry into quadrants of width 0.5 arc-seconds\n",
    "            geometry_cut = ox.quadrat_cut_geometry(polygon, quadrat_width=0.5)\n",
    "            \n",
    "            # add this to a list of cut geometries\n",
    "            cut_geoms.append(geometry_cut)\n",
    "            \n",
    "            # notify that we are taking some scissors to this polygon in particular\n",
    "            print('cutting geometry %s into %s pieces' % (index, len(geometry_cut)))\n",
    "            index_list = []\n",
    "            \n",
    "            # now go through these geometry pieces, and perform the spatial intersect\n",
    "            for P in geometry_cut:\n",
    "                \n",
    "                possible_matches_index = list(spatial_index.intersection(P.bounds))\n",
    "                possible_matches = points.iloc[possible_matches_index]\n",
    "                precise_matches = possible_matches[possible_matches.intersects(P)]\n",
    "                if len(precise_matches) > 0:\n",
    "                    index_list.append(precise_matches.index)\n",
    "                flat_list = [item for sublist in index_list for item in sublist]\n",
    "                container[index] = list(set(flat_list))\n",
    "                \n",
    "        # if it is a small polygon, just go for the intersection anyway (via spatial index)\n",
    "        else:\n",
    "            possible_matches_index = list(spatial_index.intersection(polygon.bounds))\n",
    "            possible_matches = points.iloc[possible_matches_index]\n",
    "            precise_matches = possible_matches[possible_matches.intersects(polygon)]\n",
    "            if len(precise_matches) > 0:\n",
    "                container[index] = list(precise_matches.index)\n",
    "                \n",
    "    # return the dictionary of which points lie in which polygons.\n",
    "    return container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we deploy this function to identify which graph nodes lie in which areas of control. Possible_snap_nodes now is set to the resultant dictionary, of format: {polygon : [contained nodes list]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cutting geometry 56 into 121 pieces\n",
      "cutting geometry 59 into 47 pieces\n",
      "cutting geometry 78 into 236 pieces\n",
      "cutting geometry 79 into 16 pieces\n",
      "**bag of possible node snapping locations has been successfully generated**\n"
     ]
    }
   ],
   "source": [
    "graph_node_gdf = gn.node_gdf_from_graph(G)\n",
    "gdf = graph_node_gdf.copy()\n",
    "gdf = gdf.set_index('node_ID')\n",
    "possible_snap_nodes = AggressiveSpatialIntersect(graph_node_gdf, merged_dists)\n",
    "print('**bag of possible node snapping locations has been successfully generated**')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Origins Grid\n",
    "\n",
    "This is the first time we pick up and work with the large origin GeoDataFrame. We load the correct one in for the year we are working with, and perform some housekeeping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Match on network time from origin node (time travelling along network + walking to destination)\n",
    "if year == 2018:\n",
    "    year_raster = 2018\n",
    "elif year == 2016:\n",
    "    year_raster = 2015\n",
    "grid_name = r'origins_1km_%s_snapped.csv' % year_raster\n",
    "grid = pd.read_csv(os.path.join(OD_pth, grid_name))\n",
    "grid = grid.rename({'Unnamed: 0':'PointID'}, axis = 1)\n",
    "grid['geometry'] = grid['geometry'].apply(loads)\n",
    "grid_gdf = gpd.GeoDataFrame(grid, crs = WGS, geometry = 'geometry')\n",
    "grid_gdf = grid_gdf.set_index('PointID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust Nearest Node snapping for War\n",
    "\n",
    "This gets kinda interesting. The first thing we do is generate the reference dictionary for the origin layer, of which origin points lie within which polygon of homogenous control (Remember if conflict is  turned off, then all points lie within one single homogenous polygon - Yemen's territorial boundaries (minue Socotra...I digress)).\n",
    "\n",
    "Thereafter, we remake the origin file with the polygon reference for each point in the final line, where we concat 'bundle' - the slices of the original file that lie inside each polygon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cutting geometry 56 into 121 pieces\n",
      "cutting geometry 59 into 47 pieces\n",
      "cutting geometry 78 into 236 pieces\n",
      "cutting geometry 79 into 16 pieces\n",
      "bag of possible origins locations has been successfully generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\charl\\Documents\\GitHub\\GOST_PublicGoods\\GOSTNets\\GOSTNets\\GOSTnet.py:1679: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  G_tree = spatial.KDTree(target_gdf[['x','y']].as_matrix())\n",
      "C:\\Users\\charl\\Documents\\GitHub\\GOST_PublicGoods\\GOSTNets\\GOSTNets\\GOSTnet.py:1681: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  distances, indices = G_tree.query(source_gdf[['x','y']].as_matrix())\n"
     ]
    }
   ],
   "source": [
    "origin_container = AggressiveSpatialIntersect(grid_gdf, merged_dists)\n",
    "print('bag of possible origins locations has been successfully generated')\n",
    "\n",
    "bundle = []\n",
    "for key in origin_container.keys():\n",
    "    origins = origin_container[key]\n",
    "    possible_nodes = graph_node_gdf.loc[possible_snap_nodes[key]]\n",
    "    origin_subset = grid_gdf.loc[origins]\n",
    "    origin_subset_snapped = gn.pandana_snap_points(origin_subset, \n",
    "                                possible_nodes, \n",
    "                                source_crs = 'epsg:4326', \n",
    "                                target_crs = 'epsg:32638', \n",
    "                                add_dist_to_node_col = True)\n",
    "    bundle.append(origin_subset_snapped)\n",
    "\n",
    "grid_gdf_adjusted = pd.concat(bundle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the grid_gdf back to this conflict adjusted file - so we carry forward the changes we just made. \n",
    "\n",
    "(for those working through the script, might be worth comparing grid_gdf_adjusted and grid_gdf to see the change this block makes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_gdf = grid_gdf_adjusted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust acceptable destinations for each node for the war\n",
    "\n",
    "We repeat the process in the last cell, but this time for graph nodes snapped-to as desintations, and those snapped-to as origins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cutting geometry 56 into 121 pieces\n",
      "cutting geometry 59 into 47 pieces\n",
      "cutting geometry 78 into 236 pieces\n",
      "cutting geometry 79 into 16 pieces\n",
      "cutting geometry 56 into 121 pieces\n",
      "cutting geometry 59 into 47 pieces\n",
      "cutting geometry 78 into 236 pieces\n",
      "cutting geometry 79 into 16 pieces\n"
     ]
    }
   ],
   "source": [
    "#### for Destination nodes (dest_Df.columns)\n",
    "# take the nodes GDF\n",
    "gdf = graph_node_gdf.copy()\n",
    "\n",
    "# housekeep for ID datatype\n",
    "gdf['node_ID'] = gdf['node_ID'].astype('str')\n",
    "\n",
    "# take only the nodes that also appear in the dest_df columns (i.e. the OD matrix). Cols = Dests\n",
    "gdf = gdf.loc[gdf.node_ID.isin(list(dest_df.columns))]\n",
    "\n",
    "# set the index as the node ID\n",
    "gdf = gdf.set_index('node_ID')\n",
    "\n",
    "# generate our reference dictionary for snapped-to destination nodes\n",
    "dest_container = AggressiveSpatialIntersect(gdf, merged_dists)\n",
    "\n",
    "# repeat process for Origin nodes (dest_Df.index)\n",
    "gdf = graph_node_gdf.copy()\n",
    "\n",
    "# remember, the index of dest_df (the OD matrix) is the nodes snapped-to by origin points\n",
    "gdf = gdf.loc[gdf.node_ID.isin(list(dest_df.index))]\n",
    "gdf = gdf.set_index('node_ID')\n",
    "\n",
    "# generate our reference dictionary\n",
    "origin_snap_container = AggressiveSpatialIntersect(gdf, merged_dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working out the Min-time for each polygon\n",
    "\n",
    "This is a fairly crucial block. Here, we iterate through the polygons, take only the valid origins and destinations in that polygon, and work out the minimum time, for each selected origin, to a valid destination. This is how we prevent people crossing borders - we effectively run N small-universe analyses, where the rest of the country outside the polygon doesn't exist! Remember this is on-network travel only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bundle = []\n",
    "\n",
    "# for each polygon\n",
    "for key in origin_snap_container.keys():\n",
    "    \n",
    "    # select the origins which exist in this polygon\n",
    "    origins = origin_snap_container[key]\n",
    "    \n",
    "    # select the destinations which exist in this polygon\n",
    "    destinations = dest_container[key]\n",
    "    \n",
    "    # subset the OD-matrix to include just these origins and destinations\n",
    "    Q = dest_df[destinations].loc[origins]\n",
    "    \n",
    "    # make a new column, min-time - for each origin, its closest destination\n",
    "    Q['min_time'] = Q.min(axis = 1)\n",
    "    \n",
    "    # subset to just this column\n",
    "    Q2 = Q[['min_time']]\n",
    "    \n",
    "    # append to bundle\n",
    "    bundle.append(Q2)\n",
    "\n",
    "# concatenate results    \n",
    "Q3 = pd.concat(bundle)\n",
    "\n",
    "# add to the OD matrix\n",
    "dest_df['min_time'] = Q3['min_time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return to Normal Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_gdf = grid_gdf.rename(columns = {'NN':'O_ID'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge on min Time\n",
    "\n",
    "Here, we add the min-time on the network for each origin node to each origin point. This is a large 1-to-many join for each snapped-to origin node (c. 36k unique origin nodes on the network being joined over to c. 560k origin points, many of which have duplicate closest nodes, as you would expect). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_gdf = grid_gdf.reset_index()\n",
    "grid_gdf = grid_gdf.set_index(grid_gdf['O_ID'])\n",
    "grid_gdf['on_network_time'] = dest_df['min_time']\n",
    "grid_gdf = grid_gdf.set_index('PointID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add origin node distance to network - walking time\n",
    "\n",
    "We now have 2/3 of the puzzle done for each journey - the walk from the network to the destination (baked in to all OD matrix values now) and the on-network minimum time to a valid destination (in the same polygon as the origin point!). Now, we go ahead and add the walk time from the specific origin point to the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid = grid_gdf\n",
    "grid = add_elevation(grid, 'Longitude','Latitude', srtm_pth)\n",
    "grid = grid.reset_index()\n",
    "grid = grid.set_index('O_ID')\n",
    "grid['node_elev'] = match_node_elevs['point_elev']\n",
    "grid = grid.set_index('PointID')\n",
    "grid = generate_walktimes(grid, start = 'point_elev', end = 'node_elev', dist = 'NN_dist', max_walkspeed = offroad_speed)\n",
    "grid = grid.rename({'node_elev':'nr_node_on_net_elev', \n",
    "                    'walkspeed':'walkspeed_to_net', \n",
    "                    'walk_time':'walk_time_to_net',\n",
    "                   'NN_dist':'NN_dist_to_net'}, axis = 1)\n",
    "\n",
    "# thus, the process is complete - the total time is the network time, plus the walk time to the network. \n",
    "grid['total_time_net'] = grid['on_network_time'] + grid['walk_time_to_net']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Direct Walking Time (not using road network), vs. network Time\n",
    "\n",
    "In some cases, it won't be logical to use the network at all - walking to the network for a long time, only to drive for 2 mins and then walk back off the network again, will not be logical. To circumvent this, we also calculate what happens if you walk in a straight line towards your closest destination - ignoring roads. If this time is less than the network time, we use this instead. \n",
    "\n",
    "To do this, we take our dataframe of acceptable destinations, and snap the origin points _Directly_ to the destinations - then work out the corresponding elevations, and thus travel times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cutting geometry 56 into 121 pieces\n",
      "cutting geometry 59 into 47 pieces\n",
      "cutting geometry 78 into 236 pieces\n",
      "cutting geometry 79 into 16 pieces\n"
     ]
    }
   ],
   "source": [
    "bundle = []\n",
    "W = graph_node_gdf.copy()\n",
    "W['node_ID'] = W['node_ID'].astype(str)\n",
    "W = W.set_index('node_ID')\n",
    "\n",
    "# Generate dictionary of locations by homogenously controlled polygon\n",
    "locations_gdf = gpd.GeoDataFrame(acceptable_df, geometry = 'geometry', crs = {'init':'epsg:4326'})\n",
    "locations_container = AggressiveSpatialIntersect(locations_gdf, merged_dists)\n",
    "\n",
    "# for each polygon, snap the origins in that polygon to the acceptable destinations\n",
    "for key in origin_container.keys():\n",
    "    \n",
    "    # select subset of all origin points - only the ones in this polygon\n",
    "    origins = origin_container[key]\n",
    "    origin_subset = grid.copy()\n",
    "    origin_subset = origin_subset.loc[origins]\n",
    "    \n",
    "    # select subset of all locations - only the ones in this polygon\n",
    "    locations = locations_gdf.loc[locations_container[key]]\n",
    "    \n",
    "    # control for no points of service in this polygon. Early end! No pandana snap. \n",
    "    if len(locations) < 1:\n",
    "        origin_subset['NN'] = None\n",
    "        origin_subset['NN_dist'] = None\n",
    "        bundle.append(origin_subset)\n",
    "    else:\n",
    "        # Here, we find the closest hospital to each origin point as the crow-flies by using pandana snap\n",
    "        origin_subset_snapped = gn.pandana_snap_points(origin_subset, \n",
    "                                locations, \n",
    "                                source_crs = 'epsg:4326', \n",
    "                                target_crs = 'epsg:32638', \n",
    "                                add_dist_to_node_col = True)\n",
    "        bundle.append(origin_subset_snapped)\n",
    "\n",
    "grid_gdf_adjusted = pd.concat(bundle)\n",
    "\n",
    "grid = grid_gdf_adjusted\n",
    "\n",
    "# Y is now a copy of the grid to save space\n",
    "Y = grid.copy()\n",
    "\n",
    "objs = []\n",
    "if len(Y.loc[Y['NN'].isnull() == True]) > 0:\n",
    "    Y2 = Y.loc[Y['NN'].isnull() == True]\n",
    "    Y2['walkspeed_direct'] = 0\n",
    "    Y2['walk_time_direct'] = 9999999\n",
    "    Y2['NN_dist_direct'] = 9999999\n",
    "    objs.append(Y2)\n",
    "\n",
    "# add location elevations\n",
    "location_elevs = add_elevation(locations_gdf, 'Longitude','Latitude', srtm_pth)\n",
    "\n",
    "# match on to the grid the elevation of the nearest location to that point\n",
    "Y1 = Y.loc[Y['NN'].isnull() == False]\n",
    "Y1['NN'] = Y1['NN'].astype(int)\n",
    "Y1 = Y1.set_index('NN')\n",
    "Y1['dest_NN_elev'] = location_elevs['point_elev']\n",
    "\n",
    "Y1 = Y1.reset_index()\n",
    "\n",
    "# generate walktimes for each grid point directly to the nearest point of service\n",
    "Y1 = generate_walktimes(Y1, start = 'point_elev', end = 'dest_NN_elev', dist = 'NN_dist', max_walkspeed = offroad_speed).reset_index()\n",
    "\n",
    "# housekeeping - rename columns for easier identification\n",
    "Y1 = Y1.rename({'walkspeed':'walkspeed_direct', \n",
    "                    'walk_time':'walk_time_direct',\n",
    "                   'NN_dist':'NN_dist_direct'}, axis = 1)\n",
    "objs.append(Y1)\n",
    "\n",
    "grid = pd.concat(objs)\n",
    "\n",
    "# take the MINIMUM travel time of a.) directly walking there, and b.) using the network\n",
    "grid['PLOT_TIME_SECS'] = grid[['walk_time_direct','total_time_net']].min(axis = 1)\n",
    "\n",
    "# convert to minutes\n",
    "grid['PLOT_TIME_MINS'] = grid['PLOT_TIME_SECS'] / 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Output Raster\n",
    "\n",
    "At this point, all of the maths is done. We want to load our results onto a raster for visualization. We start by copying the input raster - the Worldpop grid - which made all of this possible. Then, we adjust the metadata to allow for another band of data to be added. This band will represent the travel time to the closest facility for that grid cell. Hence, the resultant raster will be dual-band - the first layer, the population, and the second layer, the accessibility of the people living in that gridcell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**process complete**\n"
     ]
    }
   ],
   "source": [
    "# This is the raster we start from\n",
    "rst_fn = os.path.join(pth,'pop18_resampled.tif')\n",
    "\n",
    "# set output filename + filepath\n",
    "out_fn = os.path.join(basepth,'output_layers','Round 3','%s.tif' % subset)\n",
    "\n",
    "# Copy the metadata of the WorldPop raster\n",
    "rst = rt.open(rst_fn, 'r')\n",
    "meta = rst.meta.copy()\n",
    "D_type = rt.float64\n",
    "\n",
    "# By upping count to 2, we allow for a second data band\n",
    "meta.update(compress='lzw', dtype = D_type, count = 2)\n",
    "\n",
    "# open both the template file and the new file\n",
    "with rt.open(out_fn, 'w', **meta) as out:\n",
    "    with rt.open(rst_fn, 'r') as pop:\n",
    "        \n",
    "        # this is where we create a generator of geom, value pairs to use in rasterizing\n",
    "        shapes = ((geom,value) for geom, value in zip(grid.geometry, grid.PLOT_TIME_MINS))\n",
    "        \n",
    "        # we copy our population layer\n",
    "        population = pop.read(1).astype(D_type)\n",
    "        cpy = population.copy()\n",
    "        \n",
    "        # to generate the travel times, we rasterize the geometry: value pairs in the Grid GeoDataFrame\n",
    "        travel_times = features.rasterize(shapes=shapes, fill=0, out=cpy, transform=out.transform)\n",
    "\n",
    "        # We then write these bands out, one after another\n",
    "        out.write_band(1, population)\n",
    "        out.write_band(2, travel_times)\n",
    "        \n",
    "# That's it - we are done for this scenario\n",
    "print('**process complete**')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Zonal Stats\n",
    "\n",
    "This function is taken from Ben Stewart's GOSTRocks library. The only difference is the setting of 'all touched' to False to prevent over-counting of the population. \n",
    "\n",
    "We will use this function to summarize the number of people, in any given polygon, that have access / lack access to a functioning, valid facility within X minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def zonalStats(inShp, inRaster, bandNum=1, mask_A = None, reProj = False, minVal = '', maxVal = '', verbose=False , rastType='N', unqVals=[]):\n",
    "    import sys, os, inspect, logging, json\n",
    "    import rasterio, affine\n",
    "\n",
    "    import pandas as pd\n",
    "    import geopandas as gpd\n",
    "    import numpy as np\n",
    "\n",
    "    from collections import Counter\n",
    "    from shapely.geometry import box\n",
    "    from affine import Affine\n",
    "    from rasterio import features\n",
    "    from rasterio.mask import mask\n",
    "    from rasterio.features import rasterize\n",
    "    from rasterio.warp import reproject, Resampling\n",
    "    from osgeo import gdal\n",
    "    \n",
    "    ''' Run zonal statistics against an input shapefile\n",
    "    \n",
    "    INPUT VARIABLES\n",
    "    inShp [string or geopandas object] - path to input shapefile\n",
    "    inRaster [string or rasterio object] - path to input raster\n",
    "    \n",
    "    OPTIONAL\n",
    "    bandNum [integer] - band in raster to analyze\n",
    "    reProj [boolean] -  whether to reproject data to match, if not, raise an error\n",
    "    minVal [number] - if defined, will only calculation statistics on values above this number\n",
    "    verbose [boolean] - whether to be loud with responses\n",
    "    rastType [string N or C] - N is numeric and C is categorical. Categorical returns counts of numbers\n",
    "    unqVals [array of numbers] - used in categorical zonal statistics, tabulates all these numbers, will report 0 counts\n",
    "    mask_A [numpy boolean mask] - mask the desired band using an identical shape boolean mask. Useful for doing conditional zonal stats\n",
    "    \n",
    "    RETURNS\n",
    "    array of arrays, one for each feature in inShp\n",
    "    '''   \n",
    "    if isinstance(inShp, str):\n",
    "        inVector = gpd.read_file(inShp) \n",
    "    else:\n",
    "        inVector = inShp\n",
    "    if isinstance(inRaster, str):\n",
    "        curRaster = rasterio.open(inRaster, 'r+')\n",
    "    else:\n",
    "        curRaster = inRaster\n",
    "        \n",
    "    # If mask is not none, apply mask \n",
    "    if mask_A is not None:\n",
    "        \n",
    "        curRaster.write_mask(np.invert(mask_A))\n",
    "    \n",
    "    outputData=[]\n",
    "    if inVector.crs != curRaster.crs:\n",
    "        if reProj:\n",
    "            inVector = inVector.to_crs(curRaster.crs)\n",
    "        else:\n",
    "            raise ValueError(\"Input CRS do not match\")\n",
    "    fCount = 0\n",
    "    tCount = len(inVector['geometry'])\n",
    "    #generate bounding box geometry for raster bbox\n",
    "    b = curRaster.bounds\n",
    "    rBox = box(b[0], b[1], b[2], b[3])\n",
    "    for geometry in inVector['geometry']:\n",
    "        #This test is used in case the geometry extends beyond the edge of the raster\n",
    "        #   I think it is computationally heavy, but I don't know of an easier way to do it\n",
    "        if not rBox.contains(geometry):\n",
    "            geometry = geometry.intersection(rBox)            \n",
    "        try:\n",
    "            fCount = fCount + 1\n",
    "            if fCount % 1000 == 0 and verbose:\n",
    "                tPrint(\"Processing %s of %s\" % (fCount, tCount) )\n",
    "            # get pixel coordinates of the geometry's bounding box\n",
    "            ul = curRaster.index(*geometry.bounds[0:2])\n",
    "            lr = curRaster.index(*geometry.bounds[2:4])\n",
    "            '''\n",
    "            TODO: There is a problem with the indexing - if the shape falls outside the boundaries, it errors\n",
    "                I want to change it to just grab what it can find, but my brain is wrecked and I cannot figure it out\n",
    "            print(geometry.bounds)\n",
    "            print(curRaster.shape)\n",
    "            print(lr)\n",
    "            print(ul)\n",
    "            lr = (max(lr[0], 0), min(lr[1], curRaster.shape[1]))\n",
    "            ul = (min(ul[0], curRaster.shape[0]), min(ul[1]))\n",
    "            '''\n",
    "            # read the subset of the data into a numpy array\n",
    "            window = ((float(lr[0]), float(ul[0]+1)), (float(ul[1]), float(lr[1]+1)))\n",
    "            \n",
    "            if mask is not None:\n",
    "                data = curRaster.read(bandNum, window=window, masked = True)\n",
    "            else:\n",
    "                data = curRaster.read(bandNum, window=window, masked = False)\n",
    "            \n",
    "            # create an affine transform for the subset data\n",
    "            t = curRaster.transform\n",
    "            shifted_affine = Affine(t.a, t.b, t.c+ul[1]*t.a, t.d, t.e, t.f+lr[0]*t.e)\n",
    "\n",
    "            # rasterize the geometry\n",
    "            mask = rasterize(\n",
    "                [(geometry, 0)],\n",
    "                out_shape=data.shape,\n",
    "                transform=shifted_affine,\n",
    "                fill=1,\n",
    "                all_touched=False,\n",
    "                dtype=np.uint8)\n",
    "\n",
    "            # create a masked numpy array\n",
    "            masked_data = np.ma.array(data=data, mask=mask.astype(bool))\n",
    "            if rastType == 'N':                \n",
    "                if minVal != '' or maxVal != '':\n",
    "                    if minVal != '':\n",
    "                        masked_data = np.ma.masked_where(masked_data < minVal, masked_data)\n",
    "                    if maxVal != '':\n",
    "                        masked_data = np.ma.masked_where(masked_data > maxVal, masked_data)                    \n",
    "                    if masked_data.count() > 0:                        \n",
    "                        results = [masked_data.sum(), masked_data.min(), masked_data.max(), masked_data.mean()]\n",
    "                    else :\n",
    "                        results = [-1, -1, -1, -1]                \n",
    "                else:\n",
    "                    results = [masked_data.sum(), masked_data.min(), masked_data.max(), masked_data.mean()]\n",
    "            if rastType == 'C':\n",
    "                if len(unqVals) > 0:                          \n",
    "                    xx = dict(Counter(data.flatten()))\n",
    "                    results = [xx.get(i, 0) for i in unqVals]                \n",
    "                else:\n",
    "                    results = np.unique(masked_data, return_counts=True)                    \n",
    "            outputData.append(results)\n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "            outputData.append([-1, -1, -1, -1])            \n",
    "    return outputData   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Analyses / Visualization Aids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main process is now finished. However, in order to generate the many output images and special analyses for the Yemen project, standalone sub-processes have been devised to generate outputs like: \n",
    "- District and national level zonal statistics\n",
    "- 2016 v 2018 change in access detection\n",
    "- Mash-ups with the vulnerability matrix data\n",
    "We detail four of these examples below. Other can be found at the foot of the automated version of the file (these were used later). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Zonal Stats\n",
    "\n",
    "Here, we make use of our zonal statistics function to generate summary values for an administrative boundary layer. \n",
    "\n",
    "The script is currently set up to accept either a national (adm-0) level shapefile, or a district (adm-2) level shapefile - though, with some cursory and superficial modification, it could be used to generate zonal stats for any given polygon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\charl\\Anaconda3\\envs\\Cfox2\\lib\\site-packages\\pandas\\core\\indexing.py:190: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving national\n",
      "saving district\n"
     ]
    }
   ],
   "source": [
    "# sometimes, we may want to run the script from top to bottom without doing this bit. Thus, we have a control variable, \n",
    "# zonal_stats, which if set to 0 will disable this chunk. Ensure zonal_stats != 0 to continue. \n",
    "if zonal_stats == 0:\n",
    "    pass\n",
    "\n",
    "else:\n",
    "    # These are the two shapefiles we have routinely used. We generate outputs for both. \n",
    "    for resolution in ['national','district']:\n",
    "        \n",
    "        # We pick up our raster from the write location (Don't change the output names!)\n",
    "        out_fn = os.path.join(r'C:\\Users\\charl\\Documents\\GOST\\Yemen\\output_layers','Round 3',\n",
    "                              '%s.tif' % subset)\n",
    "        \n",
    "        # Set the util path - where we can load the admin boundary polygon from\n",
    "        utils = r'C:\\Users\\charl\\Documents\\GOST\\Yemen\\util_files'\n",
    "\n",
    "        # Here we load the national-level shapefile - the Yemen bound\n",
    "        yemen_shp_name = os.path.join(utils, r'Yemen_bound.shp')\n",
    "        yemen_shp = gpd.read_file(yemen_shp_name)\n",
    "        \n",
    "        # Reproject to WGS84 if necessary\n",
    "        if yemen_shp.crs != {'init': 'epsg:4326'}:\n",
    "            yemen_shp = yemen_shp.to_crs({'init': 'epsg:4326'})\n",
    "        \n",
    "        # Pick up the district shapefile, and reproject if necessary\n",
    "        district_shp_name = os.path.join(r'C:\\Users\\charl\\Documents\\GOST\\Yemen\\VulnerabilityMatrix', r'VM.shp')\n",
    "        district_shp = gpd.read_file(district_shp_name)\n",
    "        \n",
    "        if district_shp.crs != {'init': 'epsg:4326'}:\n",
    "            district_shp = district_shp.to_crs({'init': 'epsg:4326'})\n",
    "        \n",
    "        # Here we read in the output raster we just created\n",
    "        inraster = out_fn\n",
    "        ras = rt.open(inraster, mode = 'r+')\n",
    "        \n",
    "        # pop is the population band\n",
    "        pop = ras.read(1)\n",
    "        \n",
    "        # tt_matrix is the travel time band\n",
    "        tt_matrix = ras.read(2)\n",
    "        \n",
    "        # set the target shape\n",
    "        if resolution == 'national':\n",
    "            target_shp = yemen_shp\n",
    "        elif resolution == 'district':\n",
    "            target_shp = district_shp\n",
    "\n",
    "        # Add on the total population of the district to each district shape\n",
    "        mask_pop = np.ma.masked_where(pop > (200000), pop).mask\n",
    "\n",
    "        base_pop = zonalStats(target_shp,     # target shp\n",
    "                                inraster,     # our output raster\n",
    "                                bandNum = 1,  # pick band 1 - the population band\n",
    "                                mask_A = mask_pop, # use mask_pop, which removes values greater than 200,000 (i.e. errors)\n",
    "                                reProj = False, # do not reproject\n",
    "                                minVal = 0,\n",
    "                                maxVal = np.inf, \n",
    "                                verbose = True, \n",
    "                                rastType='N')  # as opposed to categorical\n",
    "        \n",
    "        # the zonalStats function returns 4 outputs - sum, min, max and mean. We only want sum\n",
    "        cols = ['total_pop','min','max','mean']\n",
    "        \n",
    "        temp_df = pd.DataFrame(base_pop, columns = cols)\n",
    "        \n",
    "        # we match the sum back on to our original target_shp file\n",
    "        target_shp['total_pop'] = temp_df['total_pop']\n",
    "        target_shp['total_pop'].loc[target_shp['total_pop'] == -1] = 0\n",
    "\n",
    "        # Having added the base population, we now calculate the population within a range \n",
    "        # of time thresholds from the destination set. \n",
    "        # we do this for four time thresholds. Note the change in the mask argument:\n",
    "        for time_thresh in [30,60,120, 240]:\n",
    "            \n",
    "            # this is our new, special, mask - we mask all values ABOVE the threshold\n",
    "            mask_obj = np.ma.masked_where(tt_matrix > (time_thresh), tt_matrix).mask\n",
    "            \n",
    "            # we pass this to our same zonal stats function. We are only counting population values below the threshold now\n",
    "            raw = zonalStats(target_shp, \n",
    "                                inraster, \n",
    "                                bandNum = 1,\n",
    "                                mask_A = mask_obj,\n",
    "                                reProj = False, \n",
    "                                minVal = 0,\n",
    "                                maxVal = np.inf, \n",
    "                                verbose = True, \n",
    "                                rastType='N')\n",
    "\n",
    "            # create  temp_df of the results\n",
    "            cols = ['pop_%s' % time_thresh,'min','max','mean']\n",
    "            temp_df = pd.DataFrame(raw, columns = cols)\n",
    "            \n",
    "            # Add in this new populaiton count to the file\n",
    "            target_shp['pop_%s' % time_thresh] = temp_df['pop_%s' % time_thresh]\n",
    "            target_shp['pop_%s' % time_thresh].loc[target_shp['pop_%s' % time_thresh] == -1] = 0\n",
    "            \n",
    "            # note the percentage of the total population that has access within this thresh\n",
    "            target_shp['frac_%s' % time_thresh] = (target_shp['pop_%s' % time_thresh]) / (target_shp['total_pop']).fillna(0)\n",
    "            target_shp['frac_%s' % time_thresh].replace([np.inf, -np.inf], 0)\n",
    "            target_shp['frac_%s' % time_thresh] = target_shp['frac_%s' % time_thresh].fillna(0)\n",
    "\n",
    "        # Save to file. The only difference is that we save a summary .csv instead of a .shp for national - as we never want to \n",
    "        # visualize the national-level results (it would be one single block of color)\n",
    "\n",
    "        if resolution == 'national':\n",
    "            print('saving national')\n",
    "            outter = target_shp[['total_pop','pop_30','frac_30','pop_60','frac_60','pop_120','frac_120','pop_240','frac_240']]\n",
    "            outter.to_csv(os.path.join(basepth, 'output_layers','Round 3','%s_zonal_%s.csv'% (subset, resolution)))\n",
    "        \n",
    "        else:\n",
    "            print('saving district')\n",
    "            target_shp['abs_pop_iso'] = target_shp['total_pop'] - target_shp['pop_30']\n",
    "            target_shp.to_file(os.path.join(basepth, 'output_layers','Round 3','%s_zonal_%s.shp' % (subset, resolution)), driver = 'ESRI Shapefile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Change Raster\n",
    "\n",
    "This analysis assumes that the user has already run a scenario for 2016, and a scenario for 2018 - with all other settings held entirely constant. \n",
    "\n",
    "We do some basic raster maths on the travel time band of two output rasters to generate a delta / change raster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set to 'off' to continue\n",
    "test_mode = 'on'\n",
    "\n",
    "if test_mode == 'on':\n",
    "    pass\n",
    "\n",
    "else:\n",
    "    # name the output file\n",
    "    subset = r'PHCs_2018_conflict_delta'\n",
    "    \n",
    "    # this is the first raster \n",
    "    pre_raster = os.path.join(basepth, 'output_layers','Round 3','driving_24th_HERAMS_PHC_NoConflict.tif')\n",
    "    \n",
    "    # this is the second raster\n",
    "    post_raster = os.path.join(basepth, 'output_layers','Round 3','driving_24th_HERAMS_PHCs_ConflictAdj.tif')\n",
    "    \n",
    "    # define the output / delta / change raster filename and location\n",
    "    out_fn = os.path.join(basepth,'output_layers','Round 3','%s.tif' % subset)\n",
    "    \n",
    "    # open the first raster, read in band 2\n",
    "    pre = rasterio.open(pre_raster, 'r')\n",
    "    arr_pre = pre.read(2)\n",
    "    \n",
    "    # repeat for second raster\n",
    "    post = rasterio.open(post_raster, 'r')\n",
    "    arr_post = post.read(2)\n",
    "    \n",
    "    # subtract post from pre\n",
    "    delta = arr_pre - arr_post\n",
    "\n",
    "    # Update metadata from template raster\n",
    "    rst_fn = os.path.join(pth,'pop18_resampled.tif')\n",
    "    rst = rasterio.open(rst_fn, 'r')\n",
    "    meta = rst.meta.copy()\n",
    "    D_type = rasterio.float64\n",
    "    meta.update(compress='lzw', dtype = D_type, count = 3)\n",
    "    \n",
    "    # write out change raster\n",
    "    with rasterio.open(out_fn, 'w', **meta) as out:\n",
    "        with rasterio.open(rst_fn, 'r') as pop:\n",
    "\n",
    "            # keep the original population later\n",
    "            population = pop.read(1).astype(D_type)\n",
    "            \n",
    "            # the only surprise here is band 3 - the number of people x the minutes of change. \n",
    "            # Useful for zonal stats to identify the areas which changed the most for the better or worse.\n",
    "            out.write_band(1, population)\n",
    "            out.write_band(2, delta)\n",
    "            out.write_band(3, delta * population)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship Graphs: Vulnerability Component Scores, Accessibility\n",
    "\n",
    "Here, we generate some line plots to try to identify any simple / obvious relationships between Vulnerability Matrix factor scores and accessibility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adjust test_mode variable to != on to continue\n",
    "if test_mode == 'on':\n",
    "    pass\n",
    "else:\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Here, we need to import a prepared target_shp file. This has the VM details in it + access stats from the zonal stats step.\n",
    "    # this is important - it must have both VM data AND access data from zonal stats in the same file. \n",
    "    # the path below is the path to the raw VM file - which should then be used in the zonal stats step before running this block.\n",
    "    target_shp = os.path.join(r'C:\\Users\\charl\\Documents\\GOST\\Yemen\\VulnerabilityMatrix', r'VM.shp')\n",
    "    \n",
    "    # rename columns for easier access\n",
    "    target_shp2 = target_shp.rename({\n",
    "        'Overall Vu':'Overall Vulnerability Level',\n",
    "        'Health Sys':'Health System Capacity Score',\n",
    "        'Hazards':'Hazard Score',\n",
    "        'Impact on':'Impact on Exposed Population Score',\n",
    "        'Food Secur':'Food Security Score',\n",
    "        'Morbidity':'Morbidity Score',\n",
    "        'Nutrition':'Nutrition Score',\n",
    "        'WASH':'WASH Score',\n",
    "        'Social Det':'Social Determinants and Health Outcomes Score',\n",
    "        'total_pop':'Total Population',\n",
    "                                   }, axis = 1)\n",
    "    \n",
    "    # pick out the factors / scores we want to generate graphs for\n",
    "    factors = ['Overall Vulnerability Level','Health System Capacity Score',\n",
    "              'Hazard Score','Impact on Exposed Population Score','Food Security Score',\n",
    "              'WASH Score','Social Determinants and Health Outcomes Score']\n",
    "    \n",
    "    # define our time thresholds of interest\n",
    "    fracs = {'frac_30':'30 minutes',\n",
    "            'frac_60': '1 hour',\n",
    "            'frac_120': '2 hours',\n",
    "            'frac_240': '4 hours'}\n",
    "    \n",
    "    # now, for each factor\n",
    "    for groupa in factors:\n",
    "        \n",
    "        # we group by each factor in turn. These factors have values between 1 and 7, so we have c. 8 groups\n",
    "        subg = target_shp2[[groupa,'Total Population','pop_30','pop_60','pop_120','pop_240']].groupby(groupa).sum()\n",
    "        \n",
    "        # work out the fraction which has access in each district\n",
    "        for i in [30, 60, 120, 240]:\n",
    "            subg['frac_%s' % i] = subg['pop_%s' % i] / subg['Total Population']\n",
    "            \n",
    "        #  We plot out the average fraction that has access for each value present in the VM factor\n",
    "        plotter = subg[['frac_30','frac_60','frac_120','frac_240']]\n",
    "        plt.clf()\n",
    "        plt.figure(figsize=(8,4))\n",
    "        \n",
    "        # title says it all\n",
    "        title = 'Fraction of Population with access to a HeRAMS hospital for each value \\nof the WHO %s' % (groupa)\n",
    "        \n",
    "        # set color palette\n",
    "        pal = sns.cubehelix_palette(4, start=2.7, rot=.1, light = .7, dark=.1)\n",
    "        ax = sns.lineplot(data = plotter, \n",
    "                          palette = pal, \n",
    "                          dashes = False).set_title(title)\n",
    "        \n",
    "        # add a legend and save the plot down\n",
    "        plt.legend(loc='center right', bbox_to_anchor=(1.2, 0.5), ncol=1)\n",
    "        plt.savefig(os.path.join(r'C:\\Users\\charl\\Documents\\GOST\\Yemen\\output_layers\\VM_graphs','pop_chart_%s' % groupa), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter plots: Vulnerability Matrix\n",
    "\n",
    "The previous analysis sees us generate plots which are summaries for each VM score for a given factor. Although interesting, it is also useful to see the distribution of districts within each VM factor band - so we can see if there is tight grouping, or not. We create a scatter plot to do that here. The code is extremely similar to the previous box.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if test_mode == 'on':\n",
    "    pass\n",
    "else:\n",
    "    \n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # again, make sure you have a correctly set up 'target_shp' file\n",
    "    target_shp2 = target_shp.rename({\n",
    "        'Overall Vu':'Overall Vulnerability Level',\n",
    "        'Health Sys':'Health System Capacity Score',\n",
    "        'Hazards':'Hazard Score',\n",
    "        'Impact on':'Impact on Exposed Population Score',\n",
    "        'Food Secur':'Food Security Score',\n",
    "        'Morbidity':'Morbidity Score',\n",
    "        'Nutrition':'Nutrition Score',\n",
    "        'WASH':'WASH Score',\n",
    "        'Social Det':'Social Determinants and Health Outcomes Score',\n",
    "        'total_pop':'Total Population',\n",
    "                                   }, axis = 1)\n",
    "    \n",
    "    p = 7\n",
    "\n",
    "    factors = ['Overall Vulnerability Level','Health System Capacity Score',\n",
    "              'Hazard Score','Impact on Exposed Population Score','Food Security Score',\n",
    "              'WASH Score','Social Determinants and Health Outcomes Score']\n",
    "\n",
    "    fracs = {'frac_30':'30 minutes',\n",
    "            'frac_60': '1 hour',\n",
    "            'frac_120': '2 hours',\n",
    "            'frac_240': '4 hours'}\n",
    "    \n",
    "    # we generate scatter plots for each accessibility threshold\n",
    "    for frac in ['frac_30','frac_60','frac_120','frac_240']:\n",
    "        \n",
    "        # and for each Vulnerability matrix factor\n",
    "        for groupa in factors:\n",
    "            plt.clf()\n",
    "            plt.figure(figsize=(8,4))\n",
    "            title = 'Fraction of district population living within %s of nearest HeRAMS hospital, \\nbreakdown by WHO %s' % (fracs[frac], groupa)\n",
    "            \n",
    "            # this time, we are not running a groupby function - so we plot all values\n",
    "            d = target_shp2[[groupa,frac]]\n",
    "            \n",
    "            # set color palette\n",
    "            pal = sns.cubehelix_palette(7, rot=-.5, light = .8, dark=.2)\n",
    "            \n",
    "            # plot on the axes\n",
    "            ax = sns.swarmplot(y = frac, x = groupa, data=d, palette=pal).set_title(title)\n",
    "            \n",
    "            # generate some boxplots to go on top to describe the data\n",
    "            ax = sns.boxplot(y = frac, x = groupa, data=d, whis=np.inf, color = \"1\", linewidth = 0.7, dodge = True)\n",
    "            \n",
    "            # save down\n",
    "            plt.savefig(os.path.join(r'C:\\Users\\charl\\Documents\\GOST\\Yemen\\output_layers\\VM_graphs','Boxplot_%s_%s.png'% (groupa, frac)), bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
