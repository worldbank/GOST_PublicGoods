{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.a - Walking Model\n",
    "\n",
    "We have built a fairly sophisticated walking model for the Yemen analysis, where walk times are adjusted based on the relative elevation and slope of direction of travel. This scripts sets up such a graph. \n",
    "\n",
    "It requires the user to have all of the SRTM tiles that relate to the country saved locally on their PC / workspace so that the various functions can call them when necessary.\n",
    "\n",
    "The tiles can be downloaded here: http://dwtkns.com/srtm30m/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the usual suspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os, time\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import rasterio as rt\n",
    "import numpy as np\n",
    "from affine import Affine\n",
    "from shapely.geometry import Point, LineString\n",
    "from functools import partial\n",
    "import pyproj\n",
    "from shapely.ops import transform\n",
    "from shapely.geometry import box\n",
    "from shapely.wkt import loads\n",
    "from rasterio import features\n",
    "import networkx as nx\n",
    "sys.path.append(r'C:\\Users\\charl\\Documents\\GitHub\\GOST_PublicGoods\\GOSTNets\\GOSTNets')\n",
    "sys.path.append(r'C:\\Users\\charl\\Documents\\GitHub\\GOST')\n",
    "import GOSTnet as gn\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Graph Read In\n",
    "\n",
    "This graph will usually be the 'final product' graph - after any adjustments for conflict, for example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bpth = r'C:\\Users\\charl\\Documents\\GOST\\Yemen\\graphtool'\n",
    "G = nx.read_gpickle(os.path.join(bpth, 'G_salty_time_conflict_adj.pickle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Walk all tiles, find path\n",
    "\n",
    "pth is the folder containing all of the SRTM tiles, downloaded in .hgt format, which rasterio can natively read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pth = r'C:\\Users\\charl\\Documents\\GOST\\Yemen\\SRTM\\high_res'\n",
    "tiles = []\n",
    "\n",
    "# pick up any files that end in extension 'hgt', add to tiles list.\n",
    "for root, folder, files in os.walk(pth):\n",
    "    for f in files:\n",
    "        if f[-3:] == 'hgt':\n",
    "            tiles.append(f[:-4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dictionary of tiles \n",
    "\n",
    "Here, we generate a dictionary, arrs, which is the loaded versions of each tile, where t is the filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arrs = {}\n",
    "for t in tiles:\n",
    "    fpath = r'C:\\Users\\charl\\Documents\\GOST\\Yemen\\SRTM\\high_res\\{}.hgt\\{}.hgt'.format(t, t)\n",
    "    arrs[t] = rt.open(fpath, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add correct tile for each node into Graph Data Dictionary\n",
    "\n",
    "In order to keep the matching on of elevation values to nodes nice and efficient, we devise a strategy whereby we assign the tile code to each node. The tiles are divided up by latitude and longitude coordinate; therefore, we can take the first two digits of the 'x' and 'y' attributes of each node to generate their tile 'code', attached as a new node attribute, 'code'. \n",
    "\n",
    "We also create a list of the unique tile codes, so we know if we have missed any when we downloaded the tiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uniques = []\n",
    "for u, data in G.nodes(data = True):\n",
    "    E = str(data['x'])[:2]\n",
    "    N = str(data['y'])[:2]\n",
    "    data['code'] = 'N{}E0{}'.format(N, E)\n",
    "    uniques.append('N{}E0{}'.format(N, E))\n",
    "    \n",
    "# create unique list of tilecodes\n",
    "unique_codes = list(set(uniques))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match on High Precision Elevation\n",
    "\n",
    "Now, for each tile, we go through and read the relevant values on to the nodes where their tilecode matches the existing tile's code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "property_name = 'elevation'\n",
    "\n",
    "# for each unique tile code...\n",
    "for code in unique_codes:\n",
    "    \n",
    "    # make a blank dictionary of nodes\n",
    "    list_of_nodes = {}\n",
    "    \n",
    "    # cycle through all nodes, and if their code is the same as current code, add their x and y to the dictionary of nodes\n",
    "    for u, data in G.nodes(data=True):\n",
    "        if data['code'] == code:\n",
    "            list_of_nodes.update({u:(data['x'], data['y'])})\n",
    "            \n",
    "    # now, we intersect these nodes with the raster, after doing an intersection\n",
    "    dataset = arrs[code]\n",
    "    b = dataset.bounds\n",
    "    \n",
    "    # make a bounding box for that tile\n",
    "    datasetBoundary = box(b[0], b[1], b[2], b[3])\n",
    "    \n",
    "    # iterate through to make sure of intersection...\n",
    "    selKeys = []\n",
    "    selPts = []\n",
    "    for key, pt in list_of_nodes.items():\n",
    "        if Point(pt[0], pt[1]).intersects(datasetBoundary):\n",
    "            selPts.append(pt)\n",
    "            selKeys.append(key)\n",
    "    \n",
    "    # then use the .sample method to get the elevation of the intersecting points\n",
    "    raster_values = list(dataset.sample(selPts))\n",
    "    raster_values = [x[0] for x in raster_values]\n",
    "\n",
    "    # generate new dictionary of {node ID: raster values}\n",
    "    ref = dict(zip(selKeys, raster_values))\n",
    "    \n",
    "    # attach this data to the nodes with that u ID. \n",
    "    for u, data in G.nodes(data=True):\n",
    "        if u in ref.keys():\n",
    "            data[property_name] = ref[u]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add on low precision elevation for missed nodes\n",
    "\n",
    "We repeat the above process, but this time we intersect with the SRTM 30 arc second product with filled voids. We only take the value from this raster if we have a no data value from the last step - i.e. the elevation is below -50m. \n",
    "\n",
    "This ensures we do not have any void values for the 'elevation' field for any nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for u, data in G.nodes(data=True):\n",
    "    if data['elevation'] < -50:\n",
    "        list_of_nodes.update({u:(data['x'], data['y'])})\n",
    "        \n",
    "tifpath = r'C:\\Users\\charl\\Documents\\GOST\\Yemen\\SRTM\\clipped'\n",
    "t = r'clipped_e20N40.tif'\n",
    "tt = os.path.join(tifpath, t)\n",
    "dataset = rt.open(tt, 'r')\n",
    "b = dataset.bounds\n",
    "datasetBoundary = box(b[0], b[1], b[2], b[3])\n",
    "selKeys = []\n",
    "selPts = []\n",
    "for key, pt in list_of_nodes.items():\n",
    "    if Point(pt[0], pt[1]).intersects(datasetBoundary):\n",
    "        selPts.append(pt)\n",
    "        selKeys.append(key)\n",
    "raster_values = list(dataset.sample(selPts))\n",
    "raster_values = [x[0] for x in raster_values]\n",
    "\n",
    "ref = dict(zip(selKeys, raster_values))\n",
    "for u, data in G.nodes(data=True):\n",
    "    if u in ref.keys():\n",
    "        data['elevation'] = ref[u]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the remaining mistakes, set elevation to 0\n",
    "\n",
    "If the above two processes STILL fail to find an elevation for every node, set it to 0 because God help that node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for u, data in G.nodes(data=True):\n",
    "    if data['elevation'] < 0:\n",
    "        data['elevation'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate dictionary of elevations for each node\n",
    "\n",
    "Having found an appropriate elevation value, we load this into a dictionary for referencing later called elev_dict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "elev_dict = {}\n",
    "for u, data in G.nodes(data = True):\n",
    "    if 'elevation' in data.keys(): \n",
    "        if data['elevation'] < 0:\n",
    "            elev_dict[u] = 0\n",
    "        else:\n",
    "            elev_dict[u] = data['elevation']\n",
    "    else:\n",
    "        elev_dict[u] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run a quick Sense check - by printing out the max and minimum elevation recorded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3652, 0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(list(elev_dict.values())), min(list(elev_dict.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate New Walking Data in existing edges\n",
    "\n",
    "Now, we can do the relatively easy bit of adding in walking travel times. We use Tobler's Hiking function to define the walking speed with respect to the incline ratio, which is itself easy to calculate now that we know the elevation of the start node and end node of every node in our graph. \n",
    "\n",
    "Tobler's hiking function: https://en.wikipedia.org/wiki/Tobler%27s_hiking_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def speed(incline_ratio, max_speed):\n",
    "    walkspeed = max_speed * np.exp(-3.5 * abs(incline_ratio + 0.05)) \n",
    "    return walkspeed\n",
    "\n",
    "# absolute max walkspeed, if heading downhill per Tobler's function\n",
    "max_walkspeed = 6\n",
    "\n",
    "# we set a floor speed to prevent ridiculous values\n",
    "min_speed = 0.1\n",
    "\n",
    "for u, v, data in G.edges(data = True):\n",
    "    \n",
    "    # pick up the start and end elevations\n",
    "    data['elev_start'] = elev_dict[u]\n",
    "    data['elev_end'] = elev_dict[v]\n",
    "    \n",
    "    # if the edge length is 0, the others will also be 0. Special case. \n",
    "    if data['length'] == 0:\n",
    "        data['walkspeed'] = 0\n",
    "        data['walk_time'] = 0\n",
    "    else:\n",
    "        # identify change in elevation \n",
    "        delta_elevation = data['elev_end'] - data['elev_start']\n",
    "        \n",
    "        # determine the incline ratio\n",
    "        incline_ratio = delta_elevation / data['length']\n",
    "        \n",
    "        # apply the speed function to get walkspeed\n",
    "        speed_kmph = speed(incline_ratio = incline_ratio, max_speed = max_walkspeed)\n",
    "        \n",
    "        # apply lower bound to values\n",
    "        speed_kmph = max(speed_kmph, min_speed)\n",
    "        \n",
    "        # set as walkspeed attirbute for that edge\n",
    "        data['walkspeed'] = speed_kmph\n",
    "        \n",
    "        # work out travel time\n",
    "        data['walk_time'] = data['length'] / 1000 * 3600 / speed_kmph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Down\n",
    "\n",
    "We save down the completed graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gn.save(G, 'walk_graph', bpth, nodes = False, edges = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
