{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Prepare OD Estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os, sys, time, importlib\n",
    "\n",
    "import GOSTnets as gn\n",
    "import importlib\n",
    "# import Network_Clean as gnClean\n",
    "importlib.reload(gn)\n",
    "\n",
    "import networkx as nx\n",
    "import osmnx\n",
    "from shapely.geometry import Point\n",
    "import numpy as np\n",
    "\n",
    "import rasterio\n",
    "from rasterio import features\n",
    "from shapely.wkt import loads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplif_meters = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_epsg = 4326\n",
    "target_epsg = 3106"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_speed = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = 'Current'\n",
    "# scenario = 'Padma'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WorldPop data determinants\n",
    "\n",
    "constraint_status = 'constrained'\n",
    "# constraint_status = 'unconstrained'\n",
    "\n",
    "wp_res = 100\n",
    "# wp_res = 250\n",
    "# wp_res = '1k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production date for outputs being used\n",
    "\n",
    "prod_date = '210329'\n",
    "# prod_date = '210503'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_pth = r'..\\..\\..\\GEO'\n",
    "\n",
    "adm_pth = r'../../../GEO/Boundaries'\n",
    "pop_pth = r'../../../GEO/Population/WorldPop'\n",
    "\n",
    "fin_pth = 'final'\n",
    "res_pth = 'results'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions and variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for sorting alphanumerically\n",
    "\n",
    "import re\n",
    "\n",
    "def sorted_nicely( l ): \n",
    "    \"\"\" Sort the given iterable in the way that humans expect.\"\"\" \n",
    "    convert = lambda text: int(text) if text.isdigit() else text \n",
    "    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ] \n",
    "    return sorted(l, key = alphanum_key)\n",
    "\n",
    "# funciton for sorting matrices smallest to largest, by origin ID then destination ID\n",
    "\n",
    "def sort_od_matrix(od_matrix):\n",
    "    \n",
    "    # sort by O_IDs, then dest node IDs\n",
    "    od_matrix = od_matrix.sort_values('Unnamed: 0').reindex(sorted_nicely(od_matrix.columns), axis=1)\n",
    "\n",
    "#     # reset O_ID column to the front\n",
    "#     od_matrix = od_matrix[ ['Unnamed: 0'] + [ col for col in od_matrix.columns if col != 'Unnamed: 0' ] ]\n",
    "\n",
    "    # set the Dest_ID column back to index so the shape is the same as the dWeight shape\n",
    "    od_matrix.set_index('Unnamed: 0',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_pckle = f'final_{scenario}_G_{simplif_meters}m_{prod_date}.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_gpickle(os.path.join(fin_pth, fin_pckle))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "origins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bgd_wp_constrained_origins_100m_2020_snapped_25m.csv\n"
     ]
    }
   ],
   "source": [
    "grid_name = f'bgd_wp_{constraint_status}_origins_{wp_res}m_2020_snapped_{simplif_meters}m.csv'\n",
    "print(grid_name)\n",
    "pop_origins = pd.read_csv(os.path.join(fin_pth,prod_date,grid_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2199542"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pop_origins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dfs of destinations as possible origins, for dest-to-dest analysis\n",
    "\n",
    "City_df = pd.read_csv(os.path.join(fin_pth,prod_date,\\\n",
    "                        f'All_cities_{constraint_status}_{wp_res}m_res_{simplif_meters}m_simplification_snapped.csv'))\n",
    "\n",
    "Dhaka_Chitt_df = pd.read_csv(os.path.join(fin_pth,prod_date,\\\n",
    "                        f'Dhaka_Chitt_{constraint_status}_{wp_res}m_res_{simplif_meters}m_simplification_snapped.csv'))\n",
    "Minor_cities_df = pd.read_csv(os.path.join(fin_pth,prod_date,\\\n",
    "                        f'Minor_cities_{constraint_status}_{wp_res}m_res_{simplif_meters}m_simplification_snapped.csv'))\n",
    "Dry_ports_df = pd.read_csv(os.path.join(fin_pth,prod_date,\\\n",
    "                        f'Dry_ports_{constraint_status}_{wp_res}m_res_{simplif_meters}m_simplification_snapped.csv'))\n",
    "River_ports_df = pd.read_csv(os.path.join(fin_pth,prod_date,\\\n",
    "                        f'River_ports_{constraint_status}_{wp_res}m_res_{simplif_meters}m_simplification_snapped.csv'))\n",
    "Deep_sea_ports_df = pd.read_csv(os.path.join(fin_pth,prod_date,\\\n",
    "                        f'Deep_sea_ports_{constraint_status}_{wp_res}m_res_{simplif_meters}m_simplification_snapped.csv'))\n",
    "\n",
    "All_SEZs_df = pd.read_csv(os.path.join(fin_pth,prod_date,\\\n",
    "                        f'All_SEZs_{constraint_status}_{wp_res}m_res_{simplif_meters}m_simplification_snapped.csv'))\n",
    "\n",
    "Active_SEZs_df = pd.read_csv(os.path.join(fin_pth,prod_date,\\\n",
    "                        f'Active_SEZs_{constraint_status}_{wp_res}m_res_{simplif_meters}m_simplification_snapped.csv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "destinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Destinations\n",
    "\n",
    "city_fil = r'Population\\\\Places\\\\bd_major_cities_pop_projs.gpkg'\n",
    "dhaka_chitt_fil = r'Population\\\\Places\\\\bd_cities_DhakaChittOnly_pop_projs.gpkg'\n",
    "minor_city_fil = r'Population\\\\Places\\\\bd_cities_noDhakaChitt_pop_projs.gpkg'\n",
    "dry_ports_fil = r'Economic\\\\Ports\\\\bd_landports_active.gpkg'\n",
    "river_ports_fil = r'P:\\BGD\\GEO\\Team\\Infrastructure_Diagnostic\\Nodes\\river_ports.gpkg'\n",
    "deep_sea_ports_fil = r'Economic\\\\Ports\\\\bd_DeepSeaPorts.gpkg'\n",
    "\n",
    "sezs_all_fil = r'Economic\\\\SEZs\\SEZs_all_pt_4326.gpkg'\n",
    "sezs_active_fil = r'Economic\\\\SEZs\\SEZs_active_pt_4326.gpkg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dests = {\"All_cities\" : city_fil, \"Dhaka_Chitt\" : dhaka_chitt_fil, \"Minor_cities\" : minor_city_fil, \\\n",
    "#          \"Dry_ports\" : dry_ports_fil, \"River_ports\" : river_ports_fil, \"Deep_sea_ports\" : deep_sea_ports_fil, \\\n",
    "#          'All_SEZs' : sezs_all_fil, 'Active_SEZs' : sezs_active_fil}\n",
    "\n",
    "# dests = {\"All_cities\" : city_fil, \"Dhaka_Chitt\" : dhaka_chitt_fil, \"Minor_cities\" : minor_city_fil, \\\n",
    "#          \"Dry_ports\" : dry_ports_fil, \"River_ports\" : river_ports_fil, \"Deep_sea_ports\" : deep_sea_ports_fil}\n",
    "\n",
    "# dests = {'All_SEZs' : sezs_all_fil, 'Active_SEZs' : sezs_active_fil}\n",
    "\n",
    "dests = {\"Dry_ports\" : dry_ports_fil, \"River_ports\" : river_ports_fil, \"Deep_sea_ports\" : deep_sea_ports_fil}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scenarios (for looping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "origins = {'PopOrigins' : [G,f'_{constraint_status}_{wp_res}m_res_{simplif_meters}m_simplification_snapped.csv',pop_origins,scenario],\\\n",
    "             'CityOrigins' : [G,f'_{constraint_status}_{wp_res}m_res_{simplif_meters}m_simplification_snapped.csv',City_df,scenario]}\n",
    "\n",
    "# origins = {'All_SEZ_Origins' : [G,f'_{constraint_status}_{wp_res}m_res_{simplif_meters}m_simplification_snapped.csv',All_SEZs_df,f'{scenario}'], \\\n",
    "#              'Active_SEZ_Origins' : [G,f'_{constraint_status}_{wp_res}m_res_{simplif_meters}m_simplification_snapped.csv',Active_SEZs_df,f'{scenario}']  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions that will generate OD matrices, and then generate direct walking times from origins to destinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def od_routine(G_input,orig_file,orig_label,scen,snap_ending):\n",
    "    \n",
    "    # Start timer\n",
    "    func_start = time.time()\n",
    "    \n",
    "    # main method\n",
    "    \n",
    "    origins_nodes = list(set(orig_file.NN)) # consolidates by shared nodes. \n",
    "    \n",
    "    for dest_type, fpth in dests.items():\n",
    "\n",
    "        snapfile = dest_type + snap_ending\n",
    "\n",
    "        print(dest_type)\n",
    "\n",
    "        dest = pd.read_csv(os.path.join(fin_pth, prod_date,snapfile))\n",
    "        dest_nodes = list(set(dest.NN))\n",
    "\n",
    "        print(len(list(set(dest.NN))))\n",
    "\n",
    "        od_time = gn.calculate_OD(G_input, origins=origins_nodes, \n",
    "                              destinations=dest_nodes, fail_value=99999999, weight='time')\n",
    "\n",
    "        od_time_df = pd.DataFrame(od_time, index=origins_nodes, columns=dest_nodes)\n",
    "\n",
    "        print(od_time_df.shape)\n",
    "\n",
    "        # Add walking time (from origin to NN) for each OD\n",
    "\n",
    "        # origins_join = origins_pop_snapped.merge(od_time_df, how='left', on='NN')\n",
    "        orig_file['NN_dist_seconds'] = ((orig_file.NN_dist / 1000) / walk_speed) * 60 * 60\n",
    "        origins_join = orig_file.join(od_time_df, on='NN', rsuffix=\"dist_\")\n",
    "\n",
    "    #     print(origins_join.head())\n",
    "\n",
    "        origins_join.columns[6:len(origins_join.columns)]\n",
    "\n",
    "        # export\n",
    "        \n",
    "        origins_join.to_csv(os.path.join(fin_pth,prod_date,f'{orig_label}_to_{dest_type}_{scenario}_walktime_{constraint_status}_{wp_res}m_res_{simplif_meters}m_simplification.csv'))\n",
    "        od_time_df.to_csv(os.path.join(fin_pth,prod_date,f'OD_matrix_{scenario}_{orig_label}_to_{dest_type}_{constraint_status}_{wp_res}m_res_{simplif_meters}m_simplification.csv'))\n",
    "        \n",
    "        # Report time\n",
    "        \n",
    "        func_end = time.time()\n",
    "        print('time elapsed for function')\n",
    "        print(str((func_end - func_start) / 60) + ' minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_shortest_TT(snap_ending,origin_file,orig_label,scen='',calc_walk_time=False):\n",
    "    \n",
    "    # Start timer\n",
    "    func_start = time.time()\n",
    "    \n",
    "    print(orig_label)\n",
    "\n",
    "    # prep origin grid referencing origin file\n",
    "\n",
    "    orig_grid = origin_file.rename(columns = {'Unnamed: 0' : 'Row_ID','NN':'O_ID','NN_dist':'walk_time_to_O_ID'})\n",
    "    orig_grid = orig_grid.set_index(orig_grid['O_ID'])\n",
    "\n",
    "    if calc_walk_time == True:\n",
    "        orig_grid['geometry'] = orig_grid['geometry'].apply(loads)\n",
    "    else:\n",
    "        None\n",
    "\n",
    "    # main method \n",
    "    for dest_type, fpth in dests.items():\n",
    "\n",
    "        snapfile = dest_type + snap_ending\n",
    "\n",
    "        print(dest_type)\n",
    "\n",
    "        # Load OD matrix\n",
    "\n",
    "        OD_name = f'OD_matrix_{scenario}_{orig_label}_to_{dest_type}_{constraint_status}_{wp_res}m_res_{simplif_meters}m_simplification.csv'\n",
    "        OD = pd.read_csv(os.path.join(fin_pth, prod_date, OD_name))\n",
    "        \n",
    "        # prepare OD matrix\n",
    "\n",
    "        OD = OD.rename(columns = {'Unnamed: 0':'O_ID'})\n",
    "        OD = OD.set_index('O_ID')\n",
    "        OD = OD.replace([np.inf, -np.inf], np.nan)\n",
    "        OD = OD.reindex(sorted_nicely(OD.columns), axis=1)\n",
    "\n",
    "        # Load in snapfile for geoms, nodes, and dest-to-node walk distance\n",
    "\n",
    "        dest_snap = pd.read_csv(os.path.join(fin_pth,prod_date,f'{dest_type}_{constraint_status}_{wp_res}m_res_{simplif_meters}m_simplification_snapped.csv'))\n",
    "        dest_snap.sort_values('NN',inplace=True)\n",
    "\n",
    "        dest_nodes = dest_snap[['NN','NN_dist']]  # let's use the same file\n",
    "        dest_nodes = dest_nodes.set_index('NN') # consider renaming something  more intuitive\n",
    "        dest_nodes.index = dest_nodes.index.map(str)\n",
    "        \n",
    "        # transpose the OD matrix and join the data to each of the destination nodes\n",
    "\n",
    "        odm_trans = OD.transpose()\n",
    "        \n",
    "        for i in odm_trans.columns:\n",
    "            dest_nodes[i] = odm_trans[i] # assign ODM values to dest_nodes -- order correct?\n",
    "\n",
    "#         for i in dest_nodes.columns:\n",
    "#             if i == 'NN_dist':\n",
    "#                 pass\n",
    "#             else:\n",
    "#                 dest_nodes[i] = dest_nodes[i] + dest_nodes['NN_dist'] # add walking time from dest point to nearest network node\n",
    "                \n",
    "        dest_nodes = dest_nodes.apply(lambda x: x + dest_nodes.NN_dist,axis=0)\n",
    "        dest_nodes['NN_dist'] = dest_nodes['NN_dist'] / 2 # restore to original value. This is faster than for looping and if filtering.\n",
    "                \n",
    "        # Prepare dest_nodes, now with corresponding ODM data per dest, to be joined to the final_grid\n",
    "\n",
    "        dest_nodes = dest_nodes.drop('NN_dist', axis = 1).transpose() # drop walk distance to nearest network node, now that it's included\n",
    "        dest_nodes['on_network_time'] = dest_nodes.min(axis = 1) # select shortest travel time as min time\n",
    "        dest_nodes['D_ID'] = dest_nodes.idxmin(axis=1) # calculation destination ID from column name\n",
    "        dest_nodes.columns.names = ['']\n",
    "        dest_nodes.index.names = ['O_ID']\n",
    "\n",
    "        # Add walk time from population origin point to network origin point\n",
    "\n",
    "        final_grid = pd.merge(orig_grid,dest_nodes[['D_ID','on_network_time']],how=\"left\",left_index=True,right_index=True)\n",
    "\n",
    "        final_grid['walk_time_to_O_ID'] = final_grid['walk_time_to_O_ID'] / 1000 * 3600 / walk_speed # refactor walking times to KMs and seconds\n",
    "        final_grid['total_time_net'] = final_grid['on_network_time'] + final_grid['walk_time_to_O_ID']\n",
    "\n",
    "        # Walking times from destination to network point -- optional subroutine\n",
    "        \n",
    "        if calc_walk_time == True:\n",
    "            \n",
    "            # prepare geodataframes of the origins (pop grid) and destinations (snap points) for walk time analysis\n",
    "            o_2_d_walktime = gpd.GeoDataFrame(final_grid, crs = {'init':'epsg:{}'.format(source_epsg)}, geometry = 'geometry')\n",
    "\n",
    "            dest_gdf = gpd.GeoDataFrame(dest_snap, crs = {'init':'epsg:4326'}, geometry = dest_snap['geometry'].apply(loads))\n",
    "            dest_gdf = dest_gdf.rename({\"NN\" : \"D_ID\"},axis=1).set_index('D_ID').drop(['Unnamed: 0','Number'],axis=1)\n",
    "\n",
    "            # Snapping to calculate direct walking times\n",
    "\n",
    "            print('start of snapping: %s\\n' % time.ctime())\n",
    "            o_2_d_walktime = gn.pandana_snap_points(o_2_d_walktime, \n",
    "                                       dest_gdf,\n",
    "                                       source_crs='epsg:{}'.format(source_epsg),\n",
    "                                       target_crs='epsg:{}'.format(target_epsg),\n",
    "                                       add_dist_to_node_col = True)\n",
    "            print('\\nend of snapping: %s' % time.ctime())\n",
    "            print('\\n--- processing complete')\n",
    "\n",
    "            # Calculate walk time direct\n",
    "            o_2_d_walktime['walk_time_direct'] = o_2_d_walktime['idx_dist'] / 1000 * 3600 / walk_speed\n",
    "            final_grid['walk_time_direct'] = o_2_d_walktime['walk_time_direct']\n",
    "            \n",
    "            # Calculate final quickest time in seconds\n",
    "            \n",
    "            final_grid['PLOT_TIME_SECS'] = final_grid[['walk_time_direct','total_time_net']].min(axis = 1)\n",
    "            \n",
    "            # Specify which value was taken as the quickest\n",
    "            \n",
    "            def choice(x):\n",
    "                if x.walk_time_direct < x.total_time_net:\n",
    "                    return 'walk'\n",
    "                else:\n",
    "                    return 'net'\n",
    "\n",
    "            final_grid['choice'] = final_grid.apply(lambda x: choice(x), axis = 1)\n",
    "        \n",
    "        # If skipping the subroutine, just calculate the seconds of travel from the total time\n",
    "        \n",
    "        else:\n",
    "            final_grid['PLOT_TIME_SECS'] = final_grid[['total_time_net']]\n",
    "            final_grid['choice'] = 'net' # to harmonize with datasets that differentiate between walking and net travel\n",
    "\n",
    "        # Final calculations and cleanup\n",
    "        \n",
    "        final_grid['PLOT_TIME_MINS'] = final_grid['PLOT_TIME_SECS'] / 60\n",
    "\n",
    "        final_grid = final_grid.drop('O_ID', axis = 1) # drop duplicate\n",
    "        \n",
    "        # Export\n",
    "\n",
    "        final_grid.to_csv(os.path.join(fin_pth,prod_date,f'final_od_grid_{scenario}_{dest_type}_{orig_label}_{constraint_status}_{wp_res}m_res_{simplif_meters}m_simplification.csv'))\n",
    "        final_grid.to_csv(os.path.join(fin_pth,prod_date,f'final_od_grid_{scenario}_{dest_type}_{orig_label}_{constraint_status}_{wp_res}m_res_{simplif_meters}m_simplification.csv'))        \n",
    "        \n",
    "        # Report time\n",
    "        \n",
    "        func_end = time.time()\n",
    "        print('time elapsed for function')\n",
    "        print(str((func_end - func_start) / 60) + ' minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load origins and destinations, get unique origin nodes, run OD, export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the OD routine function on all destinations for the 3 different scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2199542\n",
      "Dry_ports\n",
      "16\n",
      "(152099, 16)\n",
      "time elapsed for function\n",
      "6.219833568731944 minutes\n",
      "River_ports\n",
      "13\n",
      "(152099, 13)\n",
      "time elapsed for function\n",
      "11.15425864458084 minutes\n",
      "Deep_sea_ports\n",
      "4\n",
      "(152099, 4)\n",
      "time elapsed for function\n",
      "13.715617096424102 minutes\n",
      "43\n",
      "Dry_ports\n",
      "16\n",
      "(43, 16)\n",
      "time elapsed for function\n",
      "0.7144750038782756 minutes\n",
      "River_ports\n",
      "13\n",
      "(43, 13)\n",
      "time elapsed for function\n",
      "1.3122584581375123 minutes\n",
      "Deep_sea_ports\n",
      "4\n",
      "(43, 4)\n",
      "time elapsed for function\n",
      "1.4942334651947022 minutes\n"
     ]
    }
   ],
   "source": [
    "for orig, values in origins.items():\n",
    "    \n",
    "    print(len(values[2]))\n",
    "    \n",
    "    od_routine(values[0],\\\n",
    "               orig_file=values[2],\\\n",
    "               orig_label=orig,\\\n",
    "               scen=values[3],\\\n",
    "               snap_ending=values[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import completed OD matrix, calculate walking times from origins to the destinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PopOrigins\n",
      "Dry_ports\n",
      "time elapsed for function\n",
      "36.72903771400452 minutes\n",
      "River_ports\n",
      "time elapsed for function\n",
      "70.55681764682134 minutes\n",
      "Deep_sea_ports\n",
      "time elapsed for function\n",
      "106.80096070369085 minutes\n",
      "CityOrigins\n",
      "Dry_ports\n",
      "time elapsed for function\n",
      "0.0049080491065979 minutes\n",
      "River_ports\n",
      "time elapsed for function\n",
      "0.0077580650647481285 minutes\n",
      "Deep_sea_ports\n",
      "time elapsed for function\n",
      "0.01041640043258667 minutes\n"
     ]
    }
   ],
   "source": [
    "for orig, values in origins.items():\n",
    "    \n",
    "        prepare_shortest_TT(snap_ending=values[1],\\\n",
    "                            origin_file=values[2],\\\n",
    "                            orig_label=orig,\\\n",
    "                            scen=values[3],\\\n",
    "                            calc_walk_time=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That's it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
