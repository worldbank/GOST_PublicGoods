{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Catchment area generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook convert OD matrices into spatial catchment areas for destinations. The catchment area extents are determined by cutoff time extents (1 hour, 2 hours, etc.). Note that catchments are exclusive -- even if an origin location in reality has effective acces to 2 or more locations, it is only considered part of the nearest destination's catchment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import time\n",
    "\n",
    "# data science basics\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "\n",
    "# vector data basics\n",
    "import shapely\n",
    "from shapely import wkt\n",
    "from shapely.wkt import loads\n",
    "from shapely.ops import transform\n",
    "from shapely.geometry import Point, MultiPoint\n",
    "\n",
    "# raster data basics\n",
    "import rasterio\n",
    "from rasterio.profiles import DefaultGTiffProfile\n",
    "from rasterio.transform import from_origin\n",
    "from rasterio.features import rasterize\n",
    "\n",
    "# other\n",
    "import pyproj\n",
    "import geopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "def fxn():\n",
    "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fxn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for sorting alphanumerically\n",
    "\n",
    "import re\n",
    "\n",
    "def sorted_nicely( l ): \n",
    "    \"\"\" Sort the given iterable in the way that humans expect.\"\"\" \n",
    "    convert = lambda text: int(text) if text.isdigit() else text \n",
    "    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ] \n",
    "    return sorted(l, key = alphanum_key)\n",
    "\n",
    "# funciton for sorting matrices smallest to largest, by origin ID then destination ID\n",
    "\n",
    "def sort_od_matrix(od_matrix):\n",
    "    \n",
    "    # sort by O_IDs, then dest node IDs\n",
    "    od_matrix = od_matrix.sort_values('Unnamed: 0').reindex(sorted_nicely(od_matrix.columns), axis=1)\n",
    "\n",
    "    # reset O_ID column to the front\n",
    "    od_matrix = od_matrix[ ['Unnamed: 0'] + [ col for col in od_matrix.columns if col != 'Unnamed: 0' ] ]\n",
    "\n",
    "    # set the Dest_ID column back to index so the shape is the same as the dWeight shape\n",
    "    od_matrix.set_index('Unnamed: 0',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('max_columns',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplif_meters = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_epsg = 4326\n",
    "target_epsg = 3106"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WorldPop data parameters\n",
    "\n",
    "# constraint_status = 'constrained'\n",
    "constraint_status = 'unconstrained'\n",
    "\n",
    "# wp_res = 100\n",
    "wp_res = 250\n",
    "# wp_res = '1k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production date for outputs being used\n",
    "\n",
    "# prod_date = '210312'\n",
    "# prod_date = '210329'\n",
    "prod_date = '210503'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local folders\n",
    "input_pth = r'inputs\\\\dests'\n",
    "interm_pth = r'intermediate'\n",
    "fin_pth = r'final'\n",
    "res_pth = r'results'\n",
    "\n",
    "# Shared drive folders\n",
    "tab_pth = r'../../../Tabular'\n",
    "geo_pth = r'../../../GEO'\n",
    "origin_folder = r'..\\..\\..\\GEO\\Population'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define destinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Looping lists\n",
    "\n",
    "# dest_lst = ['All_cities', 'Minor_cities', 'Dhaka_Chitt',\\\n",
    "#             'Dry_ports', 'River_ports', 'Deep_sea_ports',\\\n",
    "#             'All_SEZs', 'Functioning_SEZs']\n",
    "\n",
    "dest_lst = ['All_SEZs', 'Functioning_SEZs']\n",
    "\n",
    "# destination = ['current_PopOrig_all_cities', 'current_PopOrig_deep_sea_ports', 'current_PopOrig_DhakaChitt', 'current_PopOrig_dry_ports', 'current_PopOrig_minor_cities',\\\n",
    "#                'current_CityOrig_all_cities', 'current_CityOrig_deep_sea_ports', 'current_CityOrig_DhakaChitt', 'current_CityOrig_dry_ports', 'current_CityOrig_minor_cities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping dicts\n",
    "\n",
    "# dests_time_filt_dct = {'All_cities_PopOrigins' : {}, 'Deep_sea_ports_PopOrigins' : {}, 'Dhaka_Chitt_PopOrigins' : {}, 'Dry_ports_PopOrigins' : {}, 'Minor_cities_PopOrigins' : {},\\\n",
    "#                       'All_cities_CityOrigins' : {}, 'Deep_sea_ports_CityOrigins' : {}, 'Dhaka_Chitt_CityOrigins' : {}, 'Dry_ports_CityOrigins' : {}, 'Minor_cities_CityOrigins': {}}\n",
    "\n",
    "# dests_pop_time_filt_dct = {'All_cities_PopOrigins' : {}, 'Deep_sea_ports_PopOrigins' : {}, 'Dhaka_Chitt_PopOrigins' : {}, 'Dry_ports_PopOrigins' : {}, 'Minor_cities_PopOrigins' : {},\\\n",
    "#                       'All_cities_CityOrigins' : {}, 'Deep_sea_ports_CityOrigins' : {}, 'Dhaka_Chitt_CityOrigins' : {}, 'Dry_ports_CityOrigins' : {}, 'Minor_cities_CityOrigins': {}}\n",
    "\n",
    "# dests_time_filt_dct = {'All_cities' : {}, 'Minor_cities' : {}, 'Dhaka_Chitt' : {},\\\n",
    "#                        'Dry_ports' : {}, 'River_ports' : {}, 'Deep_sea_ports': {},\\\n",
    "#                        'All_SEZs' : {}, 'Functioning_SEZs' : {}}\n",
    "\n",
    "# dests_pop_time_filt_dct = {'All_cities' : {}, 'Minor_cities' : {}, 'Dhaka_Chitt' : {},\\\n",
    "#                            'Dry_ports' : {}, 'River_ports' : {}, 'Deep_sea_ports' : {},\\\n",
    "#                            'All_SEZs' : {}, 'Functioning_SEZs' : {}}\n",
    "\n",
    "dests_time_filt_dct = {'All_SEZs' : {}, 'Functioning_SEZs' : {}}\n",
    "\n",
    "dests_pop_time_filt_dct = {'All_SEZs' : {}, 'Functioning_SEZs' : {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # rename stuff that's badly named\n",
    "# import re\n",
    "\n",
    "# for key in dests_pop_time_filt_dct.keys():\n",
    "#     print(re.search('^(.*?_){2}',key).group())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time filters to loop over (in minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minute-wise time cutoffs as needed\n",
    "\n",
    "# time_filters = [60, 90, 120, 180]\n",
    "time_filters = [15,30,45,60,90]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabular data transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New, per destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All_SEZs\n",
      "Functioning_SEZs\n"
     ]
    }
   ],
   "source": [
    "# Loop over each destination, computing all the relevant, filtered and aggregated dataframes for later comptuational usage\n",
    "\n",
    "for dest,v in dests_pop_time_filt_dct.items():\n",
    "    \n",
    "    print(dest)\n",
    "    \n",
    "    # read in od grid for calculations\n",
    "\n",
    "    dest_origs = pd.read_csv(os.path.join(res_pth,prod_date,f'final_od_grid_{dest}_PopOrigins_{constraint_status}_{wp_res}m_res_{simplif_meters}m_simplification.csv'))\n",
    "\n",
    "    # make dest_origs spatial \n",
    "\n",
    "    dest_origs['geometry'] = dest_origs['geometry'].apply(wkt.loads)\n",
    "    dest_origs_gdf = gpd.GeoDataFrame(dest_origs,geometry='geometry')\n",
    "    dest_origs_gdf['lon'] = dest_origs_gdf.geometry.x\n",
    "    dest_origs_gdf['lat'] = dest_origs_gdf.geometry.y\n",
    "\n",
    "    # Calculate raw filtered origin dataframes, populate to a dict\n",
    "\n",
    "    raw_time_filt_dct = {}\n",
    "\n",
    "    for t in time_filters:\n",
    "\n",
    "        df = dest_origs_gdf[dest_origs_gdf['PLOT_TIME_MINS'] <= t]  \n",
    "        raw_time_filt_dct.update({t:df})\n",
    "\n",
    "    # Calculate the aggregate population for the *nearest* destination per time range\n",
    "\n",
    "    pop_time_filt_dct = {}\n",
    "\n",
    "    for k, v in raw_time_filt_dct.items():\n",
    "        df = pd.pivot_table(v,values='VALUE',index='D_ID',aggfunc='sum')\\\n",
    "                .rename(columns={'VALUE' : 'Pop'})\\\n",
    "                .reset_index()\n",
    "        pop_time_filt_dct.update({k:df})\n",
    "\n",
    "    # Insert these dics of filtered, aggregated data frames as the values of the master destination dict\n",
    "\n",
    "    dests_time_filt_dct[dest] = raw_time_filt_dct\n",
    "    dests_pop_time_filt_dct[dest] = pop_time_filt_dct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consolidate population per time band per destination and export to a shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All_SEZs\n",
      "Functioning_SEZs\n"
     ]
    }
   ],
   "source": [
    "for dest_key, val_dct in dests_pop_time_filt_dct.items():\n",
    "    \n",
    "    print(dest_key)\n",
    "    \n",
    "    dest_gdf = pd.read_csv(os.path.join(fin_pth,prod_date,f'{dest_key}_{constraint_status}_{wp_res}m_res_{simplif_meters}m_simplification_snapped.csv'))\n",
    "    \n",
    "#     # rename Destination columns\n",
    "#     if 'City' in dest_gdf.columns:\n",
    "#         dest_gdf.rename({'City':'Destination'},axis=1,inplace=True)\n",
    "#     elif 'RIVER_PORT' in dest.gdf.columns:\n",
    "#         dest_gdf.rename({'RIVER_PORT':'Destination'},axis=1,inplace=True)\n",
    "#     else:\n",
    "#         None\n",
    "\n",
    "    # load geometry of GDF\n",
    "    \n",
    "    dest_gdf['geometry'] = dest_gdf['geometry'].apply(wkt.loads)\n",
    "    dest_gdf = gpd.GeoDataFrame(dest_gdf,geometry='geometry')\n",
    "    dest_gdf.rename({'NN':'D_ID'},axis=1,inplace=True)\n",
    "    dest_gdf.sort_values(by='D_ID',inplace=True)\n",
    "    \n",
    "    # Merge the population with the destination GDF, then replace the val_dct with that, renamed for interpretability\n",
    "    \n",
    "    for t, val_df in val_dct.items():\n",
    "        \n",
    "        time_cutoff = str(t) + 'min'\n",
    "        \n",
    "        dest_gdf = pd.merge(dest_gdf,val_df.rename(columns={'Pop' : time_cutoff}),how=\"left\",on='D_ID')\n",
    "#         print(dest_gdf.head())\n",
    "#         print(val_df.head())\n",
    "        val_dct.update({t:dest_gdf[['D_ID','Destination',time_cutoff]].rename(columns={time_cutoff : 'Pop'})})\n",
    "        \n",
    "    dest_gdf.to_file(os.path.join(res_pth,prod_date,f\"spatial/{dest_key}_catchment_pops.shp\"),driver=\"ESRI Shapefile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Destination</th>\n",
       "      <th>15min</th>\n",
       "      <th>30min</th>\n",
       "      <th>45min</th>\n",
       "      <th>60min</th>\n",
       "      <th>90min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gazaria: Gajaria Economic Zone, Abdul Monem Ec...</td>\n",
       "      <td>66674.635033</td>\n",
       "      <td>2.314301e+05</td>\n",
       "      <td>3.409023e+05</td>\n",
       "      <td>3.926155e+05</td>\n",
       "      <td>4.136701e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gopalganj Sadar: Gopalganj Economic Zone – 2</td>\n",
       "      <td>43343.070404</td>\n",
       "      <td>1.030999e+05</td>\n",
       "      <td>1.857709e+05</td>\n",
       "      <td>2.748822e+05</td>\n",
       "      <td>5.080918e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Meghna: Cumilla Economic Zone</td>\n",
       "      <td>20550.103360</td>\n",
       "      <td>7.473120e+04</td>\n",
       "      <td>1.797363e+05</td>\n",
       "      <td>3.123102e+05</td>\n",
       "      <td>7.438629e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jamalpur Sadar: Jamalpur Economic Zone, Jamalp...</td>\n",
       "      <td>100939.568390</td>\n",
       "      <td>2.286513e+05</td>\n",
       "      <td>3.735190e+05</td>\n",
       "      <td>6.365170e+05</td>\n",
       "      <td>1.328500e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Patiya: Patia Economic Zone</td>\n",
       "      <td>143467.746347</td>\n",
       "      <td>2.417314e+06</td>\n",
       "      <td>4.005090e+06</td>\n",
       "      <td>4.600692e+06</td>\n",
       "      <td>5.266895e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Shibalaya: Manikganj Economic Zone (Unused lan...</td>\n",
       "      <td>10672.052635</td>\n",
       "      <td>3.437969e+04</td>\n",
       "      <td>7.732375e+04</td>\n",
       "      <td>1.304657e+05</td>\n",
       "      <td>3.697796e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Gowainghat: Sylhet Special Economic Zone</td>\n",
       "      <td>5962.576889</td>\n",
       "      <td>3.147978e+04</td>\n",
       "      <td>4.814463e+04</td>\n",
       "      <td>8.538116e+04</td>\n",
       "      <td>2.611116e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Debiganj: Panchagarh Economic Zone</td>\n",
       "      <td>37004.454521</td>\n",
       "      <td>1.685896e+05</td>\n",
       "      <td>3.714651e+05</td>\n",
       "      <td>5.854799e+05</td>\n",
       "      <td>1.137145e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Chunarughat: Habiganj Economic Zone</td>\n",
       "      <td>1773.766033</td>\n",
       "      <td>1.187963e+04</td>\n",
       "      <td>2.407091e+04</td>\n",
       "      <td>3.700087e+04</td>\n",
       "      <td>1.263690e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sreepur: Shreepur Economic Zone</td>\n",
       "      <td>45441.737579</td>\n",
       "      <td>2.848126e+05</td>\n",
       "      <td>6.336589e+05</td>\n",
       "      <td>1.036795e+06</td>\n",
       "      <td>2.118544e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Destination          15min  \\\n",
       "0  Gazaria: Gajaria Economic Zone, Abdul Monem Ec...   66674.635033   \n",
       "1       Gopalganj Sadar: Gopalganj Economic Zone – 2   43343.070404   \n",
       "2                      Meghna: Cumilla Economic Zone   20550.103360   \n",
       "3  Jamalpur Sadar: Jamalpur Economic Zone, Jamalp...  100939.568390   \n",
       "4                        Patiya: Patia Economic Zone  143467.746347   \n",
       "5  Shibalaya: Manikganj Economic Zone (Unused lan...   10672.052635   \n",
       "6           Gowainghat: Sylhet Special Economic Zone    5962.576889   \n",
       "7                 Debiganj: Panchagarh Economic Zone   37004.454521   \n",
       "8                Chunarughat: Habiganj Economic Zone    1773.766033   \n",
       "9                    Sreepur: Shreepur Economic Zone   45441.737579   \n",
       "\n",
       "          30min         45min         60min         90min  \n",
       "0  2.314301e+05  3.409023e+05  3.926155e+05  4.136701e+05  \n",
       "1  1.030999e+05  1.857709e+05  2.748822e+05  5.080918e+05  \n",
       "2  7.473120e+04  1.797363e+05  3.123102e+05  7.438629e+05  \n",
       "3  2.286513e+05  3.735190e+05  6.365170e+05  1.328500e+06  \n",
       "4  2.417314e+06  4.005090e+06  4.600692e+06  5.266895e+06  \n",
       "5  3.437969e+04  7.732375e+04  1.304657e+05  3.697796e+05  \n",
       "6  3.147978e+04  4.814463e+04  8.538116e+04  2.611116e+05  \n",
       "7  1.685896e+05  3.714651e+05  5.854799e+05  1.137145e+06  \n",
       "8  1.187963e+04  2.407091e+04  3.700087e+04  1.263690e+05  \n",
       "9  2.848126e+05  6.336589e+05  1.036795e+06  2.118544e+06  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dest_gdf.filter(regex='Destination|min').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert CSV to raster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two options -- rasterio way below (https://stackoverflow.com/questions/62472750/how-to-rasterize-a-pandas-dataframe-with-many-points-per-pixel) or gdal_grid method (https://gis.stackexchange.com/questions/254330/python-gdal-grid-correct-use)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "also useful : https://gis.stackexchange.com/questions/279953/numpy-array-to-gtiff-using-rasterio-without-source-raster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function for outputting raster catchment extents, optionally with the per-cell population in the second band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extent_catch(dest,filt_val,filtered_df,pop=False):\n",
    "\n",
    "    # read in existing worldpop raster to provide metadata conditions for new layers\n",
    "\n",
    "    with rasterio.open(os.path.join(origin_folder,f'WorldPop/{constraint_status}/bgd_ppp_2020_UNadj_{constraint_status}_{wp_res}m_3106.tif')) as wp_src:\n",
    "        prof = wp_src.profile\n",
    "        if pop == False:\n",
    "            None\n",
    "        else:\n",
    "            prof.update(count=2) # set number of bands\n",
    "\n",
    "    # Rasterize by nearest destination ID and filter out above maximum value\n",
    "\n",
    "    with rasterio.open(f\"results/{prod_date}/spatial/{dest}_{filt_val}min_catch.tif\", 'w+',**prof) as out:\n",
    "\n",
    "        out.nodata = -9999\n",
    "                       \n",
    "        # Read in the respective bands for later writing\n",
    "        out_arr1 = out.read(1)\n",
    "\n",
    "        # create a generator of geom, value pairs to use in rasterizing, then rasterize\n",
    "        dest_shapes = ((geom, dest_id) for geom, dest_id in zip(filtered_df.geometry, filtered_df[\"D_ID\"].astype(int)))\n",
    "        dest_burned = rasterize(shapes=dest_shapes, fill=0, out=out_arr1, transform=out.transform)\n",
    "        \n",
    "        # write band\n",
    "        \n",
    "        out.write_band(1, dest_burned)\n",
    "                       \n",
    "        if pop == False:\n",
    "            None\n",
    "        else:\n",
    "            out_arr2 = out.read(2)\n",
    "\n",
    "            time_shapes = ((geom, time_to_reach) for geom, time_to_reach in zip(filtered_df.geometry, filtered_df[\"PLOT_TIME_MINS\"].astype(int)))\n",
    "            time_burned = rasterize(shapes=time_shapes, fill=0, out=out_arr2, transform=out.transform)\n",
    "\n",
    "            out.write_band(2, time_burned)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop over each filtered dataframe of origins and output as a raster to the prod_date folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All_SEZs\n",
      "15\n",
      "30\n",
      "45\n",
      "60\n",
      "90\n",
      "Functioning_SEZs\n",
      "15\n",
      "30\n",
      "45\n",
      "60\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "for dest_key, val_dct in dests_time_filt_dct.items():\n",
    "    print(dest_key)\n",
    "    for t, v in val_dct.items():\n",
    "        print(t)\n",
    "        extent_catch(dest_key,t,v,pop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert raster to polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to convert rasters to polygons and join in the population covered by each catchment\n",
    "\n",
    "def catch_rast_to_poly(dest_name, catch_rast, rast_profile, time_filt, dest_pop_df):\n",
    "\n",
    "    # Start timer\n",
    "    \n",
    "    func_start = time.time()\n",
    "\n",
    "    # open each created raster\n",
    "    \n",
    "    with rasterio.open(catch_rast, 'r',**rast_profile) as rast:\n",
    "\n",
    "        # populate geoms list\n",
    "\n",
    "        results = (\n",
    "            {'properties': {'D_ID': v}, 'geometry': s}\n",
    "            for i, (s, v) \n",
    "            in enumerate(\n",
    "                rasterio.features.shapes(rast.read(1), transform=rast.transform)))\n",
    "\n",
    "        geoms = list(results)\n",
    "\n",
    "        # convert to GDF, clean up, and dissolve\n",
    "\n",
    "        catch_poly = gpd.GeoDataFrame.from_features(geoms)\n",
    "        catch_poly['D_ID'] = catch_poly['D_ID'].astype(int)\n",
    "        catch_poly['D_ID'].replace(-9999,0,inplace=True) # replace nulls with 0\n",
    "        catch_poly = catch_poly.dissolve(by='D_ID')\n",
    "        \n",
    "        # join in total population, drop uncovered areas\n",
    "\n",
    "        catch_poly = pd.merge(catch_poly,dest_pop_df,how='left',on='D_ID')\n",
    "        catch_poly = catch_poly[catch_poly['D_ID'] != 0]\n",
    "        catch_poly.crs = f\"EPSG:{target_epsg}\"\n",
    "        catch_poly = catch_poly.to_crs(source_epsg)\n",
    "\n",
    "        # export to shapefile\n",
    "\n",
    "        catch_poly.to_file(f\"results/{prod_date}/spatial/{dest_name}_{time_filt}min_catch_poly.shp\",driver=\"ESRI Shapefile\")\n",
    "        return catch_poly\n",
    "            \n",
    "    # Report function time\n",
    "    \n",
    "    func_end = time.time()\n",
    "    print(f'time elapsed for summing {dest_name}')\n",
    "    print(str((func_end - func_start) / 60) + ' minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create polygons from the catchment rasters and join in the populations covered for each destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All_SEZs\n",
      "15\n",
      "30\n",
      "45\n",
      "60\n",
      "90\n",
      "Functioning_SEZs\n",
      "15\n",
      "30\n",
      "45\n",
      "60\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "with rasterio.open(os.path.join(origin_folder,f'WorldPop/{constraint_status}/bgd_ppp_2020_UNadj_{constraint_status}_{wp_res}m_{source_epsg}.tif')) as wp_src:\n",
    "    \n",
    "    prof = wp_src.profile\n",
    "    prof.update(count=2)\n",
    "\n",
    "    for dest_key, val_dct in dests_pop_time_filt_dct.items():\n",
    "        \n",
    "        # Keep track of which destination is being processed\n",
    "        print(dest_key)\n",
    "        \n",
    "        for t, val_df in val_dct.items():\n",
    "            \n",
    "            # Keep track of which travel time is being processed\n",
    "            print(t)\n",
    "            \n",
    "            catch_rast = f\"results/{prod_date}/spatial/{dest_key}_{t}min_catch.tif\"\n",
    "\n",
    "            catch_rast_to_poly(dest_name=dest_key,catch_rast=catch_rast,rast_profile = prof,time_filt=t,dest_pop_df=val_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
