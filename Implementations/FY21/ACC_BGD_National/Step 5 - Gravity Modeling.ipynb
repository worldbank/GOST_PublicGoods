{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 - Gravity Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gravity modeling process can be broken into two primary sections -- one in which the OD matrices are manipulated into shapes compatiable with the gravity modeling tools, and one in which the actual gravity models are developed and exported for origins and destinations. This notebook covers both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "\n",
    "import networkx as nx\n",
    "from scipy import spatial\n",
    "from functools import partial\n",
    "import pyproj\n",
    "\n",
    "import shapely\n",
    "from shapely.ops import transform\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Point, MultiPoint\n",
    "from shapely.wkt import loads\n",
    "\n",
    "import GOSTnets as gn\n",
    "import geopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixed parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplif_meters = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_epsg = 4326\n",
    "target_epsg = 3106"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = 'Current'\n",
    "# scenario = 'Padma'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WorldPop data determinants\n",
    "\n",
    "constraint_status = 'constrained'\n",
    "# constraint_status = 'unconstrained'\n",
    "\n",
    "wp_res = 100\n",
    "# wp_res = 250\n",
    "# wp_res = '1k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production date for outputs being used\n",
    "\n",
    "prod_date = '210329'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "origins = 'PopOrigins'\n",
    "# origins = 'CityOrigins'\n",
    "# origins = 'All_SEZ_Origins'\n",
    "# origins = 'Active_SEZ_Origins'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Path names and file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_pth = r'inputs\\\\dests'\n",
    "interm_pth = r'intermediate'\n",
    "fin_pth = r'final'\n",
    "res_pth = r'results'\n",
    "\n",
    "tab_pth = r'../../../Tabular'\n",
    "geo_pth = r'../../../GEO'\n",
    "origin_folder = r'..\\..\\..\\GEO\\Population'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('max_columns',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the gravity model throws error messages that reduce readibility, let's get rid of them\n",
    "\n",
    "import warnings\n",
    "\n",
    "def fxn():\n",
    "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fxn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for sorting alphanumerically\n",
    "\n",
    "import re\n",
    "\n",
    "def sorted_nicely( l ): \n",
    "    \"\"\" Sort the given iterable in the way that humans expect.\"\"\" \n",
    "    convert = lambda text: int(text) if text.isdigit() else text \n",
    "    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ] \n",
    "    return sorted(l, key = alphanum_key)\n",
    "\n",
    "# funciton for sorting matrices smallest to largest, by origin ID then destination ID\n",
    "\n",
    "def sort_od_matrix(od_matrix):\n",
    "    \n",
    "    # sort by O_IDs, then dest node IDs\n",
    "    od_matrix = od_matrix.sort_values('Unnamed: 0').reindex(sorted_nicely(od_matrix.columns), axis=1)\n",
    "\n",
    "#     # reset O_ID column to the front\n",
    "#     od_matrix = od_matrix[ ['Unnamed: 0'] + [ col for col in od_matrix.columns if col != 'Unnamed: 0' ] ]\n",
    "\n",
    "    # set the Dest_ID column back to index so the shape is the same as the dWeight shape\n",
    "    od_matrix.set_index('Unnamed: 0',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial join admin file to origin file based on within location\n",
    "\n",
    "def georef_origins(orig_fil,adm_fil):\n",
    "    orig_fil['geometry'] = orig_fil['geometry'].apply(wkt.loads)\n",
    "    orig_fil = gpd.GeoDataFrame(orig_fil,geometry='geometry')\n",
    "    orig_fil = gpd.sjoin(orig_fil,adm_fil,how='left',op='within')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization functions to slim the file sizes of our many large input files and thus speed up processing / export\n",
    "\n",
    "from typing import List\n",
    "\n",
    "def optimize_floats(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    floats = df.select_dtypes(include=['float64']).columns.tolist()\n",
    "    df[floats] = df[floats].apply(pd.to_numeric, downcast='float')\n",
    "    return df\n",
    "\n",
    "def optimize_ints(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    ints = df.select_dtypes(include=['int64']).columns.tolist()\n",
    "    df[ints] = df[ints].apply(pd.to_numeric, downcast='integer')\n",
    "    return df\n",
    "\n",
    "def optimize(df: pd.DataFrame, datetime_features: List[str] = []):\n",
    "    return optimize_floats(optimize_ints(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary of destination DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Destination DFs\n",
    "\n",
    "City_df = pd.read_csv(os.path.join(fin_pth,prod_date,\\\n",
    "                        f'All_cities_{constraint_status}_{wp_res}m_res_{simplif_meters}m_simplification_snapped.csv'))\n",
    "Dhaka_Chitt_df = pd.read_csv(os.path.join(fin_pth,prod_date,\\\n",
    "                        f'Dhaka_Chitt_{constraint_status}_{wp_res}m_res_{simplif_meters}m_simplification_snapped.csv'))\n",
    "Minor_cities_df = pd.read_csv(os.path.join(fin_pth,prod_date,\\\n",
    "                        f'Minor_cities_{constraint_status}_{wp_res}m_res_{simplif_meters}m_simplification_snapped.csv'))\n",
    "Dry_ports_df = pd.read_csv(os.path.join(fin_pth,prod_date,\\\n",
    "                        f'Dry_ports_{constraint_status}_{wp_res}m_res_{simplif_meters}m_simplification_snapped.csv'))\n",
    "River_ports_df = pd.read_csv(os.path.join(fin_pth,prod_date,\\\n",
    "                        f'River_ports_{constraint_status}_{wp_res}m_res_{simplif_meters}m_simplification_snapped.csv'))\n",
    "Deep_sea_ports_df = pd.read_csv(os.path.join(fin_pth,prod_date,\\\n",
    "                        f'Deep_sea_ports_{constraint_status}_{wp_res}m_res_{simplif_meters}m_simplification_snapped.csv'))\n",
    "All_SEZs_df = pd.read_csv(os.path.join(fin_pth,prod_date,\\\n",
    "                        f'All_SEZs_{constraint_status}_{wp_res}m_res_{simplif_meters}m_simplification_snapped.csv'))\n",
    "Active_SEZs_df = pd.read_csv(os.path.join(fin_pth,prod_date,\\\n",
    "                        f'Active_SEZs_{constraint_status}_{wp_res}m_res_{simplif_meters}m_simplification_snapped.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dest dictionary\n",
    "\n",
    "dests = {\"All_cities\" : City_df, \"Dhaka_Chitt\" : Dhaka_Chitt_df, \"Minor_cities\" : Minor_cities_df, \\\n",
    "         \"Dry_ports\" : Dry_ports_df, \"River_ports\" : River_ports_df, \"Deep_sea_ports\" : Deep_sea_ports_df, \\\n",
    "         \"All_SEZs\" : All_SEZs_df, \"Active_SEZs\" : Active_SEZs_df}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Admin data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load admin spatial data\n",
    "\n",
    "adm2 = gpd.read_file(os.path.join(geo_pth,'Boundaries/bd_adm_bbs.gpkg'),layer='bd_adm2_wgs84')\n",
    "adm2.crs = 'epsg:4326'\n",
    "\n",
    "adm3 = gpd.read_file(os.path.join(geo_pth,'Boundaries/bd_adm_bbs.gpkg'),layer='bd_adm3_wgs84')\n",
    "adm3.crs = 'epsg:4326'\n",
    "\n",
    "adm4 = gpd.read_file(os.path.join(geo_pth,'Boundaries/bd_adm_bbs.gpkg'),layer='bd_adm4_wgs84')\n",
    "adm4.crs = 'epsg:4326'\n",
    "\n",
    "adm5 = gpd.read_file(os.path.join(geo_pth,'Boundaries/bd_adm_bbs.gpkg'),layer='bd_adm5_wgs84')\n",
    "adm5.crs = 'epsg:4326'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix types for later joining\n",
    "adm3['adm3_pcode'] = adm3['adm3_pcode'].astype(str)\n",
    "adm4['adm4_pcode'] = adm4['adm4_pcode'].astype(str)\n",
    "\n",
    "# Fix names for later display\n",
    "adm3['adm3_en'] = adm3['adm3_en'].str.title()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set an index on the NN, reindex by an alphanumerically sorted list of IDs, then reset_index to return the NNs to the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dest, df in dests.items():\n",
    "    df[['Unweighted']] = 1\n",
    "    df = df.sort_values('NN').set_index('NN').reset_index() # sort for correct joining, set/reset index to move NN column to the front for easy checking\n",
    "    dests.update({dest:df})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check results, especially for IDs for use in gravity modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NN</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Number</th>\n",
       "      <th>City</th>\n",
       "      <th>Area_sqkm</th>\n",
       "      <th>Households</th>\n",
       "      <th>Pop_all_2011</th>\n",
       "      <th>Pop_male_2011</th>\n",
       "      <th>Pop_female_2011</th>\n",
       "      <th>Pop_all_2021</th>\n",
       "      <th>...</th>\n",
       "      <th>Pop_female_2021</th>\n",
       "      <th>Literacy_7yr_plus</th>\n",
       "      <th>Ranking_by_pop_2011</th>\n",
       "      <th>Type</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>2021_growth_rate</th>\n",
       "      <th>geometry</th>\n",
       "      <th>NN_dist</th>\n",
       "      <th>Unweighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6253</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Brahmanbaria</td>\n",
       "      <td>22.49</td>\n",
       "      <td>38329</td>\n",
       "      <td>193814</td>\n",
       "      <td>96468</td>\n",
       "      <td>97346</td>\n",
       "      <td>2.293939e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.152165e+05</td>\n",
       "      <td>64.4</td>\n",
       "      <td>18</td>\n",
       "      <td>Municipal Corporation</td>\n",
       "      <td>91.111674</td>\n",
       "      <td>23.966521</td>\n",
       "      <td>0.183578</td>\n",
       "      <td>POINT (91.11167399999999 23.966521)</td>\n",
       "      <td>51.070479</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6613</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>Mymenshingh</td>\n",
       "      <td>70.98</td>\n",
       "      <td>82687</td>\n",
       "      <td>389918</td>\n",
       "      <td>200053</td>\n",
       "      <td>189865</td>\n",
       "      <td>4.549655e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.215389e+05</td>\n",
       "      <td>66.4</td>\n",
       "      <td>8</td>\n",
       "      <td>City Corporation</td>\n",
       "      <td>90.397200</td>\n",
       "      <td>24.761270</td>\n",
       "      <td>0.166823</td>\n",
       "      <td>POINT (90.3972 24.76127)</td>\n",
       "      <td>55.394221</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21729</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>Jamalpur</td>\n",
       "      <td>55.25</td>\n",
       "      <td>35619</td>\n",
       "      <td>150172</td>\n",
       "      <td>75068</td>\n",
       "      <td>75104</td>\n",
       "      <td>1.683277e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>8.418403e+04</td>\n",
       "      <td>49.3</td>\n",
       "      <td>30</td>\n",
       "      <td>Municipal Corporation</td>\n",
       "      <td>89.944550</td>\n",
       "      <td>24.927120</td>\n",
       "      <td>0.120899</td>\n",
       "      <td>POINT (89.94455000000001 24.92712)</td>\n",
       "      <td>62.479275</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38279</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>Dhaka</td>\n",
       "      <td>316.00</td>\n",
       "      <td>2034146</td>\n",
       "      <td>8906039</td>\n",
       "      <td>4931802</td>\n",
       "      <td>3974237</td>\n",
       "      <td>9.733517e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>4.343491e+06</td>\n",
       "      <td>74.6</td>\n",
       "      <td>1</td>\n",
       "      <td>City Corporation</td>\n",
       "      <td>90.409879</td>\n",
       "      <td>23.728216</td>\n",
       "      <td>0.092912</td>\n",
       "      <td>POINT (90.409879 23.728216)</td>\n",
       "      <td>27.815607</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53188</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>Feni</td>\n",
       "      <td>22.00</td>\n",
       "      <td>31468</td>\n",
       "      <td>156971</td>\n",
       "      <td>82554</td>\n",
       "      <td>74417</td>\n",
       "      <td>1.800602e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>8.536317e+04</td>\n",
       "      <td>69.7</td>\n",
       "      <td>27</td>\n",
       "      <td>Municipal Corporation</td>\n",
       "      <td>91.395520</td>\n",
       "      <td>23.006310</td>\n",
       "      <td>0.147092</td>\n",
       "      <td>POINT (91.39551999999999 23.00631)</td>\n",
       "      <td>107.214995</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      NN  Unnamed: 0  Number          City  Area_sqkm  Households  \\\n",
       "0   6253           4       5  Brahmanbaria      22.49       38329   \n",
       "1   6613          25      25   Mymenshingh      70.98       82687   \n",
       "2  21729          17      17      Jamalpur      55.25       35619   \n",
       "3  38279          12      12         Dhaka     316.00     2034146   \n",
       "4  53188          15      15          Feni      22.00       31468   \n",
       "\n",
       "   Pop_all_2011  Pop_male_2011  Pop_female_2011  Pop_all_2021  ...  \\\n",
       "0        193814          96468            97346  2.293939e+05  ...   \n",
       "1        389918         200053           189865  4.549655e+05  ...   \n",
       "2        150172          75068            75104  1.683277e+05  ...   \n",
       "3       8906039        4931802          3974237  9.733517e+06  ...   \n",
       "4        156971          82554            74417  1.800602e+05  ...   \n",
       "\n",
       "   Pop_female_2021  Literacy_7yr_plus  Ranking_by_pop_2011  \\\n",
       "0     1.152165e+05               64.4                   18   \n",
       "1     2.215389e+05               66.4                    8   \n",
       "2     8.418403e+04               49.3                   30   \n",
       "3     4.343491e+06               74.6                    1   \n",
       "4     8.536317e+04               69.7                   27   \n",
       "\n",
       "                    Type  Longitude   Latitude  2021_growth_rate  \\\n",
       "0  Municipal Corporation  91.111674  23.966521          0.183578   \n",
       "1       City Corporation  90.397200  24.761270          0.166823   \n",
       "2  Municipal Corporation  89.944550  24.927120          0.120899   \n",
       "3       City Corporation  90.409879  23.728216          0.092912   \n",
       "4  Municipal Corporation  91.395520  23.006310          0.147092   \n",
       "\n",
       "                              geometry     NN_dist  Unweighted  \n",
       "0  POINT (91.11167399999999 23.966521)   51.070479           1  \n",
       "1             POINT (90.3972 24.76127)   55.394221           1  \n",
       "2   POINT (89.94455000000001 24.92712)   62.479275           1  \n",
       "3          POINT (90.409879 23.728216)   27.815607           1  \n",
       "4   POINT (91.39551999999999 23.00631)  107.214995           1  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check first key of dests dict\n",
    "dests[list(dests.keys())[0]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OD matrix data prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare OD files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# origin to destination\n",
    "orig_raw = gpd.read_file(os.path.join(res_pth,prod_date,'spatial',f'{scenario}_origins_{wp_res}m_TTs.gpkg'),layer=f\"{origins}\",driver='GPKG')\n",
    "orig_raw = optimize(orig_raw)\n",
    "orig_raw = orig_raw.sort_values('O_ID')\n",
    "orig_raw.rename({f'{scenario}_{origins}_DhakaChitt_mins' : f'{scenario}_{origins}_Dhaka_Chitt_mins', \\\n",
    "                 f'{scenario}_{origins}_deep_ports_mins' : f'{scenario}_{origins}_deep_sea_ports_mins'},axis=1,inplace=True)\n",
    "\n",
    "# Summarize origin populations by origin node, to enable appropriate population weighting\n",
    "# population assigned to each origin node\n",
    "\n",
    "if origins == 'PopOrigins':\n",
    "    raw_orig = pd.read_csv(os.path.join(fin_pth,prod_date,f'bgd_wp_{constraint_status}_origins_{wp_res}m_2020_snapped_25m.csv'))\n",
    "    raw_orig.rename({'NN':'O_ID','VALUE':'Pop'},axis=1,inplace=True)\n",
    "    orig_wtd = pd.pivot_table(raw_orig,values='Pop',index='O_ID',aggfunc='sum')\n",
    "    orig_wtd = optimize(orig_wtd)\n",
    "\n",
    "elif origins == 'All_SEZ_Origins':\n",
    "    raw_orig = pd.read_csv(os.path.join(fin_pth,prod_date,f'All_SEZs_{constraint_status}_{wp_res}m_res_25m_simplification_snapped.csv'))\n",
    "    raw_orig.rename({'NN':'O_ID','VALUE':'Pop'},axis=1,inplace=True)\n",
    "    orig_wtd = pd.pivot_table(raw_orig,values='All_SEZ_count',index='O_ID',aggfunc='sum')\n",
    "    orig_wtd = optimize(orig_wtd)\n",
    "    \n",
    "elif origins == 'Active_SEZ_Origins':\n",
    "    raw_orig = pd.read_csv(os.path.join(fin_pth,prod_date,f'Active_SEZs_{constraint_status}_{wp_res}m_res_25m_simplification_snapped.csv'))\n",
    "    raw_orig.rename({'NN':'O_ID','VALUE':'Pop'},axis=1,inplace=True)\n",
    "    orig_wtd = pd.pivot_table(raw_orig,values='Active_SEZ_count',index='O_ID',aggfunc='sum')\n",
    "    orig_wtd = optimize(orig_wtd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pop</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O_ID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>269.673126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1180.427368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>105.337547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9139.359375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>633.812378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Pop\n",
       "O_ID             \n",
       "3      269.673126\n",
       "4     1180.427368\n",
       "5      105.337547\n",
       "6     9139.359375\n",
       "8      633.812378"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_wtd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Populate a dictionary with dests as keys and a list containing the vehicle ODM and a filtered raw origins file as the value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['O_ID', 'Row_ID', 'VALUE', 'Current_PopOrigins_All_cities_mins',\n",
       "       'Current_PopOrigins_Dhaka_Chitt_mins',\n",
       "       'Current_PopOrigins_Minor_cities_mins',\n",
       "       'Current_PopOrigins_Dry_ports_mins',\n",
       "       'Current_PopOrigins_River_ports_mins',\n",
       "       'Current_PopOrigins_Deep_sea_ports_mins',\n",
       "       'Current_PopOrigins_All_SEZs_mins',\n",
       "       'Current_PopOrigins_Active_SEZs_mins', 'adm5_pcode', 'adm4_pcode',\n",
       "       'adm1_pcode', 'adm2_pcode', 'adm3_pcode', 'adm2_pop', 'adm3_pop',\n",
       "       'adm4_pop', 'adm5_pop', 'Current_PopOrigins_All_cities_mins_WT_adm2',\n",
       "       'Current_PopOrigins_All_cities_mins_WT_adm3',\n",
       "       'Current_PopOrigins_All_cities_mins_WT_adm4',\n",
       "       'Current_PopOrigins_All_cities_mins_WT_adm5',\n",
       "       'Current_PopOrigins_Dhaka_Chitt_mins_WT_adm2',\n",
       "       'Current_PopOrigins_Dhaka_Chitt_mins_WT_adm3',\n",
       "       'Current_PopOrigins_Dhaka_Chitt_mins_WT_adm4',\n",
       "       'Current_PopOrigins_Dhaka_Chitt_mins_WT_adm5',\n",
       "       'Current_PopOrigins_Minor_cities_mins_WT_adm2',\n",
       "       'Current_PopOrigins_Minor_cities_mins_WT_adm3',\n",
       "       'Current_PopOrigins_Minor_cities_mins_WT_adm4',\n",
       "       'Current_PopOrigins_Minor_cities_mins_WT_adm5',\n",
       "       'Current_PopOrigins_Dry_ports_mins_WT_adm2',\n",
       "       'Current_PopOrigins_Dry_ports_mins_WT_adm3',\n",
       "       'Current_PopOrigins_Dry_ports_mins_WT_adm4',\n",
       "       'Current_PopOrigins_Dry_ports_mins_WT_adm5',\n",
       "       'Current_PopOrigins_River_ports_mins_WT_adm2',\n",
       "       'Current_PopOrigins_River_ports_mins_WT_adm3',\n",
       "       'Current_PopOrigins_River_ports_mins_WT_adm4',\n",
       "       'Current_PopOrigins_River_ports_mins_WT_adm5',\n",
       "       'Current_PopOrigins_Deep_sea_ports_mins_WT_adm2',\n",
       "       'Current_PopOrigins_Deep_sea_ports_mins_WT_adm3',\n",
       "       'Current_PopOrigins_Deep_sea_ports_mins_WT_adm4',\n",
       "       'Current_PopOrigins_Deep_sea_ports_mins_WT_adm5',\n",
       "       'Current_PopOrigins_All_SEZs_mins_WT_adm2',\n",
       "       'Current_PopOrigins_All_SEZs_mins_WT_adm3',\n",
       "       'Current_PopOrigins_All_SEZs_mins_WT_adm4',\n",
       "       'Current_PopOrigins_All_SEZs_mins_WT_adm5',\n",
       "       'Current_PopOrigins_Active_SEZs_mins_WT_adm2',\n",
       "       'Current_PopOrigins_Active_SEZs_mins_WT_adm3',\n",
       "       'Current_PopOrigins_Active_SEZs_mins_WT_adm4',\n",
       "       'Current_PopOrigins_Active_SEZs_mins_WT_adm5', 'geometry'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "odm_dct= {}\n",
    "\n",
    "for dest, dest_gdf in dests.items():\n",
    "    \n",
    "    # filter origins to just relevant information for this destination\n",
    "    \n",
    "    if origins == 'PopOrigins':\n",
    "        dest_orig = orig_raw[['geometry','O_ID','Row_ID','VALUE',f'{scenario}_{origins}_{dest}_mins','adm2_pcode','adm3_pcode','adm4_pcode','adm5_pcode','adm2_pop','adm3_pop','adm4_pop','adm5_pop']]\n",
    "    else:\n",
    "        dest_orig = orig_raw\n",
    "        \n",
    "    dest_orig = dest_orig.set_index('O_ID')\n",
    "    \n",
    "    # Read in raw OD grid\n",
    "    \n",
    "    dest_odm = pd.read_csv(os.path.join(fin_pth,prod_date,f'OD_matrix_{scenario}_{origins}_to_{dest}_{constraint_status}_{wp_res}m_res_{simplif_meters}m_simplification.csv'))\n",
    "    dest_odm = optimize(dest_odm)\n",
    "    \n",
    "    # Put the origins and destinations in alphanumeric order by node so that they line up and process correctly within the gravity model. Sort by O_IDs, then dest node IDs\n",
    "    # Should work as a function but the function is returning unsorted data -- while raw code works. Curious?\n",
    "    \n",
    "    dest_odm = dest_odm.sort_values('Unnamed: 0').reindex(sorted_nicely(dest_odm.columns), axis=1)\n",
    "\n",
    "    # set the O_ID column back to index so the shape is the same as the dWeight shape\n",
    "    dest_odm.set_index('Unnamed: 0',inplace=True)\n",
    "    \n",
    "    # populate a dictionary to use in gravity modeling\n",
    "    \n",
    "    odm_dct.update({dest:[dest_orig,dest_odm,dest_gdf]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gravity models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GOSTnets.calculate_od_raw as calcOD\n",
    "# calcOD.calculate_gravity??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out weighting options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_decay_dct = {\n",
    "    'd_0.01' : '2.9mins_decay',\n",
    "    'd_0.005' : '5.8mins_decay',\n",
    "    'd_0.001' : '11.5mins_decay',\n",
    "    'd_0.0007701635' :'15mins_decay',\n",
    "    'd_0.0003850818' :'30mins_decay',\n",
    "    'd_0.0001925409' :'60mins_decay',\n",
    "    'd_9.62704e-05' : '120mins_decay',\n",
    "    'd_3.85082e-05' : '300mins_decay',\n",
    "    'd_1e-05' : '1200mins_decay' }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Origins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define your gravity weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "grav_models_Orig_wtless_dct = {\n",
    "    'All_cities' : dict.fromkeys(['Pop_all_2011','Pop_all_2021','Unweighted']),\n",
    "    'Dhaka_Chitt' : dict.fromkeys(['Pop_all_2011','Pop_all_2021','Unweighted']),\n",
    "    'Minor_cities' : dict.fromkeys(['Pop_all_2011','Pop_all_2021','Unweighted']),\n",
    "    'Dry_ports' : dict.fromkeys(['Unweighted']),\n",
    "    'River_ports' : dict.fromkeys(['EXIM','Unweighted']),\n",
    "    'Deep_sea_ports' : dict.fromkeys(['Annual_capacity_TEU','Annual_capacity_TEU_2025','Unweighted']),\n",
    "    'All_SEZs' : dict.fromkeys(['All_SEZ_count']),\n",
    "    'Active_SEZs' : dict.fromkeys(['Active_SEZ_count'])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gravity calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to generate the gravity models and append them to a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orig_grav_loop(orig_file,od_matrix,orig_id_col,orig_weight_col,dest_weight_col):\n",
    "    \n",
    "    orig_gravity = calcOD.calculate_gravity(od = np.array(od_matrix), \\\n",
    "                                            oWeight = orig_weight_col,\\\n",
    "                                            dWeight = dest_weight_col)\n",
    "\n",
    "    orig_gravity['O_ID'] = np.array(orig_id_col) # will assign incorrectly if not converted to an array first\n",
    "    orig_gravity = orig_gravity.set_index('O_ID')\n",
    "    \n",
    "    # merge on indices to speed this step up\n",
    "    orig_gravity_gdf = pd.merge(orig_file, orig_gravity, how='left',left_index=True,right_index=True)\n",
    "\n",
    "    return orig_gravity_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the function to create gravity models for all the desired weights and export them to a geopackage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Active_SEZs': {'Active_SEZ_count': None},\n",
      " 'All_SEZs': {'All_SEZ_count': None},\n",
      " 'All_cities': {'Pop_all_2011': None, 'Pop_all_2021': None, 'Unweighted': None},\n",
      " 'Deep_sea_ports': {'Annual_capacity_TEU': None,\n",
      "                    'Annual_capacity_TEU_2025': None,\n",
      "                    'Unweighted': None},\n",
      " 'Dhaka_Chitt': {'Pop_all_2011': None,\n",
      "                 'Pop_all_2021': None,\n",
      "                 'Unweighted': None},\n",
      " 'Dry_ports': {'Unweighted': None},\n",
      " 'Minor_cities': {'Pop_all_2011': None,\n",
      "                  'Pop_all_2021': None,\n",
      "                  'Unweighted': None},\n",
      " 'River_ports': {'EXIM': None, 'Unweighted': None}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(grav_models_Orig_wtless_dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All_cities\n",
      "Pop_all_2011\n",
      "\n",
      " time elapsed for function\n",
      "22.75905690987905 minutes \n",
      "\n",
      "Pop_all_2021\n",
      "\n",
      " time elapsed for function\n",
      "23.530368304252626 minutes \n",
      "\n",
      "Unweighted\n",
      "\n",
      " time elapsed for function\n",
      "23.82742187579473 minutes \n",
      "\n",
      "Dhaka_Chitt\n",
      "Pop_all_2011\n",
      "\n",
      " time elapsed for function\n",
      "23.817124489943186 minutes \n",
      "\n",
      "Pop_all_2021\n",
      "\n",
      " time elapsed for function\n",
      "24.154891757170358 minutes \n",
      "\n",
      "Unweighted\n",
      "\n",
      " time elapsed for function\n",
      "23.197356200218202 minutes \n",
      "\n",
      "Minor_cities\n",
      "Pop_all_2011\n",
      "\n",
      " time elapsed for function\n",
      "22.830280526479086 minutes \n",
      "\n",
      "Pop_all_2021\n",
      "\n",
      " time elapsed for function\n",
      "22.74421218633652 minutes \n",
      "\n",
      "Unweighted\n",
      "\n",
      " time elapsed for function\n",
      "24.439106325308483 minutes \n",
      "\n",
      "Dry_ports\n",
      "Unweighted\n",
      "\n",
      " time elapsed for function\n",
      "24.71010059515635 minutes \n",
      "\n",
      "River_ports\n",
      "EXIM\n",
      "\n",
      " time elapsed for function\n",
      "23.31399001677831 minutes \n",
      "\n",
      "Unweighted\n",
      "\n",
      " time elapsed for function\n",
      "24.555287166436514 minutes \n",
      "\n",
      "Deep_sea_ports\n",
      "Annual_capacity_TEU\n",
      "\n",
      " time elapsed for function\n",
      "24.80245396296183 minutes \n",
      "\n",
      "Annual_capacity_TEU_2025\n",
      "\n",
      " time elapsed for function\n",
      "23.896518675486245 minutes \n",
      "\n",
      "Unweighted\n",
      "\n",
      " time elapsed for function\n",
      "24.573518006006875 minutes \n",
      "\n",
      "All_SEZs\n",
      "All_SEZ_count\n",
      "\n",
      " time elapsed for function\n",
      "23.506847977638245 minutes \n",
      "\n",
      "Active_SEZs\n",
      "Active_SEZ_count\n",
      "\n",
      " time elapsed for function\n",
      "23.17338855266571 minutes \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dest_key, weight_dct in grav_models_Orig_wtless_dct.items():\n",
    "    \n",
    "    for dest, input_list in odm_dct.items():\n",
    "        \n",
    "        # match on destination names so as to call the right weights\n",
    "        \n",
    "        if dest_key == dest:\n",
    "            \n",
    "            print(dest_key)\n",
    "            \n",
    "            for wt in weight_dct:\n",
    "\n",
    "                print(wt)\n",
    "\n",
    "                # Start timer\n",
    "                func_start = time.time()\n",
    "\n",
    "                # we call in the original destinations file here, referencing the weights column calculated above\n",
    "                df = orig_grav_loop(input_list[0],input_list[1],orig_wtd.index,[1],input_list[2][wt]) \n",
    "\n",
    "                # create geodataframe, rename decay dict\n",
    "                df = gpd.GeoDataFrame(df,geometry='geometry')\n",
    "                df.rename(columns=rename_decay_dct,inplace=True)\n",
    "\n",
    "                # export to geopackage\n",
    "                gpkg_layername = str(dest_key) + '_' + wt # rename as needed\n",
    "                df.to_file(os.path.join(f'results\\\\{prod_date}\\\\spatial\\\\origins_gravity_{prod_date}_{scenario}_{origins}_no_orig_weight.gpkg'),\\\n",
    "                                           layer=gpkg_layername,\\\n",
    "                                           driver=\"GPKG\",\\\n",
    "                                           ignore_errors=True)\n",
    "\n",
    "                # populate nested weight dict with gravity model\n",
    "                grav_models_Orig_wtless_dct[dest_key][wt] = df\n",
    "\n",
    "                # Report time\n",
    "\n",
    "                func_end = time.time()\n",
    "                print('\\n time elapsed for function')\n",
    "                print(str((func_end - func_start) / 60) + ' minutes \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently the gravity statistics are per origin point. Here we aggregate these statistics at the adm4 level, weighting by origin population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grav_models_Orig_wtless_dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All_cities\n",
      "Pop_all_2011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wb467985\\.conda\\envs\\geo\\lib\\site-packages\\geopandas\\geodataframe.py:853: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super(GeoDataFrame, self).__setitem__(key, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " time elapsed for function\n",
      "1.7493743817011516 minutes \n",
      "\n",
      "Pop_all_2021\n",
      "\n",
      " time elapsed for function\n",
      "1.7659995635350545 minutes \n",
      "\n",
      "Unweighted\n",
      "\n",
      " time elapsed for function\n",
      "1.8022161841392517 minutes \n",
      "\n",
      "Dhaka_Chitt\n",
      "Pop_all_2011\n",
      "\n",
      " time elapsed for function\n",
      "1.779349426428477 minutes \n",
      "\n",
      "Pop_all_2021\n",
      "\n",
      " time elapsed for function\n",
      "1.8566164056460062 minutes \n",
      "\n",
      "Unweighted\n",
      "\n",
      " time elapsed for function\n",
      "2.002782408396403 minutes \n",
      "\n",
      "Minor_cities\n",
      "Pop_all_2011\n",
      "\n",
      " time elapsed for function\n",
      "1.969515860080719 minutes \n",
      "\n",
      "Pop_all_2021\n",
      "\n",
      " time elapsed for function\n",
      "1.7474492232004801 minutes \n",
      "\n",
      "Unweighted\n",
      "\n",
      " time elapsed for function\n",
      "1.7091326435407004 minutes \n",
      "\n",
      "Dry_ports\n",
      "Unweighted\n",
      "\n",
      " time elapsed for function\n",
      "1.6135910232861836 minutes \n",
      "\n",
      "River_ports\n",
      "EXIM\n",
      "\n",
      " time elapsed for function\n",
      "1.627599330743154 minutes \n",
      "\n",
      "Unweighted\n",
      "\n",
      " time elapsed for function\n",
      "1.7166908820470175 minutes \n",
      "\n",
      "Deep_sea_ports\n",
      "Annual_capacity_TEU\n",
      "\n",
      " time elapsed for function\n",
      "1.6451241930325826 minutes \n",
      "\n",
      "Annual_capacity_TEU_2025\n",
      "\n",
      " time elapsed for function\n",
      "1.655049188931783 minutes \n",
      "\n",
      "Unweighted\n",
      "\n",
      " time elapsed for function\n",
      "1.6622825662295024 minutes \n",
      "\n",
      "All_SEZs\n",
      "All_SEZ_count\n",
      "\n",
      " time elapsed for function\n",
      "1.6102910161018371 minutes \n",
      "\n",
      "Active_SEZs\n",
      "Active_SEZ_count\n",
      "\n",
      " time elapsed for function\n",
      "1.640747594833374 minutes \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# iterate through the resulting dict of gravity models, joining their results to adm3 and adm4 files for export and visualization\n",
    "\n",
    "adm3_grav = adm3.set_index('adm3_pcode')\n",
    "adm4_grav = adm4.set_index('adm4_pcode')\n",
    "\n",
    "# adm3_grav.set_index('adm3_pcode',inplace=True)\n",
    "# adm4_grav.set_index('adm4_pcode',inplace=True)\n",
    "\n",
    "for key, weight_dct in grav_models_Orig_wtless_dct.items():\n",
    "    \n",
    "    print(key)\n",
    "    \n",
    "    for wt, layer in weight_dct.items():\n",
    "\n",
    "        print(wt)\n",
    "\n",
    "        # Start timer\n",
    "        func_start = time.time()\n",
    "\n",
    "        # Create separate adm3 and adm4 layers\n",
    "\n",
    "        adm3_layer = layer.filter(regex='mins_decay|adm3_pcode|VALUE|adm3_pop')\n",
    "        adm4_layer = layer.filter(regex='mins_decay|adm4_pcode|VALUE|adm4_pop')\n",
    "\n",
    "        # weight index by population\n",
    "        # Note from previous troubleshooting -- perhaps this WITH the OWeight is causing weird patterns\n",
    "\n",
    "        for col in adm3_layer.loc[:,'2.9mins_decay':'1200mins_decay']:\n",
    "            adm3_layer[col] = (adm3_layer[col] * (adm3_layer['VALUE'] / adm3_layer['adm3_pop'])) # weighting by share of overall adm population\n",
    "\n",
    "        for col in adm4_layer.loc[:,'2.9mins_decay':'1200mins_decay']:\n",
    "            adm4_layer[col] = (adm4_layer[col] * (adm4_layer['VALUE'] / adm4_layer['adm4_pop'])) # weighting by share of overall adm population\n",
    "\n",
    "        # aggregate weighted indices by upazila\n",
    "\n",
    "        upz = adm3_layer.filter(regex='mins_decay|adm3_pcode').groupby('adm3_pcode').apply(lambda x: x.sum()) # new version works with non-sequential columns\n",
    "        upz.drop(columns='adm3_pcode',inplace=True)\n",
    "        upz = upz.add_prefix(key + '_' + wt + '_') # if running scenarios, modify by scenario here\n",
    "\n",
    "        # aggregate weighted indices by union\n",
    "\n",
    "        union = adm4_layer.filter(regex='mins_decay|adm4_pcode').groupby('adm4_pcode').apply(lambda x: x.sum()) # new version works with non-sequential columns\n",
    "        union.drop(columns='adm4_pcode',inplace=True)\n",
    "        union = union.add_prefix(key + '_' + wt + '_')\n",
    "\n",
    "        # join to adm4 file\n",
    "\n",
    "        adm3_grav = pd.merge(adm3_grav,upz,how='left',left_index=True,right_index=True)\n",
    "        adm4_grav = pd.merge(adm4_grav,union,how='left',left_index=True,right_index=True) \n",
    "        \n",
    "        # Report time\n",
    "\n",
    "        func_end = time.time()\n",
    "        print('\\n time elapsed for function')\n",
    "        print(str((func_end - func_start) / 60) + ' minutes \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the finished adm3 and adm4 layers for data analysis and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adm3\n",
    "\n",
    "adm3_grav.drop('geometry',axis=1).to_csv(os.path.join(res_pth,prod_date,f'tables\\\\adm3_gravity_models_{prod_date}_{origins}_nopop_wt.csv'))\n",
    "adm3_grav.to_file(os.path.join(res_pth,prod_date,f'spatial\\\\adm3_gravity_models_{scenario}_{prod_date}_{origins}_nopop_wt.geojson'),driver=\"GeoJSON\")\n",
    "adm3_grav.to_file(os.path.join(res_pth,prod_date,f'spatial\\\\adm3_gravity_models_{scenario}_{prod_date}_{origins}_nopop_wt.gpkg'),driver=\"GPKG\", ignore_errors=True)\n",
    "\n",
    "# adm4\n",
    "adm4_grav.drop('geometry',axis=1).to_csv(os.path.join(res_pth,prod_date,f'tables\\\\adm4_gravity_models_{scenario}_{prod_date}_{origins}_nopop_wt.csv'))\n",
    "adm4_grav.to_file(os.path.join(res_pth,prod_date,f'spatial\\\\adm4_gravity_models_{scenario}_{prod_date}_{origins}_nopop_wt.geojson'),driver=\"GeoJSON\")\n",
    "adm4_grav.to_file(os.path.join(res_pth,prod_date,f'spatial\\\\adm4_gravity_models_{scenario}_{prod_date}_{origins}_nopop_wt.gpkg'),driver=\"GPKG\", ignore_errors=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Destinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {scenario}_mkts_odm.transpose()\n",
    "# .iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List previously created weights\n",
    "\n",
    "gm_weights = ['good_jobs_tot_','bad_jobs_tot_','good_jobs_4and5','bad_jobs_4and5',\\\n",
    "              'n_firms_4and5','n_firms_tot','tw_tot','tw_tot_4and5','tw_ser_4and5',\\\n",
    "              'tw_ind_4and5','no_weight']\n",
    "\n",
    "# Insert weights into dict\n",
    "\n",
    "grav_models_dict = dict.fromkeys(gm_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to generate the gravity models per destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dest_grav_loop(orig_file,od_matrix,grav_param,dest_df,orig_weight_df):\n",
    "    dest_gravity = calcOD.calculate_gravity(np.array(od_matrix), oWeight=dest_df[grav_param], dWeight = orig_weight_df['Pop'])\n",
    "    dest_gravity['NN'] = dest_df['NN']\n",
    "    dest_gravity_gdf = pd.merge(mkts, dest_gravity, how='left',on='NN')\n",
    "\n",
    "    return dest_gravity_gdf\n",
    "\n",
    "# OLD\n",
    "\n",
    "# def dest_grav_loop(orig_file,od_matrix,grav_param,oW_df,dW):\n",
    "#     dest_gravity = calcOD.calculate_gravity(np.array(od_matrix), oWeight=oW_df[grav_param], dWeight = dW['Pop'])\n",
    "#     dest_gravity['NN'] = oW_df['NN']\n",
    "#     dest_gravity_gdf = pd.merge(mkts, dest_gravity, how='left',on='NN')\n",
    "\n",
    "#     return dest_gravity_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the function and append results to a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, v in grav_models_dict.items():\n",
    "    \n",
    "    print(key)\n",
    "    \n",
    "    df = dest_grav_loop(current_orig_mkts,current_mkts_odm.transpose(),key,mkts,cur_orig_grp)\n",
    "    \n",
    "    gpkg_layername = 'mkts_cur_' + key\n",
    "    df.to_file(os.path.join('results\\\\spatial\\\\gravity_models_mkts.gpkg'),layer=gpkg_layername,driver=\"GPKG\", ignore_errors=True)\n",
    "    \n",
    "    grav_models_dict[key] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OLD manual process\n",
    "\n",
    "# # sort by O_IDs, then dest node IDs\n",
    "\n",
    "# current_mkts_odm = current_mkts_odm.sort_values('Unnamed: 0').reindex(sorted_nicely(current_mkts_odm.columns), axis=1)\n",
    "# ua_mkts_odm = ua_mkts_odm.sort_values('Unnamed: 0').reindex(sorted_nicely(ua_mkts_odm.columns), axis=1)\n",
    "\n",
    "# # reset O_ID column to the front\n",
    "# current_mkts_odm = current_mkts_odm[ ['Unnamed: 0'] + [ col for col in current_mkts_odm.columns if col != 'Unnamed: 0' ] ]\n",
    "# ua_mkts_odm = ua_mkts_odm[ ['Unnamed: 0'] + [ col for col in ua_mkts_odm.columns if col != 'Unnamed: 0' ] ]\n",
    "\n",
    "# # set the Dest_ID column back to index so the shape is the same as the dWeight shape\n",
    "# current_mkts_odm.set_index('Unnamed: 0',inplace=True)\n",
    "# ua_mkts_odm.set_index('Unnamed: 0',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Georeference, spatial join files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wb467985\\.conda\\envs\\geo\\lib\\site-packages\\geopandas\\tools\\sjoin.py:61: UserWarning: CRS of frames being joined does not match!(None != epsg:4326)\n",
      "  \"(%s != %s)\" % (left_df.crs, right_df.crs)\n"
     ]
    }
   ],
   "source": [
    "# # Old manual routine \n",
    "\n",
    "# current_orig_mkts['geometry'] = current_orig_mkts['geometry'].apply(wkt.loads)\n",
    "# current_orig_mkts = gpd.GeoDataFrame(current_orig_mkts,geometry='geometry')\n",
    "# current_orig_mkts = gpd.sjoin(current_orig_mkts,adm4[['geometry','adm3_en','adm4_en','adm3_pcode','adm4_pcode']],how='left',op='within')\n",
    "\n",
    "# ua_orig_mkts['geometry'] = ua_orig_mkts['geometry'].apply(wkt.loads)\n",
    "# ua_orig_mkts = gpd.GeoDataFrame(ua_orig_mkts,geometry='geometry')\n",
    "# ua_orig_mkts = gpd.sjoin(ua_orig_mkts,adm4[['geometry','adm3_en','adm4_en','adm3_pcode','adm4_pcode']],how='left',op='within')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scenario based approach to gravity stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now repeat the process for the all updated roads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, v in grav_models_dict.items():\n",
    "    \n",
    "#     print(key)\n",
    "    \n",
    "#     df = orig_grav_loop(ua_orig_mkts,ua_mkts_odm,ua_orig_grp,mkts[key])\n",
    "    \n",
    "#     gpkg_layername = 'mkts_ua_' + key\n",
    "#     df.to_file(os.path.join('results\\\\spatial\\\\gravity_models.gpkg'),layer=gpkg_layername,driver=\"GPKG\", ignore_errors=True)\n",
    "    \n",
    "#     grav_models_dict[key] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # iterate through the dict\n",
    "\n",
    "# # adm4_grav = adm4_grav\n",
    "\n",
    "# for key, layer in grav_models_dict.items():\n",
    "    \n",
    "#     print(key)\n",
    "    \n",
    "#     # weight index by population\n",
    "    \n",
    "#     layer['adm4_pop'] = layer.groupby('adm4_pcode')['VALUE'].transform(np.sum)\n",
    "    \n",
    "#     for col in layer.loc[:,'d_0.01':'d_1e-05']:\n",
    "#         layer[col] = (layer[col] * (layer['VALUE'] / layer['adm4_pop']))\n",
    "    \n",
    "# #     [ col * (layer['VALUE'].astype(float) / layer['adm4_pop'].astype(float)) for col in layer.loc[:,'d_0.01':'d_1e-05']] # failed atempt at list comprehension\n",
    "    \n",
    "#     # aggregate weighted indices by union\n",
    "    \n",
    "#     union = layer.loc[:,'adm4_pcode':'d_1e-05'].groupby('adm4_pcode').apply(lambda x: x.sum())\n",
    "#     union.drop(columns='adm4_pcode',inplace=True)\n",
    "#     union = union.add_prefix(key + '_ua_')\n",
    "#     union.reset_index(inplace=True)\n",
    "#     union['adm4_pcode'] = union['adm4_pcode'].astype(str)\n",
    "\n",
    "#     # join to adm4 file\n",
    "\n",
    "#     adm4_grav = pd.merge(adm4_grav,union,how='left',on=['adm4_pcode'])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If doing a scenario, calculate changes in gravity resulting from that scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adm4_grav['delta60_gj_tot'] = adm4_grav['good_jobs_tot__ua_d_0.0001925409'] - adm4_grav['good_jobs_tot__cur_d_0.0001925409']\n",
    "# adm4_grav['delta60_bj_tot'] = adm4_grav['bad_jobs_tot__ua_d_0.0001925409'] - adm4_grav['bad_jobs_tot__cur_d_0.0001925409']\n",
    "# adm4_grav['delta30_bj_tot'] = adm4_grav['bad_jobs_tot__ua_d_0.0003850818'] - adm4_grav['bad_jobs_tot__cur_d_0.0003850818']\n",
    "\n",
    "# adm4_grav['delta60_gj_4and5'] = adm4_grav['good_jobs_4and5_ua_d_0.0001925409'] - adm4_grav['good_jobs_4and5_cur_d_0.0001925409']\n",
    "# adm4_grav['delta60_bj_4and5'] = adm4_grav['bad_jobs_4and5_ua_d_0.0001925409'] - adm4_grav['bad_jobs_4and5_cur_d_0.0001925409']\n",
    "\n",
    "# adm4_grav['delta60_tw_tot'] = adm4_grav['tw_tot_ua_d_0.0001925409'] - adm4_grav['tw_tot_cur_d_0.0001925409']\n",
    "# adm4_grav['delta60_nf_tot'] = adm4_grav['n_firms_tot_ua_d_0.0001925409'] - adm4_grav['n_firms_tot_cur_d_0.0001925409']\n",
    "\n",
    "# adm4_grav['delta60_tw_4and5'] = adm4_grav['tw_tot_4and5_ua_d_0.0001925409'] - adm4_grav['tw_tot_4and5_cur_d_0.0001925409']\n",
    "# adm4_grav['delta60_nf_4and5'] = adm4_grav['n_firms_4and5_ua_d_0.0001925409'] - adm4_grav['n_firms_4and5_cur_d_0.0001925409']\n",
    "\n",
    "# adm4_grav['delta60_tw_ser4and5'] = adm4_grav['tw_ser_4and5_ua_d_0.0001925409'] - adm4_grav['tw_ser_4and5_cur_d_0.0001925409']\n",
    "# adm4_grav['delta60_tw_ind4and5'] = adm4_grav['tw_ind_4and5_ua_d_0.0001925409'] - adm4_grav['tw_ind_4and5_cur_d_0.0001925409']\n",
    "\n",
    "# adm4_grav['delta60_now'] = adm4_grav['no_weight_ua_d_0.0001925409'] - adm4_grav['no_weight_cur_d_0.0001925409']\n",
    "# adm4_grav['delta30_now'] = adm4_grav['no_weight_ua_d_0.0003850818'] - adm4_grav['no_weight_cur_d_0.0003850818']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
