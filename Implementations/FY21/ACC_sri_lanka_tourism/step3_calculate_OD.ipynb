{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Calculate OD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook takes into the processed graph with assigned speeds from Step 2. It will also take origins and destinations as inputs. It will assign groups of destinations to each origin, and create an Origin-Destination (OD) matrix for each group. From these OD matrices it will generate the shortest paths as well as tables of edges that make up the shortest paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time, importlib\n",
    "import osmnx\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "sys.path.append(\"../../../GOSTnets\")\n",
    "import GOSTnets as gn\n",
    "\n",
    "from shapely.geometry import LineString, Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a Jupyter Notebook extension which reloads all of the modules whenever you run the code\n",
    "# This is optional but good if you are modifying and testing source code\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GOSTnets.load_traffic2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read graph\n",
    "G = nx.read_gpickle('../mapbox_traffic/sri_lanka_clean_w_time_largest_max_speeds.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(G.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gn.example_edge(G, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load origins\n",
    "Depending on your analysis, your would load either your airports as origins, or your cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#origins = gpd.read_file('./origins_destinations/intl_airport_updated.shp')\n",
    "origins = gpd.read_file('../mapbox_traffic/origins_destinations/cities_top10_32644.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load destinations\n",
    "Depending on your analysis, your would load either your cities as destinations, or your tourist points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#destinations = gpd.read_file('./origins_destinations/cities_top10.shp')\n",
    "destinations = gpd.read_file('../mapbox_traffic/origins_destinations/tourism_on_land_32644.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snap your origins and destinations to the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origins_gdf = gn.pandana_snap_c(G, origins, source_crs = 'epsg:32644', target_crs = 'epsg:32644')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#origins_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origins_list = list(set(origins_gdf.NN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destinations_gdf = gn.pandana_snap_c(G, destinations, source_crs = 'epsg:32644', target_crs = 'epsg:32644')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#destinations_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destinations_list = list(set(destinations_gdf.NN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#destinations_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate OD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It will use the default weight of 'time'\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "OD = gn.calculate_OD(G, origins_list, destinations_list, fail_value = 9999999)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OD_df = pd.DataFrame(OD, index = origins_list, columns = destinations_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OD_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OD_df.min(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we need to find the nearest destination for each origin point, and based on this assign a group of origin points to each of the destinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes the min index value of each column, then groups by origin (first index (0)) and takes the first entry\n",
    "groupby_obj = OD_df.idxmin(axis=0).to_frame(0).groupby(0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize groupby_obj\n",
    "#groupby_obj.apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(groupby_obj.apply(list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a nice way to visualize the groupby_obj\n",
    "#groupby_obj.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a dictionary that associates assigned origin points with each destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_destination_pt_dict = {}\n",
    "for name, group in groupby_obj:\n",
    "    #print(group)\n",
    "    for items in group.iteritems(): \n",
    "        #print(items[1])\n",
    "        if items[1] not in origin_destination_pt_dict:\n",
    "            origin_destination_pt_dict[items[1]] = [items[0]]\n",
    "        else:\n",
    "            #append value to list in dict value\n",
    "            origin_destination_pt_dict[items[1]].append(items[0])\n",
    "    #print(type(group))\n",
    "    #print(group.head(1))\n",
    "\n",
    "    #print(name)\n",
    "    #print(city_tourist_pt_dict[group])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_destination_pt_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop through dictionary in order to do a calculate_OD for each destination's nearest origins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OD = {}\n",
    "OD_df_dict = {}\n",
    "for origin, destination in origin_destination_pt_dict.items():\n",
    "    OD[origin] = gn.calculate_OD(G, [origin], destination, fail_value = 9999999)\n",
    "    OD_df_dict[origin] = pd.DataFrame(OD[origin], index = [origin], columns = destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OD_df_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now work on generating routes and visualizing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.ops import linemerge\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### speed dictionaries used to model bringing an improved highway to an average speed. Our default methodology is to import the graph where edges that did not have a traffic speed used the OSM Max speeds as their default speeds and for the tabulate_edges function to apply the mapbox_mean_speeds dictionary to apply to for all edges that have traffic to compare their mapbox traffic speed to the Mapbox mean speeds per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_speeds = {\n",
    "    'secondary': 50,\n",
    "    'secondary_link': 45,\n",
    "    'tertiary': 40,\n",
    "    'tertiary_link': 40,\n",
    "    'residential': 25,\n",
    "    'unclassified': 25,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapbox_mean_speeds = {\n",
    "    'secondary': 34,\n",
    "    'secondary_link': 9,\n",
    "    'tertiary': 25,\n",
    "    'tertiary_link': 13,\n",
    "    'residential': 20,\n",
    "    'unclassified': 20,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The tabulate_edges function will get called by the generate_complete_edges_and_routes for each route. The tabulate edges function loops through each segment of the route and calculates various metrics into both an edge list and also for each route. One of these metrics is calculating the seconds saved if a road segment could be improved. It calculates this potential improvement using the input mean speed dictionary. If the road segment's speed could be improved to the mean speed then it will calculate a positive seconds saved value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabulate_edges(route, mean_speed_dict = mapbox_mean_speeds):\n",
    "    edge_table = []\n",
    "    route_geometry = LineString()\n",
    "    improved_time = 0\n",
    "    for idx in range(0, len(route) - 1):\n",
    "        # look up line\n",
    "        #print('to node')\n",
    "        #print(route[idx])\n",
    "        #print('from node')\n",
    "        #print(route[idx+1])\n",
    "        \n",
    "        edge_geometry = G.get_edge_data(route[idx],route[idx+1])[0]['geometry']\n",
    "        \n",
    "        # get edge speed\n",
    "        edge_speed = G.get_edge_data(route[idx],route[idx+1])[0]['speed']\n",
    "        #print('print edge_speed')\n",
    "        #print(edge_speed)\n",
    "        \n",
    "        # compare edge speed to median speed\n",
    "        rural_roads_list = ['residential','secondary','secondary_link','tertiary','tertiary_link','unclassified']\n",
    "        \n",
    "        edge_infra_type = G.get_edge_data(route[idx],route[idx+1])[0]['infra_type']\n",
    "        edge_length = G.get_edge_data(route[idx],route[idx+1])[0]['length']\n",
    "        edge_time = G.get_edge_data(route[idx],route[idx+1])[0]['time']\n",
    "        \n",
    "        try:\n",
    "            mean_speed = G.get_edge_data(route[idx],route[idx+1])[0]['traffic_mean_speed']\n",
    "        except:\n",
    "            mean_speed = 0\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            edge_imp_cost = G.get_edge_data(route[idx],route[idx+1])[0]['imp_cost']\n",
    "        except:\n",
    "            edge_imp_cost = 0\n",
    "            pass\n",
    "        \n",
    "        new_time_s = None\n",
    "        \n",
    "        if mean_speed > 0:      \n",
    "            if edge_infra_type in rural_roads_list:\n",
    "                #print('print edge attributes')\n",
    "                #print(G.get_edge_data(route[idx],route[idx+1])[0])\n",
    "\n",
    "                #assumes that current edge length is in km\n",
    "                #use either the max_speeds dictionary or the mapbox_mean_speeds dictionary here\n",
    "                new_time_s = (edge_length / mean_speed_dict[edge_infra_type]) * 3600\n",
    "\n",
    "                edge_savings = edge_time - new_time_s\n",
    "                # assign savings time\n",
    "                edge_table.append([route[idx], route[idx+1], edge_savings, edge_imp_cost, edge_length,edge_time, edge_infra_type, mean_speed, edge_geometry])\n",
    "            else:\n",
    "                # sec_saved and improvement costs become 0\n",
    "                edge_table.append([route[idx], route[idx+1], 0, 0, edge_length, edge_time, edge_infra_type, mean_speed, edge_geometry])\n",
    "        else:\n",
    "            # sec_saved and improvement costs become 0\n",
    "            edge_table.append([route[idx], route[idx+1], 0, 0, edge_length, edge_time, edge_infra_type, mean_speed, edge_geometry])\n",
    "            \n",
    "        route_geometry = route_geometry.union(edge_geometry)\n",
    "        \n",
    "        # here if the road can be improved, we are using the improved time, or else we are using the time to traverse the edge\n",
    "        # we are summing this up for each route\n",
    "        if new_time_s:\n",
    "            improved_time += new_time_s\n",
    "        else:\n",
    "            improved_time += edge_time\n",
    "        \n",
    "    #print('print route_geometry')\n",
    "    #print(route_geometry)\n",
    "    \n",
    "    return(edge_table, route_geometry, improved_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The generate_complete_edges_and_routes function loops through each origin and destination and gets the complete shortest path, this means it gets each intermediary stop along the shortest path. This allows us to process each edge along each shortest path. The function returns a GeoDataFrame that contains each along each shortest path. It is possible that an edge can be traversed more than once if more than one shortest path traverses the same edge. In this case the edge becomes weighted and its 'weighted_sec_saved' value is multiplied for each time it is traversed. The generate_complete_edges_and_routes function also returns a GeoDataFrame of all of the shortest paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_complete_edges_and_routes(input_df, mean_speed_dict = mapbox_mean_speeds):\n",
    "\n",
    "    LIMIT = 9999999999\n",
    "\n",
    "    complete_edges = []\n",
    "    complete_routes = []\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    # for origin, row in sample_df.iterrows(): \n",
    "    for origin, row in islice(input_df.iterrows(), LIMIT):    \n",
    "        for destination, value in islice(row.items(), LIMIT):\n",
    "            try:\n",
    "                origin = int(origin)\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                destination = int(destination)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            count = count + 1\n",
    "\n",
    "            route = nx.dijkstra_path(G, origin, destination, weight = 'time')\n",
    "\n",
    "            edge_table, route_geometry, improved_time = tabulate_edges(route, mean_speed_dict = mean_speed_dict)\n",
    "            #print('print edge_table:')\n",
    "            #print(edge_table)\n",
    "            complete_edges = complete_edges + edge_table\n",
    "\n",
    "            complete_routes.append([edge_table[0][0], edge_table[-1][1], value, improved_time, route_geometry])\n",
    "            #print('edge_table[:-1]')\n",
    "            #print(edge_table[-1][1])\n",
    "            \n",
    "    # convert complete_edges to gdf\n",
    "    complete = pd.DataFrame(complete_edges, columns = ['o', 'd', 'sec_saved', 'imp_cost', 'length', 'time', 'infra_type', 'mean_speed', 'geometry'])\n",
    "    complete['w'] = 1\n",
    "    complete_count = complete.groupby(['o','d']).agg(\n",
    "        {\n",
    "            'w':\"count\",\n",
    "            'sec_saved': 'first',\n",
    "            'imp_cost': 'first',\n",
    "            'mean_speed': 'first',\n",
    "            'length':'first',\n",
    "            'time':'first',\n",
    "            'infra_type':'first',\n",
    "            'geometry':'first'\n",
    "        }\n",
    "    )\n",
    "    complete_count.reset_index(inplace = True)\n",
    "    complete_count['o'] = complete_count['o'].astype(str)\n",
    "    complete_count['d'] = complete_count['d'].astype(str)\n",
    "    complete_count['weighted_sec_saved'] = complete_count.w * complete_count.sec_saved\n",
    "    complete_count.sort_values(by=['weighted_sec_saved'], ascending=False)\n",
    "    complete_count_gdf = gpd.GeoDataFrame(complete_count, crs = 'epsg:4326')\n",
    "    \n",
    "    # convert complete_routes to gdf\n",
    "    complete_routes_df = pd.DataFrame(complete_routes, columns = ['origin','destination','time','improved_time','geometry'])\n",
    "    complete_routes_gdf = gpd.GeoDataFrame(complete_routes_df, crs = 'epsg:4326')\n",
    "        \n",
    "    return [complete_count_gdf, complete_routes_gdf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will be calculating metrics for each origin and its group of destinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OD_df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in OD_df_dict:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#results = generate_complete_edges_and_routes(OD_df_dict[3935302581])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we loop through each origin and its group of destinations and run the generate_complete_edges_and_routes using the appropriate speed dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "results = {}\n",
    "\n",
    "count = 0\n",
    "for key in OD_df_dict:\n",
    "    results[key] = generate_complete_edges_and_routes(OD_df_dict[key], mean_speed_dict = mapbox_mean_speeds)\n",
    "    count += 1\n",
    "    \n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results into shapefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in results:\n",
    "    # print edges\n",
    "    #print(results[key][0])\n",
    "    file_name = f\"./output_edges/cities_weighted_sec_saved_edges_{key}.shp\"\n",
    "    #print(file_name)\n",
    "    results[key][0].to_file(driver = 'ESRI Shapefile', filename = file_name)\n",
    "    routes_file_name = f\"./output_routes/cities_weighted_sec_saved_routes_{key}.shp\"\n",
    "    #print(file_name)\n",
    "    results[key][1].to_file(driver = 'ESRI Shapefile', filename = routes_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the saved shapefiles for the edges back in as one GeoDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "path = r'./output_edges' # use your path\n",
    "all_files = glob.glob(path + \"/*.shp\")\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    gdf = gpd.read_file(filename)\n",
    "    li.append(gdf)\n",
    "\n",
    "study_area = gpd.GeoDataFrame(pd.concat( li, ignore_index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_area.crs = 'epsg:32644'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the saved shapefiles for the Routes back in as one GeoDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'./output_routes' # use your path\n",
    "all_files = glob.glob(path + \"/*.shp\")\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    gdf = gpd.read_file(filename)\n",
    "    li.append(gdf)\n",
    "\n",
    "study_area_routes = gpd.GeoDataFrame(pd.concat( li, ignore_index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_area_routes.crs = 'epsg:32644'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save merged routes\n",
    "study_area_routes.to_file(driver = 'ESRI Shapefile', filename = \"./output_merged/post_step3_merged_results_routes.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The edges have varying lengths, so we can divide the weighted seconds by length (in km), in order to compare them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_area['w_sec_per_km'] = study_area['weighted_s'] / study_area['length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_area[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_study_area = study_area.sort_values(by=['w_sec_per_km'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's also add the time in min\n",
    "sorted_study_area['time_min'] = sorted_study_area['time'] / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_study_area[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the merged edges into a shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_study_area.to_file(driver = 'ESRI Shapefile', filename = \"./output_merged/post_step3_merged_results_edges.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can generate some other metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a subset for all edges that have a positive seconds saved if improved\n",
    "positive_subset = sorted_study_area.loc[sorted_study_area['sec_saved'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_subset['sec_saved'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_subset['length'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_subset['imp_cost'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a subset for all edges that would save at least 20 'weighted' seconds if improved\n",
    "w_sec_per_km_greater_than_20 = sorted_study_area.loc[sorted_study_area['w_sec_per_km'] > 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_sec_per_km_greater_than_20['sec_saved'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_sec_per_km_greater_than_20['length'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_sec_per_km_greater_than_20['imp_cost'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
