{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library installation and script setup\n",
    "This box needs to only be run once. It builds the environment to carry out the rest of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run one time only - install pip and unusual Libraries\n",
    "import pip\n",
    "import time\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "from shapely.wkt import loads\n",
    "from shapely.geometry import MultiPolygon, MultiPoint, Polygon, box\n",
    "from shapely.ops import cascaded_union, unary_union\n",
    "from shapely.ops import nearest_points\n",
    "import time\n",
    "import json\n",
    "from gbdxtools import Interface\n",
    "from gbdxtools.task import env\n",
    "from gbdxtools import CatalogImage\n",
    "import sys, os\n",
    "gbdx = Interface()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplify Complex Clustered Polygon Objects\n",
    "\n",
    "This section aims to import the AOIs as described by raw shapefiles, and draw sensible bounding boxes around them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "shps = []\n",
    "pth = r'C:\\Users\\charl\\Documents\\GOST\\LAC floods\\RE__Fathom_global_flood_data_use'\n",
    "for f in ['RC_CABA.shp','RC_Cordoba_Capital.shp','RC_Jujuy_Capital.shp','RC_RegionMetropolitanaBA.shp','RC_Resistencia.shp','RC_SantaFe_Capital.shp']:\n",
    "    gdf = gpd.read_file(os.path.join(pth,f))\n",
    "    shp = unary_union(gdf.geometry)\n",
    "    shps.append(shp)\n",
    "shape = gpd.GeoDataFrame({'geometry':shps}, geometry = 'geometry', crs = {'init':'epsg:4326'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buffer width set as 0.000000\n",
      "useful area of tight bbox: 68 percent\n",
      "useful area of reduced bbox: 68 percent\n",
      "useful area of final bbox: 68 percent\n",
      "number of AOIs: 6\n"
     ]
    }
   ],
   "source": [
    "### rawAOI = r'agebs_val_muni.shp'\n",
    "crs = {'init': 'epsg:4326'}\n",
    "bufw = 0.015\n",
    "\n",
    "# Define conversion function - objects to list of bounding boxes\n",
    "def BoundingBoxList(MultiPolygonObj):\n",
    "    boxlist = []\n",
    "    for obj in MultiPolygonObj:\n",
    "        coords = [n for n in obj.bounds]\n",
    "        bbox = box(coords[0],coords[1],coords[2],coords[3])\n",
    "        boxlist.append(bbox)\n",
    "    return boxlist\n",
    "\n",
    "polygons = MultiPolygon(shape['geometry'].loc[i] for i in shape.index)\n",
    "\n",
    "exterior = cascaded_union(polygons)\n",
    "exterior_boxxs = BoundingBoxList(exterior)\n",
    "\n",
    "# Scientific Buffer Setting based on nearest neighbour median\n",
    "dff = pd.DataFrame({'exterior': exterior})\n",
    "dff['ext.centroid'] = dff['exterior'].apply(lambda x: x.centroid)\n",
    "\n",
    "def func(x):\n",
    "    m = dff.loc[dff['ext.centroid'] != x]\n",
    "    l = MultiPoint(m['ext.centroid'].tolist())\n",
    "    n = nearest_points(x, l)\n",
    "    return x.distance(n[0])\n",
    "    i += 1\n",
    "    \n",
    "dff['nn_distance'] = dff['ext.centroid'].apply(lambda x: func(x))\n",
    "bufw = dff['nn_distance'].median()\n",
    "print 'Buffer width set as %f' % bufw\n",
    "\n",
    "# Group nearby AOIs\n",
    "tight_bbox = MultiPolygon(exterior_boxxs)\n",
    "reduced_boxes = cascaded_union(tight_bbox.buffer(bufw))\n",
    "\n",
    "rboxxs = BoundingBoxList(reduced_boxes)\n",
    "final_boxes = cascaded_union(MultiPolygon(rboxxs))\n",
    "fboxxs = BoundingBoxList(final_boxes.buffer(-bufw))\n",
    "\n",
    "pd.DataFrame({'AOI_geometry':fboxxs}).to_csv(os.path.join(pth, 'AOI_Collection.csv'))\n",
    "\n",
    "print 'useful area of tight bbox: %d percent' % (exterior.area / tight_bbox.area * 100)\n",
    "print 'useful area of reduced bbox: %d percent' % (exterior.area / reduced_boxes.area * 100)\n",
    "print 'useful area of final bbox: %d percent' % (exterior.area / final_boxes.area * 100)\n",
    "print 'number of AOIs: %d' % len(fboxxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Search Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categorical search parameters\n",
    "cutoff_cloud_cover = 25   # images with CC over this threshold discarded\n",
    "cutoff_overlap = 0     # images with AOI overlap below this threshold discarded. [N.b.: keep small if AOI large.]\n",
    "cutoff_date_low = '1-Jan-13'  # images older than this date discarded\n",
    "cutoff_date_high = '1-Jan-16' # images newer than this date discarded\n",
    "cutoff_nadir = 25 # Images at nadir angles greater than threshold discarded\n",
    "cutoff_pan_res = 1 # Images below this resolution discarded\n",
    "accepted_bands = ['PAN_MS1','PAN_MS1_MS2'] #  Images with any other band entry discarded\n",
    "\n",
    "\n",
    "# Define continuous image ranking preferences\n",
    "optimal_date =  '1-Jul-14' # Optimal date (enter as dd-mmm-yy)\n",
    "optimal_pan_res = 0.4 # Optimal pan resolution, metres\n",
    "optimal_nadir = 0 # optimal image angle. 0 = vertical\n",
    "\n",
    "# Define continuous image ranking preference weights. Must sum to 1.\n",
    "# If user cares more about scenes being contemporaneous, up 'date' weighting at expense of other categories. \n",
    "pref_weights = {\n",
    "    'cloud_cover': 0.4,\n",
    "    'overlap':0.25,\n",
    "    'date': 0.25,\n",
    "    'nadir': 0.1,\n",
    "    'resolution': 0.0\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Charles Rocks II Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def Process(AOI, \n",
    "            cutoff_cloud_cover, \n",
    "            cutoff_overlap, \n",
    "            cutoff_date_low,\n",
    "            cutoff_date_high,\n",
    "            cutoff_nadir, \n",
    "            cutoff_pan_res, \n",
    "            accepted_bands, \n",
    "            optimal_date, \n",
    "            optimal_pan_res, \n",
    "            optimal_nadir, \n",
    "            pref_weights, \n",
    "            AOI_counter\n",
    "           ):\n",
    "    \n",
    "    # Define bbox object\n",
    "    bbox = [AOI.bounds[i] for i in range(0,len(AOI.bounds))]\n",
    "    \n",
    "    # Define search function. Returns up to 1000 images where cloud cover smaller than 25%\n",
    "    def search_unordered(bbox, _type, count=1000, cloud_cover=25):\n",
    "        aoi = AOI.wkt\n",
    "        query = \"item_type:{} AND item_type:DigitalGlobeAcquisition\".format(_type)\n",
    "        query += \" AND attributes.cloudCover_int:<{}\".format(cloud_cover)\n",
    "        return gbdx.vectors.query(aoi, query, count=count)\n",
    "\n",
    "    # Run search on Area of Interest (AOI). Passes in AOI in Well Known Text format (wkt)\n",
    "    records = search_unordered(AOI.wkt, 'DigitalGlobeAcquisition')\n",
    "\n",
    "    # Create list object of all catalog IDs returned in search\n",
    "    ids = [r['properties']['attributes']['catalogID'] for r in records]\n",
    "\n",
    "    # Define Counters\n",
    "    l = 0    # number of non-IDAHO images\n",
    "    scenes = [] # list containing metadata dictionaries of all scenes in our AOI \n",
    "\n",
    "    # Toggle for printing images to screen\n",
    "    download_thumbnails = 0\n",
    "\n",
    "    # Loop catalog IDs\n",
    "    for i in ids:\n",
    "\n",
    "        # Fetch metadata dictionary for each catalog ID in ids list\n",
    "        r = gbdx.catalog.get(i)\n",
    "\n",
    "        # Check location of ID - is it in IDAHO?\n",
    "        try:\n",
    "            location = gbdx.catalog.get_data_location(i)\n",
    "        except: \n",
    "            location == 'not_delivered'\n",
    "        \n",
    "        # Defines IDAHO variable as binary 1 / 0 depending on whether it is in IDAHO already or not\n",
    "        if location == 'not_delivered':\n",
    "            l = l + 1\n",
    "            idaho = 0\n",
    "        else:\n",
    "            idaho = 1\n",
    "\n",
    "            # Download image if image in IDAHO and toggle on\n",
    "            if download_thumbnails == 1:\n",
    "                image = CatalogImage(i, band_type=\"MS\", bbox=bboxx)\n",
    "                image.plot(w=10, h=10)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        # Calculate the percentage overlap with our AOI for each scene\n",
    "        # load as a Shapely object the wkt representation of the scene footprint\n",
    "        footprint = r['properties']['footprintWkt']\n",
    "        shapely_footprint = shapely.wkt.loads(footprint)\n",
    "\n",
    "        # Calculate the object that represents the difference between the AOI and the scene footprint \n",
    "        AA = AOI.difference(shapely_footprint)\n",
    "\n",
    "        # Define frac as the fraction, between 0 and 1, of the AOI that the scene covers\n",
    "        frac = 1 - ((AA).area / AOI.area)\n",
    "\n",
    "        # Create BB - the proxy for the useful area. IF scene entirely contains AOI, then BB = AOI, else it is the intersection \n",
    "        # of the scene footprint and the AOI\n",
    "        BB = AOI \n",
    "        if frac < 1:\n",
    "            BB = AOI - AA\n",
    "    \n",
    "        # Similarly, AA, the difference area between AOI and the scene, can be set to null if the scene contains 100% of the AOI \n",
    "        if frac == 1:\n",
    "            AA = \"\"\n",
    "\n",
    "        # Append key metadata to list obejct 'scenes' for the current scene, as a dictionary. This then moves into the pandas dataframe.\n",
    "        # Several objects here are from DigitalGlobe's metadata dictionary (anything with an r start)\n",
    "        scenes.append({\n",
    "            'ID':i, \n",
    "            'TimeStamp':r['properties']['timestamp'],\n",
    "            'CloudCover':r['properties']['cloudCover'],\n",
    "            'ImageBands':r['properties']['imageBands'],\n",
    "            'On_IDAHO':idaho,\n",
    "            'browseURL': r['properties']['browseURL'],\n",
    "            'Overlap_%': frac * 100,\n",
    "            'PanResolution': r['properties']['panResolution'],\n",
    "            'MultiResolution': r['properties']['multiResolution'],\n",
    "            'OffNadirAngle': r['properties']['offNadirAngle'],\n",
    "            'Sensor':r['properties']['sensorPlatformName'],\n",
    "            'Full_scene_WKT':r['properties']['footprintWkt'],\n",
    "            'missing_area_WKT':AA,\n",
    "            'useful_area_WKT':BB\n",
    "            })\n",
    "\n",
    "    # Define column order for dataframe of search results\n",
    "    cols = ['ID','Sensor','ImageBands','TimeStamp','CloudCover','Overlap_%','PanResolution','MultiResolution','OffNadirAngle','On_IDAHO','browseURL','Full_scene_WKT','useful_area_WKT','missing_area_WKT']\n",
    "\n",
    "    #Generate pandas dataframe from results\n",
    "    out = pd.DataFrame(scenes,columns = cols)\n",
    "    \n",
    "    # Convert Timestamp field to pandas DateTime object\n",
    "    out['TS'] = out['TimeStamp'].apply(lambda x: pd.Timestamp(x))\n",
    "\n",
    "    # Add separate date and time columns for easy interpretation\n",
    "    string = out['TimeStamp'].str.split('T')\n",
    "    out['Date'] = string.str.get(0)\n",
    "    out['Time'] = string.str.get(1)\n",
    "\n",
    "    # Categorical Search: remove disqualified images. Copy of dataframe taken, renamed to 'out_1stcut'.\n",
    "    out_1stcut = out.loc[(out['CloudCover'] <= cutoff_cloud_cover) & \n",
    "                         (out['Overlap_%'] >= cutoff_overlap) & \n",
    "                         (out['TS'] > pd.Timestamp(cutoff_date_low)) & \n",
    "                         (out['TS'] < pd.Timestamp(cutoff_date_high)) & \n",
    "                         (out['ImageBands'].isin(accepted_bands)) & \n",
    "                         (out['OffNadirAngle'] <= cutoff_nadir) & \n",
    "                         (out['PanResolution'] <= cutoff_pan_res)\n",
    "                        ]\n",
    "\n",
    "    # Apply ranking method over all non-disqualified search results for each field\n",
    "    optimal_date = pd.to_datetime(optimal_date, utc = True)\n",
    "\n",
    "    # each 1% of cloud cover = 1 point\n",
    "    out_1stcut['points_CC'] = (out_1stcut['CloudCover'])  \n",
    "\n",
    "    # each 1% of overlap missed = 1 point\n",
    "    out_1stcut['points_Overlap'] = (100 - out_1stcut['Overlap_%'])  \n",
    "\n",
    "    # each week away from the optimal date = 1 point \n",
    "    out_1stcut['points_Date'] = ((abs(out_1stcut['TS'] - optimal_date)).view('int64') / 60 / 60 / 24 / 1E9) / 7 \n",
    "\n",
    "    # each degree off nadir = 1 point\n",
    "    out_1stcut['points_Nadir'] = abs(out_1stcut['OffNadirAngle'] - optimal_nadir) \n",
    "\n",
    "    # each cm of resolution worse than the optimal resolution = 1 point\n",
    "    out_1stcut['points_Res'] = (out_1stcut['PanResolution'] - optimal_pan_res).apply(lambda x: max(x,0)) * 100 \n",
    "\n",
    "    # Define ranking algorithm - weight point components defined above by the preference weighting dictionary\n",
    "    def Ranker(out_1stcut, pref_weights):\n",
    "        a = out_1stcut['points_CC'] * pref_weights['cloud_cover']\n",
    "        b = out_1stcut['points_Overlap'] * pref_weights['overlap']\n",
    "        c = out_1stcut['points_Date'] * pref_weights['date'] \n",
    "        d = out_1stcut['points_Nadir'] * pref_weights['nadir']\n",
    "        e = out_1stcut['points_Res'] * pref_weights['resolution']\n",
    "\n",
    "        # Score is linear addition of the number of 'points' the scene wins as defined above. More points = worse fit to criteria\n",
    "        rank = a + b + c + d + e\n",
    "        return rank\n",
    "\n",
    "    # Add new column - Rank Result - with the total number of points accrued by the scene \n",
    "    out_1stcut['RankResult'] = Ranker(out_1stcut, pref_weights)\n",
    "\n",
    "    # Add a Preference order column - Pref_Order - based on Rank Result, sorted ascending (best scene first)\n",
    "    out_1stcut = out_1stcut.sort_values(by = 'RankResult', axis = 0, ascending = True)\n",
    "    out_1stcut = out_1stcut.reset_index()\n",
    "    out_1stcut['Pref_order'] = out_1stcut.index + 1\n",
    "    out_1stcut = out_1stcut.drop(['index'], axis = 1)\n",
    "    \n",
    "    cols = ['ID','Sensor','ImageBands','Date','Time','CloudCover','Overlap_%','PanResolution','MultiResolution','OffNadirAngle','On_IDAHO','Pref_order','RankResult','points_CC','points_Overlap','points_Date','points_Nadir','points_Res','browseURL','Full_scene_WKT','useful_area_WKT','missing_area_WKT']\n",
    "    out_1stcut = out_1stcut[cols]\n",
    "    \n",
    "    # Create a new copy of the dataframe to work on\n",
    "    finaldf = out_1stcut\n",
    "    \n",
    "    # Add column for used scene region area, expressed as .wkt\n",
    "    finaldf['used_scene_region_WKT'] = 0\n",
    "    finaldf['used_area'] = 0\n",
    "    \n",
    "    # Set initial value of AOI_remaining to the full AOI under consideration\n",
    "    AOI_remaining = AOI\n",
    "\n",
    "    # Create two lists - usedareas for the areas of scenes used in the final product, and AOI_rems to record sequential reduction in \n",
    "    # remaining AOI that needs to be filled\n",
    "    usedareas = []\n",
    "    AOI_rems = []\n",
    "\n",
    "    # Set up loop for each image in dataframe of ranked images\n",
    "    for s in finaldf.index:\n",
    "        if AOI_remaining.area < (AOI.area / 100):\n",
    "            pass\n",
    "        else: \n",
    "            # pick up the WKT of the useful area as the useful_scene_region variable\n",
    "            useful_scene_region = finaldf['useful_area_WKT'].loc[s]\n",
    "\n",
    "            # Set up try loop - to catch if there is no intersection of AOI_remaining and useful_scene_region\n",
    "            #try\n",
    "            # define 'used_scene_region' as the useable bit of the image that overlaps the AOI\n",
    "            used_scene_region = AOI_remaining.intersection(useful_scene_region)\n",
    "\n",
    "            # calculate the area of that region\n",
    "            used_area = used_scene_region.area\n",
    "            # Check to see if this is a geometry collection. This shapely type if for 'jumbles' of outputs (e.g. Polygons + Lines)\n",
    "            # This can be created if the intersection process decides that it also wants a 1-pixel strip from the bottom of the image\n",
    "            # as well as the main chunk. This won't translate back to a shapefile, so we drop non-Polygon objects iteratively. \n",
    "            if used_scene_region.type == 'GeometryCollection':\n",
    "                xlist = []\n",
    "\n",
    "                # Iterate through all objects in the geometry collection\n",
    "                for y in used_scene_region.geoms:\n",
    "\n",
    "                    # Add polygons to a fresh list\n",
    "                    if y.type == 'Polygon':\n",
    "                        xlist.append(y)\n",
    "\n",
    "                # Convert that list to a multipolygon object\n",
    "                used_scene_region = MultiPolygon(xlist)\n",
    "\n",
    "            # Append the used bit of the image to the usedareas list. \n",
    "            usedareas.append(used_scene_region)\n",
    "            try: \n",
    "                # Add two new columns to the dataframe - the used scene geometry in wkt, and the area of the used scene\n",
    "                finaldf['used_scene_region_WKT'].loc[s] = used_scene_region\n",
    "                finaldf['used_area'].loc[s] = used_area\n",
    "            except: \n",
    "                pass\n",
    "            # Redefine the area of the AOI that needs to be filled by the next, lower-rank image\n",
    "            AOI_remaining = AOI_remaining.difference(used_scene_region)\n",
    "\n",
    "            # Add this to the AOI_rems list for troubelshooting and verification\n",
    "            AOI_rems.append(AOI_remaining)\n",
    "\n",
    "            print '\\t...after image %s, %d percent remaining' % (s+1, (AOI_remaining.area/AOI.area*100))\n",
    "\n",
    "    # Drop from the scene list any scene where the area used is less than 1% of the AOI\n",
    "    finaldf = finaldf.loc[finaldf['used_area'] > (AOI.area / 100)]\n",
    "    # Print summary statistics to consol\n",
    "    print 'AOI %s Complete. Proportion of AOI covered: %d percent' % (AOI_counter, (finaldf['used_area'].sum() / AOI.area * 100))\n",
    "    #Add counter\n",
    "    finaldf['AOI_counter'] = AOI_counter\n",
    "    return finaldf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning image identification process. Standby.\n",
      "\t...after image 1, 0 percent remaining\n",
      "AOI 1 Complete. Proportion of AOI covered: 100 percent\n"
     ]
    }
   ],
   "source": [
    "AOI_counter = 1\n",
    "list_of_dfs = []\n",
    "print 'Beginning image identification process. Standby.'\n",
    "for AOI in fboxxs:\n",
    "    time.sleep(1)\n",
    "    output = Process(\n",
    "            AOI, \n",
    "            cutoff_cloud_cover, \n",
    "            cutoff_overlap, \n",
    "            cutoff_date_low,\n",
    "            cutoff_date_high,\n",
    "            cutoff_nadir, \n",
    "            cutoff_pan_res, \n",
    "            accepted_bands, \n",
    "            optimal_date, \n",
    "            optimal_pan_res, \n",
    "            optimal_nadir, \n",
    "            pref_weights, \n",
    "            AOI_counter\n",
    "            )\n",
    "    list_of_dfs.append(output)\n",
    "    AOI_counter += 1\n",
    "    finaldf = pd.concat(list_of_dfs)\n",
    "    finaldf.to_csv('Scene_List.csv')\n",
    "print 'Process complete'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Intersection Check\n",
    "\n",
    "As the GBDX search function takes bounding boxes, we passed all AOIs to it in box format. It is possible that the above process filled in the box with images, AND that some of those images don't intersect any part of the true AOI. Hence, we remove any images from the ordering list if less than 2% of the used footprint intersects with areas we are interested in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length before: 157\n",
      "lenght after check 133\n"
     ]
    }
   ],
   "source": [
    "finaldf = pd.read_csv('Scene_List.csv')\n",
    "print 'length before: %s' % len(finaldf)\n",
    "\n",
    "def check(x): \n",
    "    if exterior.intersection(x.buffer(0)).area > x.area/50:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "finaldf['drop'] = finaldf['used_scene_region_WKT'].map(shapely.wkt.loads).apply(lambda x: check(x))\n",
    "finaldf = finaldf.loc[finaldf['drop'] == 1]\n",
    "print 'lenght after check %s' %  len(finaldf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Area Coverage\n",
    "\n",
    "This looks at the original shapes - and how far the imagery found covers that area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "def area(x):\n",
    "    return exterior.intersection(x.buffer(0)).area\n",
    "finaldf['AOI_coverage_area'] =  finaldf['used_scene_region_WKT'].map(shapely.wkt.loads).apply(lambda x: area(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Statistical Print and File Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area of mexico: 1943096 square kilometres.\n",
      "Area of AOIs: 2333 square kilometres.\n",
      "Anticipated area of compute (bounding boxes for AOIs): 15062 square kilometres\n",
      "Area of bounding boxes filled by imagery: 12827 square kilometres\n",
      "As a pecentage of Mexico, AOIs = 0.120069 percent, imagery area = 0.775202 percent\n",
      "Percentage coverage of target bounding boxes = 85.157103 percent\n",
      "Percentage coverage of actual AOIs = 98.029653 percent\n",
      "Total images used: 127\n",
      "Images on IDAHO already: 29\n",
      "Images that need to be ordered: 98\n"
     ]
    }
   ],
   "source": [
    "# Mexico ITRF2008 / LCC\n",
    "crs_targ = {'init': 'epsg:6372'}\n",
    "\n",
    "def AreaCalc(obj, crs, crs_targ):\n",
    "    df = gpd.GeoDataFrame(range(0, len(obj)), crs = crs, geometry = obj)\n",
    "    df = df.to_crs(crs_targ)\n",
    "    df['area'] = df.area / 1E6\n",
    "    return df['area'].sum()\n",
    "\n",
    "mehico = gpd.read_file(mex)\n",
    "area_of_Mexico = AreaCalc(mehico['geometry'], crs, crs_targ)\n",
    "area_of_AOIs = AreaCalc(shape['geometry'].tolist(), crs, crs_targ)\n",
    "area_of_final_bboxs = AreaCalc(fboxxs, crs, crs_targ)\n",
    "area_of_imagery_used = AreaCalc(finaldf['used_scene_region_WKT'].map(shapely.wkt.loads).tolist(), crs, crs_targ)\n",
    "\n",
    "unique_images = len(finaldf['ID'].unique())\n",
    "unique_images_on_idaho = len(finaldf.loc[finaldf['On_IDAHO'] == 1].groupby('ID'))\n",
    "to_be_ordered = unique_images - unique_images_on_idaho\n",
    "\n",
    "print 'Area of mexico: %d square kilometres.' % area_of_Mexico\n",
    "print 'Area of AOIs: %d square kilometres.' % area_of_AOIs\n",
    "print 'Anticipated area of compute (bounding boxes for AOIs): %d square kilometres' % area_of_final_bboxs\n",
    "print 'Area of bounding boxes filled by imagery: %d square kilometres' % area_of_imagery_used\n",
    "print 'As a pecentage of Mexico, AOIs = %f percent, imagery area = %f percent' % ((area_of_AOIs*100 / area_of_Mexico), (area_of_final_bboxs*100 / area_of_Mexico))\n",
    "print 'Percentage coverage of target bounding boxes = %f percent' % (area_of_imagery_used*100 / area_of_final_bboxs)\n",
    "print 'Percentage coverage of actual AOIs = %f percent' % (finaldf['AOI_coverage_area'].sum()*100 / exterior.area)\n",
    "print 'Total images used: %d' % unique_images\n",
    "print 'Images on IDAHO already: %d' % unique_images_on_idaho\n",
    "print 'Images that need to be ordered: %d' % to_be_ordered\n",
    "\n",
    "finaldf.to_csv('Final_Scene_List.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df = finaldf.drop_duplicates('ID')\n",
    "order_df.to_csv('Unique IDs.csv')\n",
    "order_df = order_df.loc[order_df['On_IDAHO'] == 0]\n",
    "order_list = order_df['ID'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order Imagery\n",
    "\n",
    "Use this cell block to order up to IDAHO all imagery in ther 'order_list' variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images to be ordered: 98\n"
     ]
    }
   ],
   "source": [
    "order_receipts = []\n",
    "print 'Number of images to be ordered: %d' % len(order_list)\n",
    "\n",
    "consent = 'I agree to ordering these image IDs to IDAHO'\n",
    "\n",
    "if consent == 'I agree to ordering these image IDs to IDAHO':\n",
    "    for x in order_list:\n",
    "        order_id = gbdx.ordering.order(x)\n",
    "        order_receipts.append(order_id)\n",
    "else: \n",
    "    print 'please write out your consent in the consent variable above'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Ordering Status\n",
    "\n",
    "Use this code block to check whether the images have yet been ordered up to IDAHO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'acquisition_id': u'1050410010BB8200', u'state': u'placed', u'location': u'not_delivered'}]\n",
      "[{u'acquisition_id': u'103001002F0A1400', u'state': u'placed', u'location': u'not_delivered'}]\n",
      "[{u'acquisition_id': u'105041001232A700', u'state': u'placed', u'location': u'not_delivered'}]\n",
      "[{u'acquisition_id': u'1050410010402000', u'state': u'placed', u'location': u'not_delivered'}]\n",
      "[{u'acquisition_id': u'103001003B35C200', u'state': u'placed', u'location': u'not_delivered'}]\n",
      "[{u'acquisition_id': u'105041001232A600', u'state': u'placed', u'location': u'not_delivered'}]\n",
      "[{u'acquisition_id': u'1050410003F6D000', u'state': u'placed', u'location': u'not_delivered'}]\n",
      "[{u'acquisition_id': u'1010010011C1AD00', u'state': u'placed', u'location': u'not_delivered'}]\n",
      "[{u'acquisition_id': u'105041001119EA00', u'state': u'placed', u'location': u'not_delivered'}]\n",
      "[{u'acquisition_id': u'103001003AC43300', u'state': u'placed', u'location': u'not_delivered'}]\n"
     ]
    }
   ],
   "source": [
    "for receipt in order_receipts[:10]:\n",
    "    print gbdx.ordering.status(receipt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.01\n",
      "2\n",
      "0.02\n",
      "3\n",
      "0.03\n",
      "4\n",
      "0.04\n",
      "5\n",
      "0.05\n",
      "6\n",
      "0.06\n",
      "7\n",
      "0.07\n",
      "8\n",
      "0.08\n",
      "9\n",
      "0.09\n",
      "10\n",
      "0.1\n",
      "11\n",
      "0.11\n",
      "12\n",
      "0.12\n",
      "13\n",
      "0.13\n",
      "14\n",
      "0.14\n",
      "15\n",
      "0.15\n",
      "16\n",
      "0.16\n",
      "17\n",
      "0.17\n",
      "18\n",
      "0.18\n",
      "19\n",
      "0.19\n",
      "20\n",
      "0.2\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "AREA_TEST = []\n",
    "AOI_NUMBER = []\n",
    "\n",
    "def BoundingBoxList(MultiPolygonObj):\n",
    "    boxlist = []\n",
    "    for obj in MultiPolygonObj:\n",
    "        coords = [n for n in obj.bounds]\n",
    "        bbox = box(coords[0],coords[1],coords[2],coords[3])\n",
    "        boxlist.append(bbox)\n",
    "    return boxlist\n",
    "\n",
    "for i in range(1, 21):\n",
    "    rawAOI = r'agebs_val_muni.shp'\n",
    "    crs = {'init': 'epsg:4326'}\n",
    "    shape = gpd.read_file(rawAOI)\n",
    "    bufw = float(i / 100.0)\n",
    "\n",
    "    polygons = MultiPolygon(shape['geometry'].loc[i] for i in shape.index)\n",
    "\n",
    "    exterior = cascaded_union(polygons)\n",
    "    exterior_boxxs = BoundingBoxList(exterior)\n",
    "\n",
    "    tight_bbox = MultiPolygon(exterior_boxxs)\n",
    "    reduced_boxes = cascaded_union(tight_bbox.buffer(bufw))\n",
    "\n",
    "    rboxxs = BoundingBoxList(reduced_boxes)\n",
    "    final_boxes = cascaded_union(MultiPolygon(rboxxs))\n",
    "    fboxxs = BoundingBoxList(final_boxes.buffer(-bufw))\n",
    "    AREA_TEST.append(final_boxes.area)\n",
    "    AOI_NUMBER.append(len(fboxxs))\n",
    "\n",
    "X = range(1,21)\n",
    "X[:] = [x / 100.0 for x in X]\n",
    "plt.plot(X, AREA_TEST, color = 'blue')\n",
    "plt.plot(X, AOI_NUMBER, color = 'green')\n",
    "plt.xlabel('buf_width')\n",
    "plt.ylabel('Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hub": {
   "id": "5a54f830cee1f025f139c6a8",
   "published": true
  },
  "kernelspec": {
   "display_name": "Python (GGCW)",
   "language": "python",
   "name": "ggcw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
